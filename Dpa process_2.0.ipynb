{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import itertools\n",
    "import worker_xml\n",
    "import multiprocessing as mp\n",
    "from datetime import datetime\n",
    "from itertools import repeat\n",
    "from numbers_delete import numbers_delete\n",
    "from Split_articles import split_mult_art\n",
    "from langdetect import detect\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the number of cores to use\n",
    "NUM_CORE = mp.cpu_count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dpa Data (1991 - 2019)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Folder with unpacked articles\n",
    "path = r'E:\\\\Userhome\\\\jbaer\\\\dpa_unpacked'\n",
    "folder_list = []\n",
    "\n",
    "# 2 folders\n",
    "for fol in [fol for fol in os.listdir(path)]:\n",
    "\n",
    "    # Within each folder: folders for different years\n",
    "    for f in [f for f in os.listdir(path + '\\\\' + fol)]:\n",
    "        folder_list.append(path + '\\\\' + fol + '\\\\' + f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select a path to the folder for storing results\n",
    "PATH = r'E:\\\\Userhome\\\\jbaer\\\\Media Tenor Results'\n",
    "os.chdir(PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2:03:23.803171\n"
     ]
    }
   ],
   "source": [
    "# Use the 'worker_xml' function to load articles\n",
    "startTime = datetime.now()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    pool = mp.Pool(NUM_CORE)\n",
    "    df_list = pool.map(worker_xml.worker_xml, folder_list)\n",
    "    data = pd.concat(df_list)\n",
    "    data.reset_index(inplace=True, drop=True)\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "\n",
    "print(datetime.now()-startTime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of data before filtering: 7539874\n"
     ]
    }
   ],
   "source": [
    "print(\"Length of data before filtering:\", len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_csv('dpa_raw.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out articles with less than 75 words\n",
    "data = data[pd.to_numeric(data['wordcount']) >= 75]\n",
    "data.reset_index(inplace=True, drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out insignificant articles based on keywords, genres etc.\n",
    "fil_titles = '''Edelmetallpreise|Tageskalender|Tabelle|SPORT|SPORT\\:|\n",
    "                Sport|Berichtigung|Sortenkurse|Devisenkurse|Impressum|Testmeldung|\n",
    "                Kurse A|Kurse B|Kurse C|DGAP-DD'''\n",
    "data.drop(data[data['title'].str.contains(fil_titles)].index, inplace=True)\n",
    "\n",
    "fil_genres = '''Tabelle|Historisches|Achtung|Sport|SPORT'''\n",
    "data.drop(data[data['genre'].str.contains(fil_genres)].index, inplace=True)\n",
    "\n",
    "fil_keywords = '''Sport|Redaktionshinweis|SUM|DGAP|Sport|SPORT|SPO'''\n",
    "data.drop(data[data['keywords'].str.contains(fil_keywords)].index, inplace=True)\n",
    "\n",
    "data.drop(data[data['rubrics'].str.contains('iq')].index, inplace=True)\n",
    "\n",
    "fil_texts = '''Schalterverkaufskurse:|dpa-news.de'''\n",
    "data.drop(data[data['texts'].str.contains(fil_texts)].index, inplace=True)\n",
    "\n",
    "data.drop(data[data['source'] == 'dpa-frei\\dpa-wahl'].index, inplace=True)\n",
    "data.reset_index(inplace=True, drop=True)\n",
    "\n",
    "# Filter out articles based on keywords which are specific to the economic news\n",
    "keyword_clean = '''Kurse|KURSE|kurse|Börse International|Terminbörse|Finanzmärkte International'''\n",
    "WiPo = data[data['topic'] == 'WiPo'] \n",
    "data.drop(WiPo[WiPo['keywords'].str.contains(keyword_clean)].index, inplace = True)\n",
    "data.reset_index(inplace=True, drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "121"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# free memory space to avoid memory errors\n",
    "del WiPo\n",
    "\n",
    "import gc\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the 'numbers_delete' function to filter out economic articles with a high share of numbers in them\n",
    "delete_index = numbers_delete(data)\n",
    "data.drop(delete_index, inplace = True)\n",
    "data.reset_index(inplace=True, drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split up articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store entries potentially consisting of multiple articles separately\n",
    "# as 'mult art' \n",
    "s_mult_art = '''dpa-Nachrichtenüberblick|Nachrichtenüberblick|\n",
    "                Vorschau|vorschau|VORSCHAU|Tagesvorschau|\n",
    "                dpa-Tagesvorschau'''\n",
    "\n",
    "mult_art = data[data['title'].str.contains(s_mult_art)]\n",
    "mult_art = mult_art.append(data[data['keywords'].str.contains(s_mult_art)])\n",
    "mult_art = mult_art.append(data[data['genre'].str.contains(s_mult_art)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete 'mult_art' from the original data\n",
    "data.drop(data[data['title'].str.contains(s_mult_art)].index, inplace=True)\n",
    "data.drop(data[data['keywords'].str.contains(s_mult_art)].index, inplace=True)\n",
    "data.drop(data[data['genre'].str.contains(s_mult_art)].index, inplace=True)\n",
    "data.reset_index(inplace=True, drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the 'split_mult_art' function to split up entries in \n",
    "# 'mult_art' and append the resulting new articles to the data\n",
    "data = data.append(split_mult_art(mult_art))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# free memory space to avoid memory errors\n",
    "del mult_art\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of data after first filtering steps: 5806223\n"
     ]
    }
   ],
   "source": [
    "print(\"Length of data after first filtering steps:\", len(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtering Based on Topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into economic and finance articles\n",
    "afx = data[data['topic'] == 'afx']\n",
    "afx.reset_index(inplace=True, drop=True)\n",
    "\n",
    "WiPo = data[data['topic'] == 'WiPo'] \n",
    "WiPo.reset_index(inplace=True, drop=True)\n",
    "\n",
    "WiPo = WiPo.drop(columns=['index'])\n",
    "afx = afx.drop(columns=['index'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of WiPo Articles: 2905412 , Number of afx Articles: 2900811\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of WiPo Articles:\", len(WiPo), \", Number of afx Articles:\", len(afx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save intermediate results\n",
    "WiPo.to_csv('dpa_WiPo_pre_filtered.csv')\n",
    "afx.to_csv('dpa_afx_pre_filtered.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# free memory space to avoid memory errors\n",
    "del data\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3146: DtypeWarning: Columns (8,12) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n"
     ]
    }
   ],
   "source": [
    "WiPo = pd.read_csv('dpa_WiPo_pre_filtered.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Delete Fuzzy Duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\gensim\\similarities\\__init__.py:15: UserWarning: The gensim.similarities.levenshtein submodule is disabled, because the optional Levenshtein package <https://pypi.org/project/python-Levenshtein/> is unavailable. Install Levenhstein (e.g. `pip install python-Levenshtein`) to suppress this warning.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:30:55.625995\n",
      "0:31:49.791557\n",
      "0:32:54.944771\n",
      "0:30:32.665614\n",
      "0:31:42.206455\n",
      "0:32:08.340749\n",
      "0:30:45.280681\n",
      "0:37:29.015236\n",
      "0:47:03.376425\n",
      "0:48:09.322768\n",
      "0:46:45.712046\n",
      "0:43:36.020484\n",
      "0:42:49.812051\n",
      "0:38:38.036791\n",
      "0:40:12.614122\n",
      "0:42:02.905433\n",
      "0:44:25.646218\n",
      "0:56:13.789029\n",
      "0:53:31.579092\n",
      "0:42:53.343497\n",
      "0:44:15.374064\n",
      "0:43:33.926395\n",
      "0:42:23.491649\n",
      "0:39:53.796130\n",
      "0:39:36.912825\n",
      "0:40:44.074491\n",
      "0:33:46.458266\n",
      "0:27:44.204960\n",
      "0:00:00.020247\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-da79ac0ebada>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[0mmerged\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mitertools\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mdelete_indices\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 35\u001b[1;33m \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmerged\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'data' is not defined"
     ]
    }
   ],
   "source": [
    "# NOTE: I ignored the multiprocessing function to avoid memory issues\n",
    "\n",
    "import fuzzy_duplicates\n",
    "delete_indices = []\n",
    " \n",
    "for year, data_yearly in WiPo.groupby('year'):\n",
    "    \n",
    "    data_yearly = data_yearly.reset_index()\n",
    "    # Prepare inputs\n",
    "    inputs_year = []\n",
    "    inputs_month = []\n",
    "\n",
    "    for month in list(set(data_yearly['month'])):\n",
    "        inputs_year.append(year)\n",
    "        inputs_month.append(month)\n",
    "\n",
    "    inputs = list(zip(inputs_year, inputs_month, repeat(data_yearly)))\n",
    "    \n",
    "    startTime = datetime.now()\n",
    "    \n",
    "    for inp in inputs:\n",
    "\n",
    "        delete_indices.append(fuzzy_duplicates.fuzzy_duplicates(inp))\n",
    "\n",
    "        # Apply function to all combinations of month-year in parallel\n",
    "        # if __name__ == \"__main__\":\n",
    "        # pool = mp.Pool(NUM_CORE)\n",
    "        # delete_indices.append(pool.map(fuzzy_duplicates.fuzzy_duplicates, inputs))\n",
    "        # pool.close()\n",
    "        # pool.join()\n",
    "\n",
    "    print(datetime.now()-startTime)\n",
    "\n",
    "merged = list(itertools.chain(*delete_indices))\n",
    "WiPo.drop(merged, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Delete English Articles (optional for afx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:00:29.154173\n"
     ]
    }
   ],
   "source": [
    "# Import a function that outputs the indices of englisch articles\n",
    "import identify_eng\n",
    "\n",
    "# Delete all English articles from the data\n",
    "\n",
    "for year, data_yearly in afx.groupby('year'):\n",
    "    \n",
    "    startTime = datetime.now()\n",
    "\n",
    "    if __name__ == \"__main__\":\n",
    "        pool = mp.Pool(NUM_CORE)\n",
    "        split_dfs = np.array_split(data_yearly, NUM_CORE)\n",
    "        index_eng = pool.map(identify_eng.identify_eng, split_dfs)\n",
    "        index_eng = list(itertools.chain(*index_eng))\n",
    "        pool.close()\n",
    "        pool.join()\n",
    "\n",
    "    print(datetime.now()-startTime)\n",
    "\n",
    "data.drop(index_eng, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save final results\n",
    "WiPo.to_csv('dpa_WiPo_filtered.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save final results\n",
    "afx.to_csv('dpa_afx_filtered.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
