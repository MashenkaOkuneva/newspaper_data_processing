{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import itertools\n",
    "import dpa_load\n",
    "import multiprocessing as mp\n",
    "from datetime import datetime\n",
    "from itertools import repeat\n",
    "import split_articles\n",
    "import numeric_articles\n",
    "import continue_articles\n",
    "# we import some functions from the Handelsblatt folder\n",
    "import sys\n",
    "sys.path.insert(1, os.getcwd().replace('dpa code', 'Handelsblatt'))\n",
    "import count_words_mp\n",
    "import identify_eng_2\n",
    "import correct_url\n",
    "import clean_dpa_articles\n",
    "import clean_dpa_references"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the number of cores to use\n",
    "NUM_CORE = mp.cpu_count()-4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DPA Data (1991 - 2018)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deutsche PresseAgentur (DPA) is the Germany's biggest news agency which sells its news reports to the leading German newspapers. We believe that the data set has a high chance to be useful for economic forecasting because DPA produces information that is timely and has a large reach.\n",
    "\n",
    "We purchased DPA data in November 2019. The corpus consists of **7,539,874** articles from January 1991 to December 2018.\n",
    "\n",
    "The data set includes news from both dpa-Basisdienstes and dpa-afx Wirtschaftsnachrichten. The former one is the basic news service covering such topics as Economy, Politics, and  Finance. The second one was created in 1999. It specializes in financial news."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we read in the data by extracting the following XML elements:\n",
    "\n",
    "* title - article's title\n",
    "* text - text of the article\n",
    "* date - publication date\n",
    "* ressort - section (Politics vs Economy)\n",
    "* source/credit - source (dpa vs afx)\n",
    "* city - which city the news article refers to\n",
    "* genre - journalistic genre, e.g., chronology, story, table\n",
    "* wortanzahl - word count\n",
    "* keywords - keywords associated with an article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Folder with unpacked articles\n",
    "#path = r'E:\\\\Userhome\\\\jbaer\\\\dpa_unpacked'\n",
    "\n",
    "#path = r'G:\\\\Test\\\\Results\\\\dpa Raw Data\\\\dpa_unpacked'\n",
    "path = os.getcwd().replace('\\\\newspaper_data_processing\\\\dpa code', '') + '\\\\dpa_unpacked'\n",
    "\n",
    "folder_list = []\n",
    "\n",
    "# 2 folders for dpa and dpa-afx \n",
    "for fol in [fol for fol in os.listdir(path)]:\n",
    "\n",
    "    # Within each folder: folders for different years\n",
    "    for f in [f for f in os.listdir(path + '\\\\' + fol)]:\n",
    "        folder_list.append(path + '\\\\' + fol + '\\\\' + f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select a path to the folder for storing results\n",
    "#PATH = r'G:\\\\Test\\\\Results'\n",
    "#os.chdir(PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:45:16.705110\n"
     ]
    }
   ],
   "source": [
    "# Use the 'dpa_load' function to load articles\n",
    "startTime = datetime.now()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    pool = mp.Pool(NUM_CORE)\n",
    "    df_list = pool.map(dpa_load.dpa_load, folder_list)\n",
    "    data = pd.concat(df_list)\n",
    "    data.reset_index(inplace=True, drop=True)\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "\n",
    "print(datetime.now()-startTime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7539874\n"
     ]
    }
   ],
   "source": [
    "print(len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.sort_values(['year', 'month', 'day'], ascending=[True, True, True]) # sort the data in chronological order\n",
    "data.reset_index(inplace=True, drop=True) # reset the index of the DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>texts</th>\n",
       "      <th>file</th>\n",
       "      <th>day</th>\n",
       "      <th>month</th>\n",
       "      <th>year</th>\n",
       "      <th>rubrics</th>\n",
       "      <th>source</th>\n",
       "      <th>keywords</th>\n",
       "      <th>title</th>\n",
       "      <th>city</th>\n",
       "      <th>genre</th>\n",
       "      <th>wordcount</th>\n",
       "      <th>topic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Schalck: Milliardenkredit sicherte Zahlungsfäh...</td>\n",
       "      <td>5739189.xml</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1991</td>\n",
       "      <td>pl</td>\n",
       "      <td>dpa</td>\n",
       "      <td>Schalck-Golodkowski</td>\n",
       "      <td>Schalck: Milliardenkredit sicherte Zahlungsfäh...</td>\n",
       "      <td>Berlin</td>\n",
       "      <td></td>\n",
       "      <td>218</td>\n",
       "      <td>WiPo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Tschads Regierung: Bevölkerung soll Waffen abl...</td>\n",
       "      <td>5739191.xml</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1991</td>\n",
       "      <td>pl</td>\n",
       "      <td>dpa</td>\n",
       "      <td>Tschad</td>\n",
       "      <td>Tschads Regierung: Bevölkerung soll Waffen abl...</td>\n",
       "      <td>N'Djamena</td>\n",
       "      <td></td>\n",
       "      <td>75</td>\n",
       "      <td>WiPo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Welajati: Iran bleibt bei einem Krieg am Golf ...</td>\n",
       "      <td>5739193.xml</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1991</td>\n",
       "      <td>pl</td>\n",
       "      <td>dpa</td>\n",
       "      <td>Golfkrise Iran</td>\n",
       "      <td>Welajati: Iran bleibt bei einem Krieg am Golf ...</td>\n",
       "      <td>Teheran</td>\n",
       "      <td></td>\n",
       "      <td>90</td>\n",
       "      <td>WiPo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Bush will offenbar seinen Außenminister erneut...</td>\n",
       "      <td>5739195.xml</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1991</td>\n",
       "      <td>pl</td>\n",
       "      <td>dpa</td>\n",
       "      <td>Golfkrise USA</td>\n",
       "      <td>Bush will offenbar seinen Außenminister erneut...</td>\n",
       "      <td>Washington</td>\n",
       "      <td></td>\n",
       "      <td>181</td>\n",
       "      <td>WiPo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Morgenzusammenfassung Neue Runde diplomatische...</td>\n",
       "      <td>5739199.xml</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1991</td>\n",
       "      <td>pl</td>\n",
       "      <td>dpa</td>\n",
       "      <td>Golfkrise</td>\n",
       "      <td>Morgenzusammenfassung Neue Runde diplomatische...</td>\n",
       "      <td>Washington/Luxemburg</td>\n",
       "      <td></td>\n",
       "      <td>504</td>\n",
       "      <td>WiPo</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               texts         file  day  month  \\\n",
       "0  Schalck: Milliardenkredit sicherte Zahlungsfäh...  5739189.xml    1      1   \n",
       "1  Tschads Regierung: Bevölkerung soll Waffen abl...  5739191.xml    1      1   \n",
       "2  Welajati: Iran bleibt bei einem Krieg am Golf ...  5739193.xml    1      1   \n",
       "3  Bush will offenbar seinen Außenminister erneut...  5739195.xml    1      1   \n",
       "4  Morgenzusammenfassung Neue Runde diplomatische...  5739199.xml    1      1   \n",
       "\n",
       "   year rubrics source             keywords  \\\n",
       "0  1991      pl    dpa  Schalck-Golodkowski   \n",
       "1  1991      pl    dpa               Tschad   \n",
       "2  1991      pl    dpa       Golfkrise Iran   \n",
       "3  1991      pl    dpa        Golfkrise USA   \n",
       "4  1991      pl    dpa            Golfkrise   \n",
       "\n",
       "                                               title                  city  \\\n",
       "0  Schalck: Milliardenkredit sicherte Zahlungsfäh...                Berlin   \n",
       "1  Tschads Regierung: Bevölkerung soll Waffen abl...             N'Djamena   \n",
       "2  Welajati: Iran bleibt bei einem Krieg am Golf ...               Teheran   \n",
       "3  Bush will offenbar seinen Außenminister erneut...            Washington   \n",
       "4  Morgenzusammenfassung Neue Runde diplomatische...  Washington/Luxemburg   \n",
       "\n",
       "  genre wordcount topic  \n",
       "0             218  WiPo  \n",
       "1              75  WiPo  \n",
       "2              90  WiPo  \n",
       "3             181  WiPo  \n",
       "4             504  WiPo  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data.to_csv('dpa_raw.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('dpa_raw.csv', encoding = 'utf-8', index_col = 0,  keep_default_na=False,\n",
    "                   dtype = {'rubrics': 'str', \n",
    "                            'source': 'str',\n",
    "                            'keywords': 'str',\n",
    "                            'title': 'str',\n",
    "                            'city': 'str',\n",
    "                            'genre': 'str',\n",
    "                            'wordcount': 'str'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.sort_values(['year', 'month', 'day'], ascending=[True, True, True]) # sort the data in chronological order\n",
    "data.reset_index(inplace=True, drop=True) # reset the index of the DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>texts</th>\n",
       "      <th>file</th>\n",
       "      <th>day</th>\n",
       "      <th>month</th>\n",
       "      <th>year</th>\n",
       "      <th>rubrics</th>\n",
       "      <th>source</th>\n",
       "      <th>keywords</th>\n",
       "      <th>title</th>\n",
       "      <th>city</th>\n",
       "      <th>genre</th>\n",
       "      <th>wordcount</th>\n",
       "      <th>topic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Schalck: Milliardenkredit sicherte Zahlungsfäh...</td>\n",
       "      <td>5739189.xml</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1991</td>\n",
       "      <td>pl</td>\n",
       "      <td>dpa</td>\n",
       "      <td>Schalck-Golodkowski</td>\n",
       "      <td>Schalck: Milliardenkredit sicherte Zahlungsfäh...</td>\n",
       "      <td>Berlin</td>\n",
       "      <td></td>\n",
       "      <td>218</td>\n",
       "      <td>WiPo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Tschads Regierung: Bevölkerung soll Waffen abl...</td>\n",
       "      <td>5739191.xml</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1991</td>\n",
       "      <td>pl</td>\n",
       "      <td>dpa</td>\n",
       "      <td>Tschad</td>\n",
       "      <td>Tschads Regierung: Bevölkerung soll Waffen abl...</td>\n",
       "      <td>N'Djamena</td>\n",
       "      <td></td>\n",
       "      <td>75</td>\n",
       "      <td>WiPo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Welajati: Iran bleibt bei einem Krieg am Golf ...</td>\n",
       "      <td>5739193.xml</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1991</td>\n",
       "      <td>pl</td>\n",
       "      <td>dpa</td>\n",
       "      <td>Golfkrise Iran</td>\n",
       "      <td>Welajati: Iran bleibt bei einem Krieg am Golf ...</td>\n",
       "      <td>Teheran</td>\n",
       "      <td></td>\n",
       "      <td>90</td>\n",
       "      <td>WiPo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Bush will offenbar seinen Außenminister erneut...</td>\n",
       "      <td>5739195.xml</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1991</td>\n",
       "      <td>pl</td>\n",
       "      <td>dpa</td>\n",
       "      <td>Golfkrise USA</td>\n",
       "      <td>Bush will offenbar seinen Außenminister erneut...</td>\n",
       "      <td>Washington</td>\n",
       "      <td></td>\n",
       "      <td>181</td>\n",
       "      <td>WiPo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Morgenzusammenfassung Neue Runde diplomatische...</td>\n",
       "      <td>5739199.xml</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1991</td>\n",
       "      <td>pl</td>\n",
       "      <td>dpa</td>\n",
       "      <td>Golfkrise</td>\n",
       "      <td>Morgenzusammenfassung Neue Runde diplomatische...</td>\n",
       "      <td>Washington/Luxemburg</td>\n",
       "      <td></td>\n",
       "      <td>504</td>\n",
       "      <td>WiPo</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               texts         file  day  month  \\\n",
       "0  Schalck: Milliardenkredit sicherte Zahlungsfäh...  5739189.xml    1      1   \n",
       "1  Tschads Regierung: Bevölkerung soll Waffen abl...  5739191.xml    1      1   \n",
       "2  Welajati: Iran bleibt bei einem Krieg am Golf ...  5739193.xml    1      1   \n",
       "3  Bush will offenbar seinen Außenminister erneut...  5739195.xml    1      1   \n",
       "4  Morgenzusammenfassung Neue Runde diplomatische...  5739199.xml    1      1   \n",
       "\n",
       "   year rubrics source             keywords  \\\n",
       "0  1991      pl    dpa  Schalck-Golodkowski   \n",
       "1  1991      pl    dpa               Tschad   \n",
       "2  1991      pl    dpa       Golfkrise Iran   \n",
       "3  1991      pl    dpa        Golfkrise USA   \n",
       "4  1991      pl    dpa            Golfkrise   \n",
       "\n",
       "                                               title                  city  \\\n",
       "0  Schalck: Milliardenkredit sicherte Zahlungsfäh...                Berlin   \n",
       "1  Tschads Regierung: Bevölkerung soll Waffen abl...             N'Djamena   \n",
       "2  Welajati: Iran bleibt bei einem Krieg am Golf ...               Teheran   \n",
       "3  Bush will offenbar seinen Außenminister erneut...            Washington   \n",
       "4  Morgenzusammenfassung Neue Runde diplomatische...  Washington/Luxemburg   \n",
       "\n",
       "  genre wordcount topic  \n",
       "0             218  WiPo  \n",
       "1              75  WiPo  \n",
       "2              90  WiPo  \n",
       "3             181  WiPo  \n",
       "4             504  WiPo  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Light pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove short articles (<100 words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Short articles are often incoherent or contain only insiginicant news. For this reason we decided to filter out articles that consist of less than 100 words. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:02:03.186732\n"
     ]
    }
   ],
   "source": [
    "startTime = datetime.now()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    pool = mp.Pool(NUM_CORE)\n",
    "    count_results = pool.map(count_words_mp.count_words_mp, [text.replace('PARAGRAPH', ' ') for text in data['texts']]) \n",
    "    pool.close()\n",
    "    pool.join()\n",
    "    \n",
    "print(datetime.now()-startTime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the result as a new column \"word_count\"\n",
    "data['word_count'] = count_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5364634\n"
     ]
    }
   ],
   "source": [
    "# remove articles with less than 100 words\n",
    "data = data[data['word_count']>=100]\n",
    "data.reset_index(inplace=True, drop=True)\n",
    "print(len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data.to_csv('dpa_prepro_step1.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove exact duplicates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A few examples of duplicates in our corpus:\n",
    "* The same article enters the corpus twice with different publication dates (e.g., 10.1.1991 and 11.1.1991). In this case, a natural solution is to keep the first entry.\n",
    "* The same article appears twice with a slight variation in the metadata (e.g., the word count is a little different even though the articles are identical, or the keywords differ).\n",
    "* The same article enters the corpus twice with the same publication date and metadata.\n",
    "* The same article is published by dpa and dpa-afx (in this case, we keep an article published by dpa)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.sort_values(['year', 'month', 'day', 'topic'], ascending=[True, True, True, True]) # dpa articles come before afx articles\n",
    "data.reset_index(inplace=True, drop=True) # reset the index of the DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1595777"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# All the duplicated articles are saved as 'dpa_duplicates' for further exploration.\n",
    "dpa_duplicates = data[data['texts'].duplicated(keep = False)]\n",
    "len(dpa_duplicates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>texts</th>\n",
       "      <th>file</th>\n",
       "      <th>day</th>\n",
       "      <th>month</th>\n",
       "      <th>year</th>\n",
       "      <th>rubrics</th>\n",
       "      <th>source</th>\n",
       "      <th>keywords</th>\n",
       "      <th>title</th>\n",
       "      <th>city</th>\n",
       "      <th>genre</th>\n",
       "      <th>wordcount</th>\n",
       "      <th>topic</th>\n",
       "      <th>word_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Neues Schema für Investmentkurse Achtung: Vom ...</td>\n",
       "      <td>5739254.xml</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1991</td>\n",
       "      <td>wi</td>\n",
       "      <td>dpa</td>\n",
       "      <td>KURSE DREI A)</td>\n",
       "      <td>Neues Schema für Investmentkurse Achtung: Vom ...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>544</td>\n",
       "      <td>WiPo</td>\n",
       "      <td>283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>NEUES INVESTMENTSCHEMA. 125) FF RESERVE FONDS ...</td>\n",
       "      <td>5739268.xml</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1991</td>\n",
       "      <td>wi</td>\n",
       "      <td>dpa</td>\n",
       "      <td>KURSE DREI B</td>\n",
       "      <td>NEUES INVESTMENTSCHEMA.</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>489</td>\n",
       "      <td>WiPo</td>\n",
       "      <td>258</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>NEUES INVESTMENTSCHEMA. 247) RE-INRENTA 248) R...</td>\n",
       "      <td>5739273.xml</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1991</td>\n",
       "      <td>wi</td>\n",
       "      <td>dpa</td>\n",
       "      <td>KURSE DREI C)</td>\n",
       "      <td>NEUES INVESTMENTSCHEMA.</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>430</td>\n",
       "      <td>WiPo</td>\n",
       "      <td>262</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>209</th>\n",
       "      <td>Neues Schema für Investmentkurse Achtung: Vom ...</td>\n",
       "      <td>5740026.xml</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1991</td>\n",
       "      <td>wi</td>\n",
       "      <td>dpa</td>\n",
       "      <td>KURSE DREI A)</td>\n",
       "      <td>Neues Schema für Investmentkurse Achtung: Vom ...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>544</td>\n",
       "      <td>WiPo</td>\n",
       "      <td>283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>210</th>\n",
       "      <td>NEUES INVESTMENTSCHEMA. 125) FF RESERVE FONDS ...</td>\n",
       "      <td>5740028.xml</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1991</td>\n",
       "      <td>wi</td>\n",
       "      <td>dpa</td>\n",
       "      <td>KURSE DREI B</td>\n",
       "      <td>NEUES INVESTMENTSCHEMA.</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>489</td>\n",
       "      <td>WiPo</td>\n",
       "      <td>258</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 texts         file  day  \\\n",
       "13   Neues Schema für Investmentkurse Achtung: Vom ...  5739254.xml    1   \n",
       "15   NEUES INVESTMENTSCHEMA. 125) FF RESERVE FONDS ...  5739268.xml    1   \n",
       "16   NEUES INVESTMENTSCHEMA. 247) RE-INRENTA 248) R...  5739273.xml    1   \n",
       "209  Neues Schema für Investmentkurse Achtung: Vom ...  5740026.xml    2   \n",
       "210  NEUES INVESTMENTSCHEMA. 125) FF RESERVE FONDS ...  5740028.xml    2   \n",
       "\n",
       "     month  year rubrics source       keywords  \\\n",
       "13       1  1991      wi    dpa  KURSE DREI A)   \n",
       "15       1  1991      wi    dpa   KURSE DREI B   \n",
       "16       1  1991      wi    dpa  KURSE DREI C)   \n",
       "209      1  1991      wi    dpa  KURSE DREI A)   \n",
       "210      1  1991      wi    dpa   KURSE DREI B   \n",
       "\n",
       "                                                 title city genre wordcount  \\\n",
       "13   Neues Schema für Investmentkurse Achtung: Vom ...                  544   \n",
       "15                             NEUES INVESTMENTSCHEMA.                  489   \n",
       "16                             NEUES INVESTMENTSCHEMA.                  430   \n",
       "209  Neues Schema für Investmentkurse Achtung: Vom ...                  544   \n",
       "210                            NEUES INVESTMENTSCHEMA.                  489   \n",
       "\n",
       "    topic  word_count  \n",
       "13   WiPo         283  \n",
       "15   WiPo         258  \n",
       "16   WiPo         262  \n",
       "209  WiPo         283  \n",
       "210  WiPo         258  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dpa_duplicates.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some articles are more than once in the corpus. We filter out all duplicates and only keep the articles with the oldest date or the articles published by dpa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4542316\n"
     ]
    }
   ],
   "source": [
    "data.drop_duplicates(['texts'], keep = 'first', inplace=True)\n",
    "data.reset_index(inplace=True, drop=True)\n",
    "print(len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data.to_csv('dpa_prepro_step2.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Corrections and updates of text news"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two types of articles that we consider separately: text news corrections and updates.\n",
    "\n",
    "* In text news corrections (Berichtigung), journalists usually change only a few facts. More rarely, they add or rewrite several paragraphs. In most of the cases, corrected and original news texts are pubished on the same day.\n",
    "\n",
    "* In the updated texts news (Aktualisierung), journalists add a small amount of text to reflect the latest status. These articles can be published a few days later than the original article. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Corrected news texts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The corrected news texts do not contain much new information, so we treat them as duplicates. However, before deleting these articles, we check if we can always find the original articles in the dataset.\n",
    "\n",
    "Often the titles of corrected news articles include the titles of the original articles. For example, the title of the corrected article 'Berichtigung: Zahl der betroffenen US-Staaten Clinton will 23,5 Millionen Hektar Nationalforst schützen.' includes information on what was changed (Berichtigung: Zahl der betroffenen US-Staaten) and the title of the original article 'Clinton will 23,5 Millionen Hektar Nationalforst schützen.'.\n",
    "\n",
    "We use this fact and try to find pairs of original and corrected news reports with the same title. While the titles of corrected news reports may have a different format, being able to find pairs of articles sharing the same title in each time period means that there are no temporal changes in the data.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Corrected news reports contain 'Berichtigung' (correction) in the title. However, we exclude the articles on rectified shares that include 'Berichtigungsaktien' in the title."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Corrected news reports in dpa data.\n",
    "Berichtigung = data[(data.title.str.contains('Berichtigung')) & (~data.title.str.contains('Berichtigungsaktien')) & (data.topic == 'WiPo')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:12:04.247080\n"
     ]
    }
   ],
   "source": [
    "# track time\n",
    "startTime = datetime.now()\n",
    "# List with a publication year of the corrected articles\n",
    "inputs_year = []\n",
    "# List with a publication month of the corrected articles\n",
    "inputs_month = []\n",
    "# List with a publication day of the corrected articles\n",
    "inputs_day = []\n",
    "# List with the titles of the corrected articles\n",
    "inputs_Berichtigung_title = []\n",
    "# List with the titles of all the articles published on the same day as the corrected article\n",
    "inputs_titles = []\n",
    "# In this project we will concentrate on dpa rather than dpa-afx data. Please see explanation below.\n",
    "dpa_data = data[data.topic=='WiPo']\n",
    "\n",
    "for ind in Berichtigung.index:\n",
    "    inputs_year.append(Berichtigung['year'][ind])\n",
    "    inputs_month.append(Berichtigung['month'][ind])\n",
    "    inputs_day.append(Berichtigung['day'][ind])\n",
    "    inputs_Berichtigung_title.append(Berichtigung['title'][ind])\n",
    "    inputs_titles.append(list(dpa_data[(dpa_data['year'] == Berichtigung['year'][ind]) & (dpa_data['month'] == Berichtigung['month'][ind]) & (dpa_data['day'] == Berichtigung['day'][ind])][\"title\"]))        \n",
    "inputs = list(zip(inputs_year, inputs_month, inputs_day, inputs_Berichtigung_title, inputs_titles))\n",
    "print(datetime.now()-startTime)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the function `Berichtigung_pairs` to output the dataframe that contains the titles of the corrected and original articles along with their publication date. This dataframe will help us understand if there are some temporal changes in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:00:25.306595\n"
     ]
    }
   ],
   "source": [
    "startTime = datetime.now()\n",
    "\n",
    "import Berichtigung_pairs\n",
    "import pandas as pd\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    pool = mp.Pool(NUM_CORE)\n",
    "    Berichtigung_intermediate = pool.map(Berichtigung_pairs.Berichtigung_pairs, inputs)\n",
    "    Berichtigung_df = pd.concat(Berichtigung_intermediate) \n",
    "    pool.close()\n",
    "    pool.join()\n",
    "    \n",
    "print(datetime.now()-startTime)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that this dataframe includes 15060 titles of the corrected and original articles, while the total number of corrected articles is equal to 30230."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of pairs found = 15060\n",
      "The number of corrected articles = 30230\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>day</th>\n",
       "      <th>month</th>\n",
       "      <th>year</th>\n",
       "      <th>title_Berichtigung</th>\n",
       "      <th>title_main</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "      <td>6</td>\n",
       "      <td>1994</td>\n",
       "      <td>Berichtigung - Neue Einleitung - Zusammenfassu...</td>\n",
       "      <td>Koalition verzichtet auf gemeinsames Steuerkon...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>11</td>\n",
       "      <td>1994</td>\n",
       "      <td>Berichtigung Zusammenfassung Süssmuth - Berlin...</td>\n",
       "      <td>Bubis warnt vor Verharmlosung der NS-Verbrechen.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>27</td>\n",
       "      <td>5</td>\n",
       "      <td>1996</td>\n",
       "      <td>Berichtigung - Militärjunta - Rangun/0734 - Ac...</td>\n",
       "      <td>Militärjunta in Rangun weist Forderungen nach ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>1997</td>\n",
       "      <td>(Berichtigung - Korrektur im letzten Satz) «Fr...</td>\n",
       "      <td>«Frankfurter Neue Presse» zur Mars-Expedition.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>1997</td>\n",
       "      <td>(Berichtigung - Korrektur im zweiten und letzt...</td>\n",
       "      <td>«Frankfurter Neue Presse» zur Mars-Expedition.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index  day  month  year                                 title_Berichtigung  \\\n",
       "0      0   16      6  1994  Berichtigung - Neue Einleitung - Zusammenfassu...   \n",
       "1      0    9     11  1994  Berichtigung Zusammenfassung Süssmuth - Berlin...   \n",
       "2      0   27      5  1996  Berichtigung - Militärjunta - Rangun/0734 - Ac...   \n",
       "3      0    4      7  1997  (Berichtigung - Korrektur im letzten Satz) «Fr...   \n",
       "4      0    4      7  1997  (Berichtigung - Korrektur im zweiten und letzt...   \n",
       "\n",
       "                                          title_main  \n",
       "0  Koalition verzichtet auf gemeinsames Steuerkon...  \n",
       "1   Bubis warnt vor Verharmlosung der NS-Verbrechen.  \n",
       "2  Militärjunta in Rangun weist Forderungen nach ...  \n",
       "3     «Frankfurter Neue Presse» zur Mars-Expedition.  \n",
       "4     «Frankfurter Neue Presse» zur Mars-Expedition.  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Berichtigung_df = Berichtigung_df.reset_index()\n",
    "print(\"The number of pairs found = {}\".format(len(Berichtigung_df)))\n",
    "print(\"The number of corrected articles = {}\".format(len(Berichtigung)))\n",
    "Berichtigung_df['day'] = list(map(int, Berichtigung_df.day))\n",
    "Berichtigung_df['month'] = list(map(int, Berichtigung_df.month))\n",
    "Berichtigung_df['year'] = list(map(int, Berichtigung_df.year))\n",
    "Berichtigung_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To understand why this is the case, we create a dataframe with the number of corrected articles (column 'texts') and identified pairs (column 'title_Berichtigung') per year.\n",
    "\n",
    "We can see that before 2001, the number of corrected articles is low, which explains why the number of identified original articles is also low. Between 2001 and 2011, the number of corrected articles ranges between between 900 and 2000, and we successfully identify most of the original articles that have the same title as their corrections.\n",
    "\n",
    "However, from 2012 onwards, the number of original articles that we are able to find decreases significantly. After further exploration, we realised that over this period of time most of the original articles had been removed. Therefore, we decided to delete only those corrected articles that were published before 2012 to avoid the risk of getting rid of unique articles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>year</th>\n",
       "      <th>texts</th>\n",
       "      <th>title_Berichtigung</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1991</td>\n",
       "      <td>18</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1992</td>\n",
       "      <td>38</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1993</td>\n",
       "      <td>34</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1994</td>\n",
       "      <td>15</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1995</td>\n",
       "      <td>14</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1996</td>\n",
       "      <td>17</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1997</td>\n",
       "      <td>11</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1998</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1999</td>\n",
       "      <td>31</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2000</td>\n",
       "      <td>26</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2001</td>\n",
       "      <td>902</td>\n",
       "      <td>551</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2002</td>\n",
       "      <td>1306</td>\n",
       "      <td>966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2003</td>\n",
       "      <td>1478</td>\n",
       "      <td>1092</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2004</td>\n",
       "      <td>1345</td>\n",
       "      <td>993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2005</td>\n",
       "      <td>1569</td>\n",
       "      <td>1206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2006</td>\n",
       "      <td>1571</td>\n",
       "      <td>1242</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>2007</td>\n",
       "      <td>1770</td>\n",
       "      <td>1352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2008</td>\n",
       "      <td>2228</td>\n",
       "      <td>1725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>2009</td>\n",
       "      <td>2083</td>\n",
       "      <td>1594</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>2010</td>\n",
       "      <td>1990</td>\n",
       "      <td>1588</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>2011</td>\n",
       "      <td>1885</td>\n",
       "      <td>1269</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>2012</td>\n",
       "      <td>1861</td>\n",
       "      <td>102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>2013</td>\n",
       "      <td>1820</td>\n",
       "      <td>98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>2014</td>\n",
       "      <td>1782</td>\n",
       "      <td>98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>2015</td>\n",
       "      <td>1706</td>\n",
       "      <td>64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>2016</td>\n",
       "      <td>1645</td>\n",
       "      <td>45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>2017</td>\n",
       "      <td>1519</td>\n",
       "      <td>43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>2018</td>\n",
       "      <td>1554</td>\n",
       "      <td>37</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    year  texts  title_Berichtigung\n",
       "0   1991     18                <NA>\n",
       "1   1992     38                <NA>\n",
       "2   1993     34                <NA>\n",
       "3   1994     15                   2\n",
       "4   1995     14                <NA>\n",
       "5   1996     17                   1\n",
       "6   1997     11                   2\n",
       "7   1998     12                   1\n",
       "8   1999     31                   5\n",
       "9   2000     26                   6\n",
       "10  2001    902                 551\n",
       "11  2002   1306                 966\n",
       "12  2003   1478                1092\n",
       "13  2004   1345                 993\n",
       "14  2005   1569                1206\n",
       "15  2006   1571                1242\n",
       "16  2007   1770                1352\n",
       "17  2008   2228                1725\n",
       "18  2009   2083                1594\n",
       "19  2010   1990                1588\n",
       "20  2011   1885                1269\n",
       "21  2012   1861                 102\n",
       "22  2013   1820                  98\n",
       "23  2014   1782                  98\n",
       "24  2015   1706                  64\n",
       "25  2016   1645                  45\n",
       "26  2017   1519                  43\n",
       "27  2018   1554                  37"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1 = Berichtigung.groupby('year').nunique()['texts'].to_frame().reset_index()\n",
    "df2 = Berichtigung_df.groupby('year').nunique()['title_Berichtigung'].to_frame().reset_index()\n",
    "merged_df = df1.merge(df2, how = 'left')\n",
    "merged_df['title_Berichtigung'] = merged_df['title_Berichtigung'].astype('Int64')\n",
    "merged_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we delete the corrected articles published before 2012."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4523973\n"
     ]
    }
   ],
   "source": [
    "data.drop(data[(data['title'].str.contains('Berichtigung', na = False)) & (~data['title'].str.contains('Berichtigungsaktien', na=False)) & \\\n",
    "               (data.topic == 'WiPo') & (data.year < 2012)].index, inplace=True)\n",
    "data.reset_index(inplace=True, drop=True)\n",
    "print(len(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Updated news texts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The updated news texts might contain relatively large chunks of new information. Therefore, we do not want to delete them. Instead, we have done the following:\n",
    "\n",
    "1. If we manage to find an updated news text and its original version and both are published on the same day, we delete the original article as it does not contain the latest information.\n",
    "\n",
    "2. If the update and the original article are published on different days, we keep both news reports. The original article is important because it was published on the day market participants received the news. At the same time, the updated artilce might contain important new information. If there are several updates, we only keep the latest one as containing the full information. In case the latest update is very close to the original article, it will be removed later by the `fuzzy_duplicates` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:00:07.758560\n"
     ]
    }
   ],
   "source": [
    "from aktualisierung import delete_aktualisierung_index\n",
    "# select all \"Aktualisierung\" articles\n",
    "data_aktualisierung = data[data['title'].str.contains('\\((?:.*?)Aktualisierung(?:.*?)\\)', na = False)]\n",
    "dates = data_aktualisierung.groupby(['year', 'month', 'day'])\n",
    "dates = list(dates.groups)\n",
    "dates = np.array_split(dates, NUM_CORE)\n",
    "chunks = [pd.concat([data_aktualisierung[(data_aktualisierung['year'] == t[0]) & (data_aktualisierung['month'] == t[1]) & (data_aktualisierung['day'] == t[2])] for t in tup]) for tup in dates]\n",
    "# use 'delete_aktualisierung_index' to get list with indices of all outdated 'Aktualisierung' articles\n",
    "startTime = datetime.now()\n",
    "if __name__ == \"__main__\":\n",
    "    pool = mp.Pool(NUM_CORE)\n",
    "    aktualisierung_index = pool.map(delete_aktualisierung_index, chunks)\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "print(datetime.now()-startTime)\n",
    "aktualisierung_index = [idx for l in aktualisierung_index for idx in l]\n",
    "# delete outdated 'Aktualisierung' articles based on indices\n",
    "#data.drop(aktualisierung_index, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data.to_csv('dpa_prepro_step3.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filtering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately, dpa articles are not as consistently sorted into sections and subsections as articles from other \n",
    "news media. Instead, we investigate the most commonly used titles and keywords and remove irrelevant articles based on them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We exclude irrelevant articles based on the following titles\n",
    "\n",
    "* 1) Londoner Edelmetallpreise: Precious metal prices (without text)\n",
    "* 2) Tageskalender: List of upcoming events\n",
    "* 3) (Tabelle), TABELLE: : Tables in text form\n",
    "* 4) SPORT, Sport (except for the titles that contain TRANSPORT, INTERSPORT, PASSPORT, Sportartikel, news about sportswear manufacturers, sporting goods industry, SPORTARTIKLER, Sportmodelle, or Sportswear): news related to Sports. Beware that news about sports marketing agencies (e.g., Sportfive), sports media websites (e.g., Sportal), and new sports models of car manufacturers might be removed as well. \n",
    "* 5) KORREKTUR: Article corrections\n",
    "* 6) Impressum: Dpa contact data\n",
    "* 7) Testmeldung: Test-articles from dpa\n",
    "* 8) Kurse A, Kurse B, Kurse C, Kurse D, KURSE A, KURSE B, KURSE C, KURSE DREI, KURSE Drei, Kurse drei: Stock charts without text\n",
    "* 9) DGAP-DD: DGAP reports\n",
    "* 10) New Yorker Aktien-Schlußkurse: Stock closing prices at the New York stock exchange (articles only occur from 1997 to 2002)\n",
    "* 11) VERMISCHTES: Miscellaneous with no relation to economics\n",
    "* 12) Angekündigte US-Quartalszahlen auf einen Blick: Quarterly US figures\n",
    "* 13) Terminvorschau: Appointment preview\n",
    "* 14) Aus der Landespolitik: News reports on regional politics. We think they are unlikely to be important for our research question. \n",
    "* 15) Die Woche in Berlin, Die Woche in Bonn, Die politische Woche: Announcements of political (and economic) news for next week. The format is very different from other news reports and difficult for the models we use.\n",
    "* 16) Notizen aus der Politik, NOTIZEN AUS DER POLITIK: Collection of varios short political articles which contentwise  do not seem to be relevant for our research question and are covered only within the limited time period, 2002-2011.\n",
    "* 17) Presseschau (except for the titles that contain 'zu'): Press reviews that include only headlines of important news reports from various media outlets. We can not analyze the full texts and headlines together because different types of data require different models. Articles with headlines containing both 'Presseschau' and 'zu' are actual press reviews (not just headlines) of a single newspaper on a single topic.\n",
    "* 18) ÜBERBLICK: Analysten-Umstufungen: Changes in stock ratings (different article format).\n",
    "* 19) Chronologie, CHRONOLOGIE: Chronology - important dates and events. We remove these articles because for our research question backward-looking articles are arguable not that important.\n",
    "* 20) dpa-Landesdienst: Regional news unlikely to be important for our research question.\n",
    "* 21) Die Top-Themen am Aktienmarkt: Short news articles about stock market. They do contain relevant information, but they also have a very different format and they were first issued in 2011. \n",
    "* 22) Aktien Asien Schluss.: Quantitative information about the stock market.\n",
    "* 23) Börsentag auf einen Blick: Articles about the stock market, different format, a lot of quantitative information.\n",
    "* 24) US-Quartalszahlen vom Vortag: Quarterly stock market figures, tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4159132\n"
     ]
    }
   ],
   "source": [
    "# Filter out non-economic articles based on titles.\n",
    "fil_titles = '''Londoner Edelmetallpreise|Tageskalender|\\(Tabelle\\)|SPORT|Sport|KORREKTUR|Impressum|Testmeldung|Kurse A[^a-z]|Kurse B[^a-z]|Kurse C[^a-z]|Kurse D[^a-z]|KURSE A|KURSE B|KURSE C|KURSE DREI|Kurse drei|KURSE Drei|Kurse/drei|DGAP-DD|New Yorker Aktien-Schlusskurse|VERMISCHTES|Angekündigte US-Quartalszahlen|Terminvorschau|Aus der Landespolitik|Die Woche in Berlin|Die Woche in Bonn|Die politische Woche|Notizen aus der Politik|NOTIZEN AUS DER POLITIK|ÜBERBLICK: Analysten-Umstufungen|TABELLE:|Chronologie|CHRONOLOGIE|dpa-Landesdienst|Die Top-Themen am Aktienmarkt|Aktien Asien Schluss\\.|Börsentag auf einen Blick|US-Quartalszahlen vom Vortag'''\n",
    "titles_exc = '''TRANSPORT|INTERSPORT|PASSPORT|Sportartikel|SPORTARTIKLER|Sportmodelle|Sportswear'''\n",
    "data.drop(data[(data['title'].str.contains(fil_titles, na = False)) & (~data['title'].str.contains(titles_exc, na=False))].index, inplace=True)\n",
    "# Filter out press reviews that consist of headlines only\n",
    "data.drop(data[(data['title'].str.contains('Presseschau', na = False)) & (~data['title'].str.contains('zu', na=False))].index, inplace=True)\n",
    "data.reset_index(inplace=True, drop=True)\n",
    "print(len(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We exclude articles based on a title and a text:\n",
    "\n",
    "* 1) A title contains 'Überblick: ANALYSTEN-EINSTUFUNGEN', and a text contains 'Folgende Investmentbanken haben sich': Changes in stock ratings (different article format)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4156139\n"
     ]
    }
   ],
   "source": [
    "# Filter out articles based on a title and a text.\n",
    "data.drop(data[data.title.str.contains('Überblick: ANALYSTEN-EINSTUFUNGEN', na = False) & data.texts.str.contains('Folgende Investmentbanken haben sich', na=False)].index, inplace=True)\n",
    "data.reset_index(inplace=True, drop=True)\n",
    "print(len(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We exclude articles based on a title and keywords:\n",
    "\n",
    "* 1) A title contains 'Dispositionen', and keywords contain 'Wahl' or 'wahl': quantitative information about elections (different article format)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4155710\n"
     ]
    }
   ],
   "source": [
    "# Filter out articles based on a title and keywords.\n",
    "data.drop(data[data.title.str.contains('Dispositionen', na = False) & data.keywords.str.contains('Wahl|wahl', na=False)].index, inplace=True)\n",
    "data.reset_index(inplace=True, drop=True)\n",
    "print(len(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We exclude irrelevant articles based on the following sections.\n",
    "\n",
    "* 1) Tabelle: Tables in text form (some articles are still left after the previos step)\n",
    "* 2) Historisches: News about historical events\n",
    "* 3) Achtung: Announcemt of upcoming news"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4132700\n"
     ]
    }
   ],
   "source": [
    "# Filter out non-economic articles based on sections.\n",
    "fil_genres = '''Tabelle|Historisches|Achtung'''\n",
    "data.drop(data[data['genre'].str.contains(fil_genres, na = False)].index, inplace=True)\n",
    "data.reset_index(inplace=True, drop=True)\n",
    "print(len(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We exclude non-economic articles based on the following keywords.\n",
    "* 1) Redaktionshinweis: Editor's notes for Dpa journalists\n",
    "* 2) DGAP: DGAP reports\n",
    "* 3) Sport, SPORT, SPO (except for Sportartikel, this section contains articles on sports companies): Sport news (some sports articles are still left after the previos steps)\n",
    "* 4) Kurse A, Kurse B, Kurse C, Kurse D, KURSE A, KURSE B, KURSE C, Kurse D,\n",
    "     KURSE DREI, Kurse drei, KURSE Drei, KURSE drei, Kurse Drei, Kurse/drei: Stock charts without text (some articles are  still left after the previos steps)\n",
    "* 5) Tagesvorschau, Vorschau, VORSCHAU, vorschau: List of titles of upcoming news\n",
    "* 6) Bilderdienst: Dpa Picture Service\n",
    "* 7) Geschichte: News related to historical events\n",
    "* 8) Landespolitik: Regional news irrelevant for economic forecasting\n",
    "* 9) dpa-Morgenlage: News about what happened last night. These articles have a special format: quick and very short reports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3888952\n"
     ]
    }
   ],
   "source": [
    "# Filter out non-economic articles based on keywords.\n",
    "fil_keywords = '''Redaktionshinweis|DGAP|Sport|SPORT|SPO|Kurse A|Kurse B|Kurse C|Kurse D|KURSE A|KURSE B|KURSE C|KURSE DREI|Kurse drei|KURSE Drei|KURSE drei|Kurse Drei|Kurse/drei|Kurse D|Tagesvorschau|Vorschau|vorschau|VORSCHAU|Bilderdienst|Geschichte|Landespolitik|dpa-Morgenlage'''\n",
    "keywords_exc = '''Sportartikel'''\n",
    "data.drop(data[(data['keywords'].str.contains(fil_keywords, na = False)) & (~data['keywords'].str.contains(keywords_exc, na=False))].index, inplace=True)\n",
    "data.reset_index(inplace=True, drop=True)\n",
    "print(len(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We exclude articles based on the following bits of text.\n",
    "* 1) Schalterverkaufskurse: Precios metal prices\n",
    "* 2) dpa-news.de: News regarding the Dpa website\n",
    "* 3) Wirtschafts- und Finanztermine, Wirtschafts- und Finanz-Termine, Konjunktur- und Wirtschaftstermine: List of dates when economic data will be published/economic events will take place\n",
    "* 4) DGAP (except for articles that contain DGAP standing for Deutsche Gesellschaft für Auswärtige Politik): DGAP reports\n",
    "* 5) Bitte verwenden Sie diese Meldung nicht: Retracted articles\n",
    "* 6) \\( Wiederholung: Repeated articles\n",
    "* 7) [§] 26 Abs., § 15a WpHG 1, § 15 WpHG, Artikel 19 MAR, article 19 Market Abuse Regulation (MAR): Regulatory news\n",
    "* 8) Die Pivotpunkte für den Dax-Future: Pivot points for the Dax-Future\n",
    "* 9) An der Frankfurter Wertpapierbörse wurden, Die Aktien im Dow Jones EuroStoxx 50, Die Aktien im Dow Jones Euro Stoxx 50: Stock charts\n",
    "* 10) Ihr Ansprechpartner: Redaktion Politik International: List of current political news headlines\n",
    "* 11) (Achtung - Sonderdisposition): List of articles on a particular topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3688970\n"
     ]
    }
   ],
   "source": [
    "fill_text = '''Schalterverkaufskurse:|dpa-news\\.de|Wirtschafts- und Finanztermine|Wirtschafts- und Finanz-Termine|DGAP|Bitte verwenden Sie diese Meldung nicht|Konjunktur- und Wirtschaftstermine|\\(Wiederholung|[§] 26 Abs\\. 1|§ 15a WpHG|§ 15 WpHG|Artikel 19 MAR|article 19 Market Abuse Regulation \\(MAR\\)|Die Pivotpunkte für den Dax-Future|An der Frankfurter Wertpapierbörse wurden|Die Aktien im Dow Jones EuroStoxx 50|Die Aktien im Dow Jones Euro Stoxx 50|\\(Achtung - Sonderdisposition\\)'''\n",
    "text_exc = '''Auswärtige Politik'''\n",
    "data.drop(data[(data['texts'].str.contains(fill_text, na = False)) & (~data['texts'].str.contains(text_exc, na=False))].index, inplace=True)\n",
    "data.reset_index(inplace=True, drop=True)\n",
    "print(len(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to exclude irrelevant articles based on the following two sources.\n",
    "* 1) dpa-frei: Article corrections\n",
    "* 2) dpa-wahl: Articles about federal election results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3688940\n"
     ]
    }
   ],
   "source": [
    "data.drop(data[data['source'].str.contains('dpa-frei|dpa-wahl', na = False)].index, inplace=True)\n",
    "data.reset_index(inplace=True, drop=True)\n",
    "print(len(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We exclude articles regarding dpa itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3688935\n"
     ]
    }
   ],
   "source": [
    "data.drop(data[data['city'] == 'Die Deutsche Presse-Agentur'].index, inplace=True)\n",
    "data.reset_index(inplace=True, drop=True)\n",
    "print(len(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We keep articles on two subjects: Politics ('wi') and Economy ('wi'). Articles with the 'rs' value of the column 'rubrics' are removed, because 'rs' stands for editorial management. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3687766\n"
     ]
    }
   ],
   "source": [
    "data = data[(data['rubrics']==u'wi') | (data['rubrics']==u'pl')]\n",
    "data.reset_index(inplace=True, drop=True)\n",
    "print(len(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove four articles published in 2019. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3687762\n"
     ]
    }
   ],
   "source": [
    "data = data[data['year']<2019]\n",
    "data.reset_index(inplace=True, drop=True)\n",
    "print(len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data.to_csv('dpa_prepro_step4.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split up articles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes multiple articles are collected and merged into one entry. For example, articles with the title, keyword, or genre \n",
    "'Nachrichtenüberblick' are a collection of the most important articles of the day. Because these smaller articles\n",
    "can have different sentiments and topics, we separate articles that consist of multiple smaller articles. Articles consisting of multiple smaller articles can be identified with the following words which can appear in titles, keywords, or genres."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- dpa-Nachrichtenüberblick, Nachrichtenüberblick: An overview of news for the upcomming days or news which are a few days old\n",
    "- Kurznachrichten Wirtschaft: Collection of short economic news\n",
    "- Analysten-Einstufungen, ANALYSTEN-EINSTUFUNGEN: Analyst stock ratings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We delete one of the articles containing 'Nachrichtenüberblick' in the title because it has too many errors that make it difficult to split the article into paragraphs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' PARAGRAPH     Rentenreform verabschiedet\\n  PARAGRAPH     BERLIN - Der Weg für die Förderung der privaten Altersvorsorge\\nmit knapp 21 Mrd. DM ist frei.  e K\\n B h e i r t de. Auch das rot-rot-regierte Mecklenburg-Vorpommern\\nstimmte zu. Die SPD-CDU-Koalition von Bremen enthielt sich. In\\nMecklenburg-Vorpommern droht nach der überraschenden Zustimmung von\\nMinisterpräsident Ringstorff eine Koalitionskrise. Nach PDS-Ansicht\\nhat die SPD den Koalitionsvertrag gebrochen und damit den\\nRegierungspartner herausgefordert.\\n  PARAGRAPH    Mieter erhalten mehr Rechte\\n  PARAGRAPH    BERLIN - Die mehr als 20 Mio. Mieter in Deutschland erhalten von\\nSeptember an mehr Rechte. Die Grenze für Mieterhöhungen wird gesenkt\\nund die Kündigungsfristen werden zu Gunsten der Mieter geändert. Das\\nsieht die Mietrechtsreform der Bundesregierung vor, die am Freitag im\\nBundesrat die letzte parlamentarische Hürde nahm. Bisher betrugen die\\nKündigungsfristen für Mieter un d Vermieter gleichermaßen maximal\\nzwölf Monate. Im Interesse beruflicher Mobilität, aber auch für den\\nFall d     i   u   b  Für den\\nVermieter beträgt sie künftig maximal neun Monate.\\n  PARAGRAPH    Vertreter von NS-Zwangsarbeitern hoffen auf schnelle\\nEntschädigung\\n  PARAGRAPH    WARSCHAU/PRAG/MOSKAU - Nach der Zurückweisung der letzten\\nSammelklage in den USA hoffen Opfervertreter und Politiker in Mittel-\\nund Osteuropa auf eine möglichst schnelle Entschädigung der\\nehemaligen NS-Zwangsarbeiter. Auf die Auszahlung der 10 Mrd. DM aus\\ndem deutschen Entschädigungsfonds warten mehr als 1,5 Mio. NS-Opfer.\\nDavon leben eine Million auf dem Gebiet der ehemaligen Sowjetunion\\nund rund eine halbe Million in Polen. Vor der Auszahlung der Gelder\\nmuss der Bundestag noch formell feststellen, dass für die deutschen\\nUnternehmen Rechtssicherheit besteht.\\n  PARAGRAPH    Einheits-Regierung in Mazedonien immer noch fraglich\\n  PARAGRAPH    SKOPJE - Das Ringen um eine große Koalition der Volksgruppen in\\nMazedonien ist am Freitag weitergegangen.  a\\n n d n i e n sgruppe. Der mazedonische\\nMinisterpräsident Georgievski warnte die Albaner, dass das\\ninternational unterstützte Regierungsprojekt notfalls auch ohne ihre\\nPartei beginnen werde.\\n  PARAGRAPH    Nach Granaten-Angriff teilt Armee erneut Gazastreifen\\n  PARAGRAPH    GAZA/KAIRO - Mit einer Blockade mehrerer Straßen haben\\nisraelische Truppen heute den Gazastreifen praktisch geteilt und sind\\nerneut in autonomes Palästinensergebiet eingedrungen. Dabei\\nzerstörten Panzer und schwere Räumgeräte mehrere Gebäude der\\npalästinensischen Sicherheitskräfte. Zuvor hatten militante\\nPalästinenser Handgranaten auf zwei israelische Soldaten geworfen.\\n  PARAGRAPH    EU-Ausschuss wegen Abhörsystem von Amerikanern abgewiesen\\n  PARAGRAPH    WASHINGTON - Eine Delegation des Europäischen Parlaments, die\\nsich in den USA über das umstrittene Abhörsystem Echelon informierten\\nwollte, ist von der US-Regierung und dem amerikanischen Gehe I   u h\\n t l o ieben wird. Die\\nEuropaparlamentarier protestierten energisch gegen die kurzfristige\\nAbsage der abgesprochenen Treffen durch die Amerikaner./DP/ar\\n '"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The article to delete\n",
    "file_drop = ['8180362.xml']\n",
    "ind = data[data.file == '8180362.xml']['texts'].index[0]\n",
    "data[data.file == '8180362.xml']['texts'][ind]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.drop(data[data['file'].isin(file_drop)].index, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While testing the code, we have decided to delete a quantitative part of the following two articles:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' PARAGRAPH     BERLIN(dpa-AFX) - Mit dem Bundeshaushalt 2004 und dem Finanzplan\\nbis 2007 bleibt die von der Bundesregierung geplante Neuverschuldung\\nvorerst im kritischen Bereich. Sie soll jetzt im kommenden Jahr 28,8\\nMilliarden Euro betragen, nachdem Finanzminister Hans Eichel (SPD)\\nund die Koalition übereingekommen sind, 2 Milliarden Erlöse aus dem\\nVerkauf weiterer Telekom- <DTE.ETR> und Postaktien <DPW.ETR> an die\\nKreditanstalt für Wiederaufbau zu erzielen. Diese soll das Paket\\nspäter bei guten Kursen an der Börse platzieren.\\nFür das laufende Jahr hat Eichel inzwischen eine Neuverschuldung\\nvon 35 Milliarden in den Haushaltsentwurf eingestellt, spricht aber\\nbereits von einer Verdoppelung der zunächst gesetzlich\\nverabschiedeten 18,9 Milliarden Euro. Dies wären etwa 38 Milliarden\\nneue Schulden. Die im Herbst erwartete Höhe soll dann in einen\\nNachtragshaushalt aufgenommen werden. Andere Fachleute der Koalition\\nsprechen schon von rund 40 Milliarden Euro. Die Opposition nennt\\nunter Berücksichtigung der höchst unsicheren Bundesratsentscheidungen\\nüber einzelne Sparvorhaben Eichels sogar eine Neuverschuldung bis 50\\nMilliarden Euro.\\nDie weitere Haushaltsentwicklung hängt wesentlich vom Gelingen der\\nReformen und den Steuereinnahmen ab. Bis 2007 möchte Eichel die neuen\\nSchulden auf 10 Milliarden herunterfahren. Hier die Eckwerte des\\nFinanzplans bis 2007:\\n\\nIstSollVollzugEntwurfFinanzplan\\n2002200320032004200520062007\\n=====================================================================\\nAusgaben249,3248,2257251,2251,2251,2254,9\\nSteuereinnahmen192,0203,3196201,4211,8221,4229,9\\nsonstige Einnahmen25,426,02620,918,414,815,0\\nNettokreditaufnahme 31,918,935  28,821,015,010,0\\nInvestitionen24,126,726,724,824,724,724,7\\n° /wb/DP/aa\\n'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ind1 = data[data.file == '8833807.xml']['texts'].index[0]\n",
    "data[data.file == '8833807.xml']['texts'][ind1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' PARAGRAPH     BERLIN (dpa-AFX) - Der Entwurf des Bundeshaushalts 2004 sieht\\nAusgaben in Höhe von 251,2 Milliarden Euro vor. Das sind 1,2 Prozent\\nmehr als für das laufende Jahr gesetzlich geplant, aber 2,3 Prozent\\nweniger als das Finanzministerium jetzt für 2003 mit 257 Milliarden\\nEuro erwartet. Die neuen Schulden sind mit 28,8 Milliarden Euro\\neingeplant.\\nDas größte Ausgabevolumen verwaltet das Ressort für Gesundheit und\\nSoziales. Die hohe Steigerungsrate im Arbeits- und Wirtschaftsetat\\nvon Minister Wolfgang Clement (SPD) wird durch den hohen Zuschuss an\\ndie Bundesanstalt für Arbeit von 5,2 Milliarden Euro bestimmt.\\n\\nDie Übersicht über die Einzelpläne:\\nSollEntwurfVeränderung\\n20032004zum Vorjahr\\n(in Millionen Euro)(Prozent)\\n--------------------------------------\\nBundespräsidialamt20,4722,96+ 12,2\\nBundestag540,73548,93+1,5\\nBundesrat17,0617,80+4,3\\nBundeskanzleramt1.483,561.488,06+0,3\\nAuswärtiges Amt2.229,912.183,40-2,1\\nInnen4.014,004.092,58+2,0\\nJustiz345,35344,27-0,3\\nFinanzen3.286,623.338,03+1,6\\nWirtschaft und Arbeit18.508,1925.003,35+ 35,0\\nVerbraucherschutz und Agrar5.627,195.209,10-7,4\\nVerkehr, Bau und Wohnungswesen26.069,1026.491,84+1,6\\nVerteidigung24.378,7824.248,81-0,5\\nGesundheit und Soziales82.033,3181.882,49-0,2\\nUmwelt und Reaktorsicherheit794,02791,41-0,3\\nFamilie, Senioren, Jugend5.101,394.746,13-7,0\\nBundesverfassungsgericht16,2117,27+6,6\\nBundesrechnungshof75,2390,26+ 20,0\\nWirtschaftliche Zusammenarbeit3.767,543.800,00+0,9\\nBildung und Forschung8.364,228.209,19-1,8\\nBundesschuld39.940,1539.935,24+/- 0\\nVersorgung8.806,028.981,01+2,0\\nAllgemeine Finanzverwaltung12.779,989.757,86- 23,6\\n-------------------------------------\\ninsgesamt248.199,00251.200,00+1,2\\n=====================================°\\n/wb/DP/aa\\n'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ind2 = data[data.file == '8833809.xml']['texts'].index[0]\n",
    "data[data.file == '8833809.xml']['texts'][ind2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.at[ind1, 'texts'] = data['texts'][ind1].split(' Hier die Eckwerte')[0]\n",
    "data.at[ind2, 'texts'] = data['texts'][ind2].split('\\n\\nDie Übersicht')[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_mult_art = '''dpa-Nachrichtenüberblick|Nachrichtenüberblick|Kurznachrichten Wirtschaft|Analysten-Einstufungen|ANALYSTEN-EINSTUFUNGEN'''\n",
    "mult_art = data[data['title'].str.contains(s_mult_art, na = False)]\n",
    "mult_art = mult_art.append(data[data['keywords'].str.contains(s_mult_art, na = False)])\n",
    "mult_art = mult_art.append(data[data['genre'].str.contains(s_mult_art, na = False)])\n",
    "mult_art.drop_duplicates(['texts'], keep = 'first', inplace=True)\n",
    "mult_art.reset_index(inplace=True, drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete 'mult_art' from the original data\n",
    "data.drop(data[data['title'].str.contains(s_mult_art, na = False)].index, inplace=True)\n",
    "data.drop(data[data['keywords'].str.contains(s_mult_art, na = False)].index, inplace=True)\n",
    "data.drop(data[data['genre'].str.contains(s_mult_art, na = False)].index, inplace=True)\n",
    "data.reset_index(inplace=True, drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate chunck size \n",
    "chunk_size = int(mult_art.shape[0]/NUM_CORE)\n",
    "\n",
    "# split data into chunks \n",
    "chunks = [mult_art.iloc[mult_art.index[i:i + chunk_size]] for \n",
    "          i in range(0, mult_art.shape[0], chunk_size)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:01:44.853607\n",
      "922184\n"
     ]
    }
   ],
   "source": [
    "# split up articles into smaller articles and append the resulting new articles \n",
    "# to the corpus\n",
    "from datetime import datetime\n",
    "startTime = datetime.now()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    pool = mp.Pool(NUM_CORE)\n",
    "    results = pool.map(split_articles.split_articles, chunks) \n",
    "    pool.close()\n",
    "    pool.join()\n",
    "    \n",
    "print(datetime.now()-startTime)\n",
    "\n",
    "results = pd.concat(results)\n",
    "print(len(results))\n",
    "results.reset_index(inplace=True, drop=True) # reset the index of the DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The separated articles consist of fewer words than the articles from which they originally stemmed. Therefore, we count the number of words of the new articles with the count_words_mp function from before and filter out articles with less than 100 words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:00:06.228115\n"
     ]
    }
   ],
   "source": [
    "# count the number of words for the separated articles and filter out articles with less\n",
    "# than 100 words\n",
    "startTime = datetime.now()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    pool = mp.Pool(NUM_CORE)\n",
    "    count_results = pool.map(count_words_mp.count_words_mp, [text for text in results['texts']]) \n",
    "    pool.close()\n",
    "    pool.join()\n",
    "    \n",
    "print(datetime.now()-startTime)\n",
    "results['word_count'] = count_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "results['word_count'] = count_results\n",
    "results = results[results['word_count']>=100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3669146\n"
     ]
    }
   ],
   "source": [
    "# append separated articles to corpus\n",
    "data = data.append(results)\n",
    "data = data.sort_values(['year', 'month', 'day'], ascending=[True, True, True]) # sort the data in chronological order\n",
    "data.reset_index(inplace=True, drop=True) # reset the index of the DataFrame\n",
    "print(len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data.to_csv('dpa_prepro_step5.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### dpa-Basisdienstes and dpa-afx Wirtschaftsnachrichten"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Up to this point, we have been cleaning data from dpa-Basisdienstes (hereafter dpa) and dpa-afx Wirtschaftsnachrichten (hereafter afx) together. This made us realise that afx data has a completely different format and, while useful for forecasting, in our view should be analyzed separately from dpa data. Here we explain why."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dpa news articles are very similar to the news we read in other newspapers, and they cover similar topics, namely major political and economic events.\n",
    "\n",
    "The afx news, on the other hand, is aimed at investors and traders rather than the general public, and closely follows stock market developments, companies' financial results, and the state of key industries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make it clear what we mean, here are a few articles on topics that are frequently discussed in afx data.\n",
    "\n",
    "1. The first two articles discuss stock market developments.\n",
    "2. Articles three and four represent Ad hoc messages (information reported by companies that might influence the price of their securitites).\n",
    "3. Articles five and six give recommendations to traders whether they should sell, buy, or hold shares.\n",
    "4. The last two articles talk about financial results and plans of companies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- Articles on the stock market ----\n",
      "\n",
      "\n",
      "Aktien Neuer Markt: Kurse abgebröckelt - Etwas Erleichterung durch US -Daten. FRANKFURT (dpa-AFX) - Die deutschen Wachstumsaktien am Neuen Markt sind nach einer freundlichen Eröffnung stetig abgebröckelt. Im Vorfeld der mit Spannung erwarteten US-Konjunkturzahlen am Nachmittag (ab 14:30) wurden neue Tagestiefs markiert, bevor die Daten «in line» für etwas Erleichterung sorgten. Der Nemax-All-Share-Index <NMDK.ETR> lag zuletzt um 1,08% unter dem Vortagesniveau bei 3.231,70/-35,13 Punkten, während der Nemax50-Index <NMKX.ETR> 1,23% auf 4.068,60/-50,94 Punkte verlor. Vor den Zahlen saßen «alle wie das Kaninchen vor der Schlange und warteten», erklärte ein Händler aus Düsseldorf. Offenbar rechneten einige Marktteilnehmer mit negativen Reaktionen der Märkte und gaben Stücke bei sehr geringer Handelstätigkeit aus der Hand. Dies habe den Gesamtmarkt im frühen Verlauf unter Druck gesetzt. Gleich nach den ersten Zahlen um 14:30 Uhr, bei denen die US-Verbraucherpreise im Juli mit einem Anstieg u\n",
      "\n",
      "\n",
      "Aktien Frankfurt Schluss: Mit moderatem Plus --3(Karstadt, Linde, Kam ps). (Fortsetzung) - Zu den wenigen Verlierern im Dax zählte heute der Kaufhauskonzern Karstadt <KAR.ETR>. Die Aktie büßte 7,99 Euro auf 461,50 Euro ein. Die Münchener Investmentbank Merck Finck & Co hatte Karstadt als «Marketperformer» bestätigt und geäußert, die Ankündigungen des Einzelhandelskonzerns seien nach wie vor «nicht sehr transparent». Ebenfalls im Minus Linde <LIN.ETR>, die 90 Cent auf 57,10 Euro abgaben. Das Unternehmen hatte am Nachmittag mitgeteilt, die Ems-Syntech übernehmen zu wollen. Finanzielle Einzelheiten wurden nicht genannt. Linde werde zunächst 80% der Ems-Syntech akquirieren, die restlichen 20% dann innerhalb der nächsten zwei Jahre. Die Aktie der Metallgesellschaft <MET.ETR> konnte zuletzt 11 Cent auf 20,90 Euro zulegen. Auf der außerordentlichen Hauptversammlung der mg stimmten heute Nachmittag 99,9% der anwesenden oder vertretenen Aktionäre für die Fusion mit der Gea AG. Gea-Vorzüge <GEA3\n",
      "\n",
      "\n",
      "---- Ad hoc messages ----\n",
      "\n",
      "\n",
      "Ad hoc-Service Computec Media AG <CMD > Teil 2. (Fortsetzung) - durchschnittlich zwei Internet-Spiele pro Jahr erweitern. Business Unit CD-PUBLISHING Innerhalb der Business Unit CD-PUBLISHING wurde im 9-Monats-Zeitraum ein Umsatz von 415 TDM und ein Ergebnis der operativen Tätigkeit von -237 TDM erzielt. Das Geschäft mit Erweiterungssoftware zu populären Spielen blieb damit deutlich hinter den Erwartungen zurück. Die Geschäftsentwicklung wurde dabei durch den Wechsel des Vertriebspartners und die damit verbundenen Verzögerungen in der Veröffentlichung von Produkten beeinträchtigt. Mehrere Softwaretitel konnten erst in der schwachen Frühling-/Sommer-Saison veröffentlicht werden und erreichten deswegen nicht den geplanten Absatz. In den Folgequartalen rechnet COMPUTEC MEDIA in diesem Segment jedoch wieder mit deutlich ansteigenden Umsatzerlösen. Der vollständige Quartalsbericht steht ab sofort auf der Website des Unternehmens unter www.computec.de/Investor Relations/Berichte und Präsenta\n",
      "\n",
      "\n",
      "Ad hoc-Service Austria Tabak AG <ATK > --2. 1893 Date: 19990820 Time: 08530630 ISIN: AT0000619200 Market: VIE Number: 5048 SubNumber: 02 Headline: Ad hoc-Service: Austria Tabak AG <ATK > deutsch n Geschäftsbe- reich Großhandel um rund zehn Prozent verbessern. Ohne Berücksichtigung von Amortisationsquoten betreffend den sich aus den Fusionen ergebenden Goodwill wird sich das Periodenergebnis bzw. der Jahresüberschuß 1999 - jeweils nach Minderheitsanteilen - im Großhandel jedoch nicht wesentlich verändern. Die Akquisition in Schweden wird im Geschäftsbereich Tabakindustrie auf Jahresbasis das Absatzvolumen um rund 30 Prozent und die Kennzahl EBITDA um ca. 65 Prozent erhöhen. Dieser operativen Ergebnisverbesserung, die sich im 2. Halbjahr 1999 voll in der Konzernrechnung niederschlagen wird, stehen jedoch Goodwill-Abschreibungen und Finanzierungskosten gegenüber. Nennens- werte Auswirkungen auf das Jahresergebnis 1999 werden aufgrund von Einmal- kosten im Anfangsjahr nicht erwartet. Vom A\n",
      "\n",
      "\n",
      "---- Stock market analysis, recommendations for traders ----\n",
      "\n",
      "\n",
      "AKTIE IM FOKUS/Clariant mit Gewinnen nach BTP-Übernahmeofferte. ZÜRICH (dpa-AFX) - Die Aktien der Clariant AG <CLN.ZRH> reagieren mit Gewinnen auf die von beiden involvierten Partnern bestätigte Übernahmeofferte für die britische BTP PLC. Laut den Verlautbarungen unterbreitet Clariant für die BTP eine Cash-Übernahmeofferte in der Höhe von 600 Pence pro BTP-Aktie, was gesamthaft einen Übernahmepreis von 1,08 Mrd GBP ergibt. Um 09.15 Uhr notiert die Aktie 6 CHF bzw. 0,82 Prozent höher auf 734 CHF. Der SMI gewinnt 0,4 Prozent auf 7.264,70 Zähler. Laut einem Handelsanalysten einer Schweizer Grossbank sei die angekündigte Übernahme in strategischer Hinsicht bestimmt ein guter Deal für Clariant, dies gelte sowohl hinsichtlich geographischer als auch produktespezifischer Überlegungen. Hingegen sei der Übernahmepreis relativ hoch. Die Eigenkapitalquote von Clariant werde bestimmt deutlich sinken und sehrwahrscheinlich müsse Clariant schon bald auf dem Kapitalmarkt aktiv werden. «Qualität ist h\n",
      "\n",
      "\n",
      "AKTIE IM FOKUS: Aus Mannesmann-Aktie weicht nach Übernahmeschlacht di e Luft. DÜSSELDORF (dpa-AFX) - Angesichts einer möglichen Einigung in der Übernahmeschlacht um Mannesmann <MMN.ETR> durch Vodafone <VOD.ISE> weicht aus der steil gestiegenen Aktie des Düsseldorfer Telekommunikationskonzerns die Luft. Zwar stieg das Papier am Frankfurter Aktienmarkt am Donnerstag zunächst noch um 4,3 Prozent auf 340 Euro. Die Aktie fiel dann aber unter 300 Euro zurück. Sie lag um 13.58 Uhr bei 306 Euro und damit 5,84 Prozent tiefer als am Vortag. Rund 1,76 Milliarden Aktien wurden bis zum frühen Nachmittag gehandelt. Bei Bekanntwerden der Übernahmenpläne von Vodafone am 22. Oktober 1999 hatte die Mannesmann-Aktie mit 144 Euro notiert. Die Vodafone-Aktie büßte am Donnerstag in Frankfurt 6,79 Prozent oder 0,43 Euro auf 5,90 Euro ein. Von ihr wurden rund 3,72 Millionen Papiere gehandelt. «Derzeit gehen einige Portfolio-Manager aus Mannesmann raus, weil sie keine Vodafone-Aktien halten dürfen», berichtete\n",
      "\n",
      "\n",
      "---- News on companies ----\n",
      "\n",
      "\n",
      "ROUNDUP: Siemens will beim Umsatz in 1999/00 einstellig zulegen. MÜNCHEN (dpa-AFX) - Für das angelaufene Geschäftsjahr 1999/00 erwartet der Siemens-Konzern <SIE.ETR> ein einstelliges Umsatzwachstum und ein darüber liegendes Ergebniswachstum. Aufgrund wirksam werdender Desinvestitionen sollen Auftragseingang und Umsatz allerdings rechnerisch unter dem jeweiligen Vorjahreswert liegen, sagte Konzernchef Heinrich von Pierer am Donnerstag auf der Bilanzpressekonferenz in München. Aus den bislang getätigten Desinvestitionen erwartet Siemens für das laufende Geschäftsjahr 1999/00 nach Angaben von Finanzchef Heinz-Joachim Neubürger außerordentliche Nachsteuererträge von mindestens 3 Mrd. DM. «Die genaue Höhe der Sondererträge hängt vor allem vom Infineon-Börsengang ab,» sagte von Pierer. Von den insgesamt im Rahmen des 10-Punkte-Programms vom Sommer 98 geplanten Desinvestitionen sei bereits ein Teil abgeschlossen oder grundsätzlich vereinbart. Hier nannte von Pierer den Verkauf der Sparte Elek\n",
      "\n",
      "\n",
      "ROUNDUP: Intel erwartet im ersten Quartal Umsatzminus - Lagerbestände. FRANKFURT (dpa-AFX) - Die Intel Corp. <INTC.NAS> erwartet im ersten Quartal 2000 einen Umsatz leicht unter dem des Vorquartals. Das Unternehmen nannte als Grund dafür am Donnerstagabend zwar saisonale Faktoren. Die Aussage dürfte aber Investoren enttäuscht haben, die auf einem Chip-Boom nach dem Jahr-2000-Problem gehofft hatten. Intel-Finanzchef Andy Bryant sagte in einer Telefonkonferenz mit Analysten immerhin, der Umsatzrückgang werde im ersten Quatals 2000 nicht so stark sein wie im Vorjahreszeitraum, als der Umsatz des ersten Quartals 1999 gegenüber dem des vierten Quartals 1998 um sieben Prozent zurückging. Die Bruttomarge sollte im ersten Quartal im Vergleich zur Marge von 61,3% im vierten Quartal nahezu unverändert bleiben. Im Gesamtjahr 2000 werde sie im Mittel 61% «plus oder minus ein paar Punkte» erreichen, sagte Bryant. 1999 lag sie bei 59,7%. Analysten erwarten eine festere Notiz der Aktie am Freitag. Al\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Stock_market = data[(data.texts.str.contains('^Aktien')) & (data.topic == 'afx')]\n",
    "Ad_hoc = data[(data.texts.str.contains('Ad hoc-Service')) & (data.topic == 'afx')]\n",
    "Analysis = data[(data.texts.str.contains('^AKTIE IM FOKUS')) & (data.topic == 'afx')]\n",
    "Companies = data[(data.texts.str.contains('ROUNDUP')) & (data.topic == 'afx')]\n",
    "\n",
    "print('---- Articles on the stock market ----')\n",
    "print('\\n')\n",
    "print(Stock_market.reset_index()['texts'][0][:1000])\n",
    "print('\\n')\n",
    "print(Stock_market.reset_index()['texts'][100][:1000])\n",
    "print('\\n')\n",
    "print('---- Ad hoc messages ----')\n",
    "print('\\n')\n",
    "print(Ad_hoc.reset_index()['texts'][0][:1000])\n",
    "print('\\n')\n",
    "print(Ad_hoc.reset_index()['texts'][1][:1000])\n",
    "print('\\n')\n",
    "print('---- Stock market analysis, recommendations for traders ----')\n",
    "print('\\n')\n",
    "print(Analysis.reset_index()['texts'][0][:1000])\n",
    "print('\\n')\n",
    "print(Analysis.reset_index()['texts'][1][:1000])\n",
    "print('\\n')\n",
    "print('---- News on companies ----')\n",
    "print('\\n')\n",
    "print(Companies.reset_index()['texts'][0][:1000])\n",
    "print('\\n')\n",
    "print(Companies.reset_index()['texts'][10][:1000])\n",
    "print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overall, we can conclude that these types of articles are of interest to financial market participants, but may be too detailed for the general public.\n",
    "\n",
    "A few exceptions, which we also consider, are afx articles on business cycle (Konjunktur) and Politics.\n",
    "\n",
    "* Articles on business cycle might be very important for economic forecasting. We therefore considered including news reports with the keyword 'Konjunktur' (business cycle) published by afx. However, we encountered two problems.\n",
    "\n",
    "* First, articles with the keyword 'Konjunktur' aimed at the general public often enter our database twice because they are published by both dpa and afx. Articles published exclusively by afx are often more quantitative and seem to target financial market participants. \n",
    "\n",
    "* Second, while the proportion of dpa articles with the keyword 'Konjunktur' is relatively stable and ranges from 0.5% to 4% (see 'dpa' column in the dataframe below), the proportion of 'Konjunktur' articles in afx data increases significantly from 3% in 2000 to 17% in 2018 (see 'afx' column). We believe this is due to the fact that in the afx data, the keyword 'Konjunktur' was initially used only in the articles with a lot of quantitative information, and then began to be used in a much wider range of articles. In any case, if we included all dpa articles and afx articles with the keyword 'Konjunktur', we would see that the share of articles with the keyword 'Konjunktur' increases over the years (see 'All' column), most likely due to structural changes in the database rather than economic developments. Therefore, we decided to exclude afx articles with the keyword 'Konjunktur' from further analysis.\n",
    "\n",
    "* We also exclude afx articles with the keyword 'Politik', because in most cases dpa publishes the same articles. Thus, these articles can be considered as duplicates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>year</th>\n",
       "      <th>All</th>\n",
       "      <th>dpa</th>\n",
       "      <th>afx</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1991</td>\n",
       "      <td>0.005292</td>\n",
       "      <td>0.005292</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1992</td>\n",
       "      <td>0.006772</td>\n",
       "      <td>0.006772</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1993</td>\n",
       "      <td>0.008648</td>\n",
       "      <td>0.008648</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1994</td>\n",
       "      <td>0.007026</td>\n",
       "      <td>0.007026</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1995</td>\n",
       "      <td>0.006377</td>\n",
       "      <td>0.006377</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1996</td>\n",
       "      <td>0.007515</td>\n",
       "      <td>0.007515</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1997</td>\n",
       "      <td>0.007414</td>\n",
       "      <td>0.007414</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1998</td>\n",
       "      <td>0.009876</td>\n",
       "      <td>0.009876</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1999</td>\n",
       "      <td>0.007539</td>\n",
       "      <td>0.007539</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2000</td>\n",
       "      <td>0.018935</td>\n",
       "      <td>0.008570</td>\n",
       "      <td>0.026061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2001</td>\n",
       "      <td>0.052003</td>\n",
       "      <td>0.017722</td>\n",
       "      <td>0.074072</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2002</td>\n",
       "      <td>0.069401</td>\n",
       "      <td>0.011531</td>\n",
       "      <td>0.103539</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2003</td>\n",
       "      <td>0.074112</td>\n",
       "      <td>0.016428</td>\n",
       "      <td>0.099043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2004</td>\n",
       "      <td>0.078548</td>\n",
       "      <td>0.013996</td>\n",
       "      <td>0.097797</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2005</td>\n",
       "      <td>0.076451</td>\n",
       "      <td>0.014536</td>\n",
       "      <td>0.092030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2006</td>\n",
       "      <td>0.067660</td>\n",
       "      <td>0.011156</td>\n",
       "      <td>0.082000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>2007</td>\n",
       "      <td>0.069656</td>\n",
       "      <td>0.010739</td>\n",
       "      <td>0.080005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2008</td>\n",
       "      <td>0.091461</td>\n",
       "      <td>0.022542</td>\n",
       "      <td>0.091994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>2009</td>\n",
       "      <td>0.108142</td>\n",
       "      <td>0.037187</td>\n",
       "      <td>0.099638</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>2010</td>\n",
       "      <td>0.114165</td>\n",
       "      <td>0.017607</td>\n",
       "      <td>0.122001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>2011</td>\n",
       "      <td>0.112538</td>\n",
       "      <td>0.015565</td>\n",
       "      <td>0.122455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>2012</td>\n",
       "      <td>0.140459</td>\n",
       "      <td>0.015451</td>\n",
       "      <td>0.152895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>2013</td>\n",
       "      <td>0.137155</td>\n",
       "      <td>0.018079</td>\n",
       "      <td>0.142732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>2014</td>\n",
       "      <td>0.128538</td>\n",
       "      <td>0.020674</td>\n",
       "      <td>0.133688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>2015</td>\n",
       "      <td>0.147958</td>\n",
       "      <td>0.015360</td>\n",
       "      <td>0.145856</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>2016</td>\n",
       "      <td>0.139478</td>\n",
       "      <td>0.013722</td>\n",
       "      <td>0.141100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>2017</td>\n",
       "      <td>0.158985</td>\n",
       "      <td>0.011881</td>\n",
       "      <td>0.147736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>2018</td>\n",
       "      <td>0.189317</td>\n",
       "      <td>0.012732</td>\n",
       "      <td>0.168730</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    year       All       dpa       afx\n",
       "0   1991  0.005292  0.005292       NaN\n",
       "1   1992  0.006772  0.006772       NaN\n",
       "2   1993  0.008648  0.008648       NaN\n",
       "3   1994  0.007026  0.007026       NaN\n",
       "4   1995  0.006377  0.006377       NaN\n",
       "5   1996  0.007515  0.007515       NaN\n",
       "6   1997  0.007414  0.007414       NaN\n",
       "7   1998  0.009876  0.009876       NaN\n",
       "8   1999  0.007539  0.007539       NaN\n",
       "9   2000  0.018935  0.008570  0.026061\n",
       "10  2001  0.052003  0.017722  0.074072\n",
       "11  2002  0.069401  0.011531  0.103539\n",
       "12  2003  0.074112  0.016428  0.099043\n",
       "13  2004  0.078548  0.013996  0.097797\n",
       "14  2005  0.076451  0.014536  0.092030\n",
       "15  2006  0.067660  0.011156  0.082000\n",
       "16  2007  0.069656  0.010739  0.080005\n",
       "17  2008  0.091461  0.022542  0.091994\n",
       "18  2009  0.108142  0.037187  0.099638\n",
       "19  2010  0.114165  0.017607  0.122001\n",
       "20  2011  0.112538  0.015565  0.122455\n",
       "21  2012  0.140459  0.015451  0.152895\n",
       "22  2013  0.137155  0.018079  0.142732\n",
       "23  2014  0.128538  0.020674  0.133688\n",
       "24  2015  0.147958  0.015360  0.145856\n",
       "25  2016  0.139478  0.013722  0.141100\n",
       "26  2017  0.158985  0.011881  0.147736\n",
       "27  2018  0.189317  0.012732  0.168730"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The proportion of articles with the keyword 'Konjunktur' in the whole dataset.\n",
    "Konjunktur_all_series = data[(data.keywords.str.contains('Konjunktur'))].groupby('year').nunique()['texts']/data[(data.topic == 'WiPo') | ((data.topic == 'afx') & (data.keywords.str.contains('Konjunktur')))].groupby('year').nunique()['texts']\n",
    "Konjunktur_all_df = Konjunktur_all_series.to_frame().reset_index()\n",
    "Konjunktur_all_df = Konjunktur_all_df.rename(columns = {\"texts\": \"All\"})\n",
    "# The proportion of articles with the keyword 'Konjunktur' in dpa dataset.\n",
    "Konjunktur_dpa_series = data[(data.keywords.str.contains('Konjunktur')) & (data.topic == 'WiPo')].groupby('year').nunique()['texts']/data[(data.topic == 'WiPo')].groupby('year').nunique()['texts']\n",
    "Konjunktur_dpa_df = Konjunktur_dpa_series.to_frame().reset_index()\n",
    "Konjunktur_dpa_df = Konjunktur_dpa_df.rename(columns = {\"texts\": \"dpa\"})\n",
    "# The proportion of articles with the keyword 'Konjunktur' in afx dataset.\n",
    "Konjunktur_afx_series = data[(data.keywords.str.contains('Konjunktur')) & (data.topic == 'afx')].groupby('year').nunique()['texts']/data[(data.topic == 'afx')].groupby('year').nunique()['texts']\n",
    "Konjunktur_afx_df = Konjunktur_afx_series.to_frame().reset_index()\n",
    "Konjunktur_afx_df = Konjunktur_afx_df.rename(columns = {\"texts\": \"afx\"})\n",
    "# The dataframe with the above mentioned datafames merged together.\n",
    "merged_df = Konjunktur_all_df.merge(Konjunktur_dpa_df, how = 'left', on = 'year')\n",
    "merged_df = merged_df.merge(Konjunktur_afx_df, how = 'left', on = 'year')\n",
    "merged_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All in all, we only keep articles published by dpa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2315579\n"
     ]
    }
   ],
   "source": [
    "data = data[data.topic == 'WiPo']\n",
    "data = data.sort_values(['year', 'month', 'day'], ascending=[True, True, True]) # sort the data in chronological order\n",
    "data.reset_index(inplace=True, drop=True) # reset the index of the DataFrame\n",
    "print(len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_csv('dpa_prepro_step6.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('dpa_prepro_step6.csv', encoding = 'utf-8', index_col = 0,  keep_default_na=False,\n",
    "                   dtype = {'rubrics': 'str', \n",
    "                            'source': 'str',\n",
    "                            'keywords': 'str',\n",
    "                            'title': 'str',\n",
    "                            'city': 'str',\n",
    "                            'genre': 'str',\n",
    "                            'wordcount': 'str'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Identify and Delete English Articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1:48:39.715753\n"
     ]
    }
   ],
   "source": [
    "# Delete all English articles from the data  \n",
    "startTime = datetime.now()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    pool = mp.Pool(NUM_CORE)\n",
    "    eng_results = pool.map(identify_eng_2.identify_eng_2, [text for text in data['texts']]) \n",
    "    pool.close()\n",
    "    pool.join()\n",
    "\n",
    "print(datetime.now()-startTime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2064333\n"
     ]
    }
   ],
   "source": [
    "data['language'] = eng_results\n",
    "data = data[data.language==0]\n",
    "data.reset_index(inplace=True, drop=True)\n",
    "print(len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data.to_csv('dpa_prepro_step7.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Identify articles that predominantly consist of numbers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Articles that consist predominately of numbers sometimes carry little sentiment. Filtering out all numbers only helps a little, because the resulting texts are often grammatical nonsensical. To get a better understanding of how these articles look we identify all articles that consist of more than 80% of numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:01:23.476436\n"
     ]
    }
   ],
   "source": [
    "# use the 'numeric_articles' function to identify economic articles with a high share of numbers in them\n",
    "inputs = zip(data['texts'], data['word_count'], itertools.repeat(0.80))\n",
    "\n",
    "startTime = datetime.now()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    pool = mp.Pool(NUM_CORE)\n",
    "    numeric_list = pool.starmap(numeric_articles.numeric_articles, inputs)\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "\n",
    "print(datetime.now()-startTime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric = data[numeric_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'FLENSBURG (dpa-AFX) - Der langjährige Renner VW-Golf <VOW.ETR> ist auch 1999 an der Spitze der Auto-Neuzulassungen in Deutschland geblieben. Den 356.000 Golf-Exemplaren - einschließlich seiner Abwandlungen Vento und Bora - folgten erneut rund 230.000 Opel-Astra <GM.NYS>. Das geht aus einer Statistik der Kraftfahrt-Bundesamts (KBA) in Flensburg hervor, die am Dienstag veröffentlicht wurde. Insgesamt kamen 1999 knapp über 3,8 Millionen fabrikneue Personenwagen in den Straßenverkehr. 1998 waren es 3,73 Millionen.      Auf den dritten Platz liegen mit 143.000 Erstanmeldungen die Fahrzeugtypen der Dreier-Reihe von BMW <BME.ETR>, die den ebenfalls seit Jahren mit führenden VW-Passat (136.500) auf den vierten Rang verdrängten. Neuling in der 1999er Tabellenspitze der 25 Wagentypen - von insgesamt über 70 - sind der Ford-Focus <F.NYS> auf Platz fünf mit 113.000 Neuzulassungen, der VW-Lupo (16./61.000), der Peugeot 206 (22./44.000) sowie der Opel-Zafira (25./38.500). Auf Erfolgskurs ist auch die an siebter Stelle platzierte A-Klasse (100.000) von Mercedes/DaimlerChrysler <DCX.ETR>.    Die 25 Erstplatzierten der Pkw-Neuzulassungen 1999 (in Klammern 1998):  1. VW-Golf/Vento/Bora    356 246   ( 1. 347.151)  2. Opel-Astra            229 756   ( 2. 220.095)  3. BMW-Dreier            143 103   ( 6. 123.937)  4. VW-Passat             136 486   ( 3. 160.127)  5. Ford-Focus            112 873   (55.   9.256)  6. Opel-Corsa            111 781   ( 4. 128.141)  7. Mercedes A-Klasse     100 376   (16.  76.443)  8. Audi A4                97 976   ( 9. 101.935)  9. Opel-Vectra            94 057   ( 5. 124.645) 10. VW-Polo                90 858   ( 6. 106.341) 11. Mercedes E-Klasse      90 603   (10.  93.449) 12. Mercedes C-Klasse      89 909   ( 7. 112.513) 13. Renault-Megane         89 845   (11.  88.417) 14. BMW-Fünfer             69 552   (13.  81.904) 15. Audi A6                69 176   (17.  70.428) 16. Audi A3                61 237   (18.  59.923) 17. VW-Lupo                60 965   (47.  18.500) 18. Renault-Twingo         54 832   (19.  58.495) 19. Ford-Fiesta            53 213   (14.  78.660) 20. Ford-Mondeo            47 712   (12.  84.781) 21. Renault-Clio           46 234   (28.  30.524) 22. Peugeot 206            43 836   (56.   8.138) 23. Fiat-Punto             42 312   (21.  55.491) 24. VW-Transporter         40 068   (24.  37.825) 25. Opel-Zafira            38 584   (        122) /fc/ba/DP'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# inspect example article with high share of numbers\n",
    "numeric['texts'].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Operative Kosten                        40.616     48.002    18,2% Vertriebs- und allg.                    25.892     36.359    40,4% Verwaltungskosten Zentrale Verwaltungskosten              16.114     18.884    17,2% Nicht liquiditätswirksame                    0      5.393 Personalkosten aus der Gewährung von Aktienoptionen Abschreibungen und Amortisierung       122.614    119.848    -2,3% ------------------------------------------------------------------ --- Betriebsaufwand                        205.236    228.486    11,3% ------------------------------------------------------------------ --- ------------------------------------------------------------------ --- Betriebsergebnis (-verlust)            -21.193    -21.268     0,4%  ------------------------------------------------------------------ --- Zinsaufwand                            -55.901    -31.046   -44,5% Anderer Aufwand                         -1.298     -1.500    15,6% ------------------------------------------------------------------ --- Gewinn (Verlust) aus der gewöhnl.      -78.392    -53.814   -31,4% Geschäftstätigkeit vor Anteilen von Minderheitsgesellschaftern ------------------------------------------------------------------ --- Anteile von                               -584       -137   -76,5% Minderheitsgesellschaftern Ertragssteuern                          -3.835     -3.260   -15,0% ------------------------------------------------------------------ --- ERGEBNIS DER GEWÄHL.                   -82.811    -57.211   -30,9% GESCHÄFTSTÄTIGKEIT ------------------------------------------------------------------ --- Eingestellte Unternehmensbereiche       -3.469          0  -100,0% ------------------------------------------------------------------ --- Nettogewinn (-verlust)                 -86.280    -57.211   -33,7% ------------------------------------------------------------------ --- EBITDA                                 100.123     97.081    -3,0% EBITDA-Marge                             54,4%      46,8%  EBITDA (angepasst)                     101.421    103.974     2,5% EBITDA Marge (angepasst)                 55,1%      50,2%  Durchschn. Video Teilnehmer            793.715    907.574    14,3% Durchschn, Umsatzerlös pro               19,32      19,03    -1,5% Teilnehmer (DM) PrimaComAG, Hegelstr. 61, 55122 Mainz Tel++49-6131-93100  PrimaCom AG und Tochterunternehmen Konzernbilanzen (in tausend)                                       1998 (DM)        1999 (DM)  Flüssige Mittel                          15.347           16.365 Forderungen aus Lieferungen und           5.444            7.265 Leistungen Latente Steuerguthaben                   87.678           92.522 Sachanlagen                             544.393          541.351 Geschäfts- oder Firmenwert              473.600          467.150 Devisentermingeschäfte                   20.807                - Sonstige Vermögensgegenstände            44.280           22.708 Nettovermögen der zu verkaufenden             -                - Geschäftsbereiche ------------------------------------------------------------------ AKTIVA                                1.191.549        1.147.361 ------------------------------------------------------------------  Verbindlichkeiten aus Lieferungen         8.990           11.353 und Leistungen Rückstellungen                           51.732           28.946 Passive Rechnungsabgrenzungsposten        9.603           24.630 Ausstehende Kaufpreisverpflichtung        6.627            4.507 Leasingverbindlichkeiten                 86.382           27.620 Verbindlichkeiten gegenüber den             516              480 der Gesellschaft nahestehenden Personen oder Unternehmen Verbindlichkeiten gegenüber             281.395          406.990 Kreditinstituten und anderen Verbindlichkeiten Senior Notes                            275.981                - ------------------------------------------------------------------- SUMME DER VERBINDLICHKEITEN             721.226          504.526 ------------------------------------------------------------------- Minderheitenanteile                         862              165 Stammkapital                             78.914           98.643 Kapitalrücklage                         473.982          684.672 Verlustvortrag                          -83.435         -140.645 ------------------------------------------------------------------- EIGENKAPITAL/(NICHT DURCH               469.461          642.670 EIGENKAPITAL GEDECKTER FEHLBETRAG) ------------------------------------------------------------------- ------------------------------------------------------------------- PASSIVA                               1.191.549        1.147.361 ------------------------------------------------------------------- Ende der Mitteilung'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numeric['texts'].iloc[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5412"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(numeric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data.to_csv('dpa_prepro_step8.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge continuations of articles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some dpa-afx articles are split into multiple entries marked by the word 'Fortsetzung' at the beginning of the texts of following entries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'FRANKFURT (dpa-AFX) - Der deutsche Aktienmarkt hat am Montag für wenige Augenblicke seinen Rekordschluss vom Juli 1998 überschritten. Mit einem Höchststand von 6.188,68 lag der Dax über dem Tageschluss vom 21. Juli 1998, als der Index 6.186 Punkte erreichte. Der Dax <DAX.ETR> schloss den Handel am ersten Wochentag bei 6.142,19 Zählern und damit um 0,38% oder 23,02 Punkte fester ab. Der Nebenwerteindex M-Dax <MDAX.ETR> gab dagegen auf 3.990,00 Punkte oder um 1,43% nach, und der Neue Markt-Index Nemax 50 <NMKX.ETR> schloss bei 4.610,66 Zählern (-0,58%).      Spätestens auf dem Niveau des All-Time-High bei ungefähr 6.200 Punkten werde der Index auf einen massiven Widerstand stoßen, heißt es am Montag in einem Research-Report von der Nürnberger Schmidt Bank. Michael Schubert, Analyst der Bankgesellschaft Berlin, sagte in einem Gespräch mit dpa-AFX, derzeit sei der einzig negative Faktor, dass die Börse nicht von der Breite getragen werde, sondern nur von wenigen Titeln. Wer jetzt noch nicht investiert habe, sei in einer unangenehmen Situation: Er müsse dem Markt hinterherlaufen. Nach Meinung von Schubert ist eine Konsolidierung bis auf einen Wert von 6.000 oder 5.800 Zähler durchaus sinnvoll. Der Rückschritt soll sich am Besten ein bis zwei Wochen hinziehen, sagte Schubert./mr/ba'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['texts'].iloc[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'(Fortsetzung) - Der Tagesgewinner SAP <SAP.ETR> übernahm bereits am Vormittag die Führung bei den Gewinnern ein und schloss den Handel mit einem Kurszuwachs von 6,27% auf 475,20 Euro ab. Händler waren sich einig darüber, dass SAP weiteres Wachstumpotenzial haben. Ende vergangener Woche habe es Kaufempfehlungen für das Papier gegeben, sagte ein Händler. Die ABN Amro Bank hatte ihr Kursziel mit 600 Euro festgesetzt.      Den Wert umgaben zudem Gerüchte über eine bevorstehende Allianz mit einem Weltkonzern, hieß es. Ein anderer Händler hielt den aktuellen SAP-Kurs noch immer für zu billig. Wachstumswerte stünden weiter in der Gunst der Anleger. Bei SAP hätten Image und Aktienkurs unter Problemen in den USA gelitten. Das werde nach Erwartung des Händlers aber spätestens mit den Geschäftszahlen des ersten Quartals 2000 überwunden sein. Mit dem Ende der Angst vor dem Jahr-2000-Problem würde der Kurs weiter steigen.     Ihren Kurshöhenflug fortsetzen konnten zudem die Aktien von Siemens <SIE.ETR>. Sie schlossen den Handel um 4,94% fester bei 114,70 Euro. Der Technologiekonzern hatte am Vormittag bekannt gegeben, dass das Unternehmen seinen Atombereich mit der französischen Framatome zusammenlegen will. An dem Joint Venture soll Framatome 66%, Siemens wird 34% der Anteile halten. Für Alexander Koch von der KBC Bank Deutschland unterstützt diese Meldung den positiven Trend bei Siemens. Das ist jener Dax-Wert, der charttechnisch zur Zeit am besten aussieht, sagte er.     Die Aktie der Mannesmann AG <MMN.ETR> hat sich nach einer Klage der Mannesmann-Aktionäre gegen das eigene Management am Montag ebenfalls fest präsentiert. Der Titel schloss bei einem Kurs von 235,00 Euro und damit 4,44% fester. Gegen die Werbekampagne von Mannesmann bei Anlegern und Investoren, mit der eine Übernahmen durch Vodafone Airtouch <VOD.ISS> verhindert werden soll, sind inzwischen einige Aktionäre gerichtlich vorgegangen.(...)/mr/fs'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['texts'].iloc[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'(Fortsetzung) - Überraschend fest tendierten am Montag die Autowerte. Volkswagen <VOW.ETR> gewannen 1,6% auf 49,35 Euro. Vielleicht sehen wir jetzt tatsächlich eine Branchen-Rotation zur Autobranche hin, sagte ein Händler der DG-Bank. Die nahe Zukunft des Wertes stehe oder falle mit der 50 Euro Marke. Auch BMW <BMW.ETR> (28,10 Euro/+1,04%) und DaimlerChrysler <DCX.ETR> (67,25 Euro/+0,67%)  verbuchten Kurszuwächse. Alle drei deutschen Hersteller verbuchten einem Pressebericht zufolge im Gesamtjahr 1999 steigende Absatzzahlen in den USA.       Die Verliererliste führten am Montag Lufthansa <LHA.ETR> (22,30 Euro/-3,88%) und Veba <VEB.ETR> (45,47 Euro/-3,87%) an. Unter Gewinnmitnahmen litt laut Händlern der Kurs der T-Aktie <DTE.ETR>: Der Titel verlor im Montagshandel 1,31% auf 59,30 Euro. Wäre die Telekom mit dem Indextrend im Wert gestiegen, hätte der Dax den Rekordstand vom Juli 1998 eingestellt, rechnete ein Händler am Nachmittag vor./mr/fs'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['texts'].iloc[7]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We identify which entries belong together and merge them to one article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Divide data into roughly equal sized chunks where articles from one day only fall under the same chunk\n",
    "data_cont = data[data['topic'] == 'afx']\n",
    "dates = data_cont.groupby(['year', 'month', 'day'])\n",
    "dates = list(dates.groups)\n",
    "dates = np.array_split(dates, NUM_CORE)\n",
    "\n",
    "meta_data = data_cont.loc[:, data_cont.columns != 'texts']\n",
    "\n",
    "chunks = [pd.concat([data_cont[(data_cont['year'] == t[0]) & (data_cont['month'] == t[1]) & (data_cont['day'] == t[2])] for t in tup]) for tup in dates]\n",
    "chunks = [[chunk, meta_data] for chunk in chunks]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:14:56.636717\n"
     ]
    }
   ],
   "source": [
    "startTime = datetime.now()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    pool = mp.Pool(NUM_CORE)\n",
    "    continue_results = pool.map(continue_articles.continue_articles, chunks) \n",
    "    pool.close()\n",
    "    pool.join()\n",
    "    \n",
    "print(datetime.now()-startTime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.drop(list(itertools.chain(*[tup[0] for tup in continue_results])), inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "continue_articles = pd.concat([tup[1] for tup in continue_results])\n",
    "print(len(continue_articles))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1974426\n"
     ]
    }
   ],
   "source": [
    "data = data.append(continue_articles)\n",
    "data.reset_index(inplace=True, drop=True)\n",
    "print(len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data.to_csv('dpa_prepro_step9.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove URLs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:06:33.551105\n"
     ]
    }
   ],
   "source": [
    "startTime = datetime.now()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    pool = mp.Pool(NUM_CORE)\n",
    "    url_corrected = pool.map(correct_url.correct_url, [text for text in data['texts']]) \n",
    "    pool.close()\n",
    "    pool.join()\n",
    "\n",
    "print(datetime.now()-startTime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['texts'] = url_corrected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data.to_csv('dpa_prepro_step10.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove dpa references"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We remove dpa references (e.g. NEW YORK (dpa) - ...) from each article."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:15:15.393896\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    pool = mp.Pool(NUM_CORE)\n",
    "    dpa_ref_removed = pool.map(clean_dpa_references.clean_dpa_references, [text for text in data['texts']]) \n",
    "    pool.close()\n",
    "    pool.join()\n",
    "    \n",
    "print(datetime.now()-startTime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['texts'] = dpa_ref_removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data.to_csv('dpa_prepro_step11.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Correct spelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Umlauts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Older articles (1991 - 2000) from the section 'Kommentar' (Commentary) are often missing umlauts and correct capitalization. To\n",
    "fix these two issues, we use the notebook Umlauts_fix written in Python 2 and the notebook Truecasing written in Python 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "umlauts = ['ä', 'ö', 'ü', 'ß', 'Ä', 'Ö', 'Ü']\n",
    "umlauts_replace = ['ae', 'oe', 'ue', 'ss', 'AE', 'OE', 'UE']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dpa_umlauts_fix = data[(data.texts.str.contains('|'.join(umlauts_replace))) & (~data.texts.str.contains('|'.join(umlauts))) & (data.year<2001)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Ad-hoc announcement sent by DGAP. The sender is solely responsible for the contents of this announcement.  Ad-hoc Mitteilung Nach @ 15 WpHG  WizCom Technologies Ltd. (WizCom) <WZM.FSE> (Neuer Markt:WZM,WKN:915 856) veroeffentlicht das Ergebnis fuer das am 31. Dezember 1999 endende Geschaeftsjahr 20. Maerz 2000, Jerusalem, Israel - Der Umsatz belief sich im Jahr 1999 auf US$ Mio. 11,613. Der Vorjahreswert lag bei US$ Mio. 15,799. Der Umsatz im ersten bzw. zweiten Halbjahr 1999 betrug US$ Mio. 4,046 bzw. US$ Mio. 7,567. Das Unternehmen sieht den Umsatzrueckgang als voruebergehend an und vor allem im zweiten Halbjahr begruendet, weil das neue Produkt, der QuickLink-Pen, erst mit einigen Monaten Verzoegerung auf den Markt gebracht werden konnte. Im 4. Quartal 1999 konnte WizCom den QuickLink Pen in den USA erfolgreich auf den Markt bringen. Im 1. Quartal 2000 vermarktet WizCom den QuickLink Pen in weiteren Laendern, unter anderem Australien, Grossbritannien, Deutschland und Frankreich. Der Bruttogewinn verringerte sich von US$ Mio. 5,503 im Jahr 1998 auf US$ Mio. 3,221 im Jahr 1999. Dies ist hauptsaechlich auf den voruebergehenden Rueckgang beim Umsatz und auf die Fixkosten fuer das neue Produkt, den QuickLink-Pen, zurueckzufuehren. Der Nettoverlust fuer das Jahr 1999 betrug US$ Mio. 11,692, gegenueber US$ Mio. 4,130 in 1998. Hierin enthalten ist die einmalige Rueckzahlung eines Disagios auf Wandel-schuldverschreibungen in Hoehe von US$ Mio. 4,659 sowie eine Abschreibung auf  laufende Forschungs- und Entwicklungsarbeiten in Hoehe von US$ Mio. 2,271. WizCom ist ein Weltmarktfuehrer im Bereich der tragbaren Datenerfassungstechnologien. WizCom produziert und vermarktet die Produkte der Quicktionary Produktfamilie - tragbare stiftfoermige Scanner fuer den Linguistik- und Lehrmitttelmarkt. WizCom verfuegt ueber die einzigartige OCR (Optical Character Resolution)- Technologie, die in den Produkten implementiert sind. Seit Januar 2000 hat Wizcom seine Produktreihe um die erste Generation der Quicktionary Produkte, das Quicktionary II und den QuickLink-Pen, erweitert. Wizcom erwartet ein deutliches Umsatzwachstum sowohl im ersten Quartal 2000, als auch fuer den Rest des Jahres. Die Umsatzsteigerung wird zum einen auf dem Verkauf des QuickLink-Pen in den USA und weltweit basieren, zum anderen auf der Vermarktung des Quicktionary II nach der erwarteten Markteinfuehrung im 2. Quartal 2000. Zusaetzliche Einnahmen wird der Verkauf der Wizcom Plug- ins fuer Mobiltelefone im 4. Quartal bringen. Vor Kurzem hat Wizcom ein Lizenzabkommen mit Ericsson angekuendigt. Dieses Abkommen ermoeglicht es Wizcom, Technologie von Ericsson bei der Entwicklung von Plug-ins fuer Mobiltelefone von Ericsson zu verwenden. Mit diesen Plug-ins koennen Besitzer von Mobiltelefonen auf Papier vorliegenden Text direkt in ihr Mobiltelefon einscannen und so Daten speichern, via WAP dem Internet zur Verfuegung stellen oder per SMS versenden. Weitere Anwendungen werden entwickelt. Dieses Abkommen ist ein weiterer Schritt auf Wizcoms Weg zur Marktfuehrung im Sektor der tragbaren Datenlesegeraete und zur weiteren Verbreitung seiner Scann- und Texterkennungstechnologien im Marktsegment der tragbaren Geraete. Nach dem Abkommen mit Ericsson haben auch andere namhafte Hersteller und Serviceanbieter mit Wizcom Kontakt aufgenommen, um Wege fuer den Einsatz dieser innovativen Technologie zu finden.  Ende der Mitteilung'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dpa_umlauts_fix['texts'].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dpa_umlauts_fix.to_csv('dpa_umlauts_fix.csv', encoding='utf-8-sig', sep = ';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dpa_umlauts_fixed = pd.read_csv('dpa_umlauts_fixed.csv', encoding = 'utf-8', sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data.to_csv('dpa_prepro_step12.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Truecasing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "dpa_cases_fix = data[data.texts.str.contains('^(?!.*[A-Z])')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'new york (vwd) - enttaeuschend verlief das geschaeft am mittwoch, dem ersten handelstag im neuen jahr, an der new yorker aktienboerse. die zunaechst gesehenen leichten gewinne konnten nur bis in das fruehe nachmittagsgeschaeft behauptet werden. in den letzten 2-1/2 geschaeftststunden gerieten die kurse in die minuszone und wall street schloss auf breiter front schwaecher. der dow-jones-index fuer 30 industriewerte gab um 23,02 auf 2.610,64 punkte nach. auch die uebrigen marktbestimmenden indizes gerieten in die minuszone. bei einem umsatz von 126,28 (114,13) millionen aktien standen die kursverlierer den -gewinnern im verhaeltnis von rund neun zu sieben gegenueber. verantwortlich fuer die schwaeche waren wiederauflebende befuerchtungen ueber eine anhaltende rezessionsphase. nachdem sogar das weisse haus jetzt von einer rezessionaeren entwicklung spricht, hielten sich die meisten anleger mit ihren engagements zurueck, wodurch der vorherrschende abgabedruck ausreichte, um die kurse in die minuszone zu treiben. vor allem neue konjunktureckdaten, die auf eine rezession hindeuten, standen hinter der abwaertsbewegung. bei vielen anlegern setzte sich die befuerchtung durch, dass die schwache konjunktur zu einer schmaelerung der unternehmensertraege fuehren werde. vwd/2.1.91/06/mp/hs'"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dpa_cases_fix['texts'].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "dpa_cases_fix.to_csv('dpa_case_fix.csv', encoding='utf-8-sig', sep = ';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "dpa_cases_fixed = pd.read_csv('dpa_cases_fixed.csv', encoding = 'utf-8', sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.loc[dpa_cases_fixed.index, 'texts'] = dpa_cases_fixed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'New York( Vwd)- Enttaeuschend verlief das Geschaeft am Mittwoch, dem ersten Handelstag im neuen Jahr, an der New Yorker Aktienboerse. Die Zunaechst Gesehenen leichten Gewinne konnten nur bis in das Fruehe Nachmittagsgeschaeft behauptet werden. In den letzten 2-1/2 Geschaeftststunden gerieten die Kurse in die Minuszone und Wall Street schloss auf breiter Front Schwaecher. Der Dow-Jones-Index Fuer 30 Industriewerte gab um 23,02 auf 2.610,64 Punkte nach. Auch die Uebrigen Marktbestimmenden Indizes gerieten in die Minuszone. Bei einem Umsatz von 126,28( 114,13) Millionen Aktien standen die Kursverlierer den -Gewinnern im Verhaeltnis von rund neun zu sieben Gegenueber. Verantwortlich Fuer die Schwaeche waren Wiederauflebende Befuerchtungen Ueber eine anhaltende Rezessionsphase. Nachdem sogar das Weisse Haus jetzt von einer Rezessionaeren Entwicklung spricht, hielten sich die meisten Anleger mit ihren Engagements Zurueck, wodurch der vorherrschende Abgabedruck ausreichte, um die Kurse in die Minuszone zu treiben. Vor allem neue Konjunktureckdaten, die auf eine Rezession hindeuten, standen hinter der Abwaertsbewegung. Bei vielen Anlegern setzte sich die Befuerchtung durch, dass die schwache Konjunktur zu einer Schmaelerung der Unternehmensertraege Fuehren werde. Vwd/2.1.91/06/Mp/Hs'"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fixed version\n",
    "data['texts'].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data.to_csv('dpa_prepro_step13.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fixing tokens containing a number and a word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In quite a few cases, a number and a word are erroneously merged into a single token. Splitting these tokens into two tokens helps us to deal with the following problems:\n",
    "\n",
    "(see Handelsblatt notebook)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:02:13.158789\n"
     ]
    }
   ],
   "source": [
    "startTime = datetime.now()\n",
    "\n",
    "import split_number_word\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    pool = mp.Pool(NUM_CORE)\n",
    "    split_corrected = pool.map(split_number_word.split_number_word, [text for text in data['texts']]) \n",
    "    pool.close()\n",
    "    pool.join()\n",
    "\n",
    "print(datetime.now()-startTime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['texts'] = split_corrected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data.to_csv('dpa_prepro_step14.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Delete Fuzzy Duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "types_keep = ['russische Aktienmarkt', 'europäischen Börsen', 'Deutsche Börse' ,'Europäische Zentralbank',\n",
    "'Moskauer Aktienmarkt', 'deutsche Aktienmarkt', 'deutschen Aktienmarkt', 'Folgende Investmentbanken',\n",
    "'deutsche Rentenmarkt', 'amerikanischen Treasury Bonds', 'IRW-PRESS', 'Deutsche Bank', 'Ausgewählte Analysten-Einstufungen',\n",
    "'Deutsche Staatsanleihen', 'Der japanische Aktienmarkt']\n",
    "\n",
    "exceptions = ['NO_EXCEPTIONS']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1991.0\n",
      "1992.0\n",
      "1993.0\n",
      "1994.0\n",
      "1995.0\n",
      "1996.0\n",
      "1997.0\n",
      "1998.0\n",
      "1999.0\n",
      "2000.0\n",
      "2001.0\n",
      "2002.0\n",
      "2003.0\n",
      "2004.0\n",
      "2005.0\n",
      "2006.0\n",
      "2007.0\n",
      "2008.0\n",
      "2009.0\n",
      "2010.0\n",
      "2011.0\n",
      "2012.0\n",
      "2013.0\n",
      "2014.0\n",
      "2015.0\n",
      "2016.0\n",
      "2017.0\n",
      "2018.0\n",
      "6:58:11.847691\n"
     ]
    }
   ],
   "source": [
    "# import a function that outputs the indices of duplicates \n",
    "import fuzzy_duplicates\n",
    "delete_indices = []\n",
    "startTime = datetime.now() \n",
    "for year in list(set(data['year'])):\n",
    "    data_input = data[(data['year'] == year)]\n",
    "    for month in list(set(data_input[data_input['year'] == year]['month'])): # old: list(set(data))\n",
    "            # Prepare inputs\n",
    "            inputs_year = []\n",
    "            inputs_month = []\n",
    "            inputs_month_year = []\n",
    "            inputs_year.append(year)\n",
    "            inputs_month.append(month)\n",
    "            inputs_month_year.append(data_input[(data_input['year'] == year) & (data_input['month'] == month)][[\"month\", \"year\", \"texts\"]])\n",
    "\n",
    "            #from itertools import repeat\n",
    "            inputs = list(zip(inputs_year, inputs_month, inputs_month_year))\n",
    "            from datetime import datetime\n",
    "            if __name__ == \"__main__\":\n",
    "                pool = mp.Pool(NUM_CORE)\n",
    "                # apply function to all combinations of month-year in parallel\n",
    "                delete_intermediate = pool.starmap(fuzzy_duplicates.fuzzy_duplicates, zip(inputs, repeat(types_keep), repeat(exceptions)))\n",
    "                delete_indices = delete_indices + delete_intermediate # create one list of indices\n",
    "                pool.close()\n",
    "                pool.join()  \n",
    "    print(year)\n",
    "print(datetime.now()-startTime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "delete_indices = list(set([item for sublist in delete_indices for item in sublist]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1580324\n"
     ]
    }
   ],
   "source": [
    "data.drop(delete_indices, inplace=True)\n",
    "data.reset_index(inplace=True, drop=True)\n",
    "print(len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data.to_csv('dpa_prepro_step15.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean articles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dpa articles include some unnecessary text passages like inquiry notes or references to webpages. We decided to clean the \n",
    "affected articles from these text passages to make the sentiment classification easier for our model(s)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We remove the following terms and sections from the texts:\n",
    "* 1) stock symbols\n",
    "* 2) additional metadata in the text meant for the author\n",
    "* 3) references to previos articles\n",
    "* 4) references to dpa and dpa-AFX webpage\n",
    "* 5) uncorrected original article on which a correction is based on\n",
    "* 6) inquiry notes\n",
    "* 7) reference to english article on which some articles are based on\n",
    "* 8) date of the article\n",
    "* 9) references for aditional information (phone numbers, webpages etc.)\n",
    "* 10) references to sender\n",
    "* 11) references to Debitos\n",
    "* 12) references to issuer\n",
    "* 13) reference to authors\n",
    "* 14) references to summary of article "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:58:52.745164\n"
     ]
    }
   ],
   "source": [
    "startTime = datetime.now()\n",
    "if __name__ == \"__main__\":\n",
    "    pool = mp.Pool(NUM_CORE)\n",
    "    cleaned_articles = pool.map(clean_dpa_articles.clean_dpa_articles, [text for text in data['texts']]) \n",
    "    pool.close()\n",
    "    pool.join()\n",
    "    \n",
    "print(datetime.now()-startTime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['texts'] = cleaned_articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data.to_csv('dpa_prepro_final.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
