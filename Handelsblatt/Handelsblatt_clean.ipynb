{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Handelsblatt data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Handelsblatt* is a popular German daily newspaper. According to the IVW, Informationsgemeinscahft zur Feststellung der Verbreitung von Werbetr√§gern (Information Community for the Assessment of the Circulation of Media), it had a circulation of 140612 daily copies in the first quarter of 2021. An appealing feature of Handelsblatt for forecasting economic activity is its focus on the economy.\n",
    "\n",
    "We purchased Handelsblatt data from Genios, a German provider of business information. The corpus includes **980516** articles from January 1994 to November 2018. The data was purchased in February 2019. \n",
    "\n",
    "The data set includes 25 subfolders corresponding to a particular year (e.g., HB_1994). Each subfolder contains a few XML files which we parse to extract information relevant to our research project. Unfortunately, the copyright prevents us from publishing the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we need to read in the data. We create the list including the names of the 25 subfolders (`folder_list`) and apply the function `hb_load` to them in parallel by exploiting Python's `multiprocessing` library."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We extract the following XML elements:\n",
    "\n",
    "* datum - publication date\n",
    "* worte - the word count\n",
    "* ressort - section/subsection of the newspaper\n",
    "* titel-liste/serientitel - the name of the series\n",
    "* quelle/name - the name of the newspaper\n",
    "* quelle/seite-start - the page number in the newspaper\n",
    "* titel-liste/titel - article's title\n",
    "* titel-liste/dachzeile - article's kicker\n",
    "* titel-liste/untertitel - article's subheading\n",
    "* inhalt/vorspann - annotation\n",
    "* inhalt/text - text of the article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Handelsblatt is the main folder with 25 subfolders in it\n",
    "path = 'E:\\\\Userhome\\\\mokuneva\\\\Handelsblatt' # your path here\n",
    "\n",
    "# Create the list of all subfolders within Hb main folder.\n",
    "folder_list=[]\n",
    "\n",
    "for f in [f for f in os.listdir(path) ]: \n",
    "    # os.listdir(path) - names of directories                                         \n",
    "    folder_list.append(path + '\\\\' + f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of cores that will be used: 12\n"
     ]
    }
   ],
   "source": [
    "import multiprocessing as mp # use multiprocessing module for parallel computing\n",
    "\n",
    "NUM_CORE = mp.cpu_count()-4 # set the number of cores to use\n",
    "\n",
    "print(\"The number of cores that will be used: {}\".format(NUM_CORE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:01:18.892753\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "startTime = datetime.now() # track time\n",
    "\n",
    "import pandas as pd # load pandas: python data analysis library\n",
    "import hb_load # import a function that loads the data from one folder (see hb_load.py file for details)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    pool = mp.Pool(NUM_CORE)\n",
    "    data_intermediate = pool.map(hb_load.hb_load, folder_list) # load data from each folder in parallel\n",
    "    data = pd.concat(data_intermediate) # concatenate DataFrames corresponding to different folders\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "    \n",
    "print(datetime.now()-startTime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.sort_values(['year', 'month', 'day'], ascending=[True, True, True]) # sort the data in chronological order\n",
    "data.reset_index(inplace=True, drop=True) # reset the index of the DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>day</th>\n",
       "      <th>kicker</th>\n",
       "      <th>month</th>\n",
       "      <th>newspaper</th>\n",
       "      <th>page</th>\n",
       "      <th>rubrics</th>\n",
       "      <th>series_title</th>\n",
       "      <th>texts</th>\n",
       "      <th>title</th>\n",
       "      <th>title_only</th>\n",
       "      <th>word_c</th>\n",
       "      <th>year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>3</td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "      <td>Handelsblatt</td>\n",
       "      <td></td>\n",
       "      <td>Nachrichten</td>\n",
       "      <td></td>\n",
       "      <td>Nachrichten. Yuan abgewertet dpa PEKING. China...</td>\n",
       "      <td>Nachrichten.</td>\n",
       "      <td>Nachrichten.</td>\n",
       "      <td>68</td>\n",
       "      <td>1994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>3</td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "      <td>Handelsblatt</td>\n",
       "      <td></td>\n",
       "      <td>Nachrichten</td>\n",
       "      <td></td>\n",
       "      <td>Nachrichten. Ruecktritt abgelehnt afp NEU DELH...</td>\n",
       "      <td>Nachrichten.</td>\n",
       "      <td>Nachrichten.</td>\n",
       "      <td>58</td>\n",
       "      <td>1994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>3</td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "      <td>Handelsblatt</td>\n",
       "      <td></td>\n",
       "      <td>Nachrichten</td>\n",
       "      <td></td>\n",
       "      <td>Nachrichten. Ungarn wertet Forint ab rtr BUDAP...</td>\n",
       "      <td>Nachrichten.</td>\n",
       "      <td>Nachrichten.</td>\n",
       "      <td>33</td>\n",
       "      <td>1994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>3</td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "      <td>Handelsblatt</td>\n",
       "      <td></td>\n",
       "      <td>Nachrichten</td>\n",
       "      <td></td>\n",
       "      <td>Nachrichten. FDP zum Beamtenrecht dpa BONN. Di...</td>\n",
       "      <td>Nachrichten.</td>\n",
       "      <td>Nachrichten.</td>\n",
       "      <td>55</td>\n",
       "      <td>1994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>3</td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "      <td>Handelsblatt</td>\n",
       "      <td></td>\n",
       "      <td>Nachrichten</td>\n",
       "      <td></td>\n",
       "      <td>Nachrichten. Neue Drohungen dpa HAMBURG. Der F...</td>\n",
       "      <td>Nachrichten.</td>\n",
       "      <td>Nachrichten.</td>\n",
       "      <td>86</td>\n",
       "      <td>1994</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    day kicker  month     newspaper page      rubrics series_title  \\\n",
       "10    3             1  Handelsblatt       Nachrichten                \n",
       "11    3             1  Handelsblatt       Nachrichten                \n",
       "12    3             1  Handelsblatt       Nachrichten                \n",
       "13    3             1  Handelsblatt       Nachrichten                \n",
       "14    3             1  Handelsblatt       Nachrichten                \n",
       "\n",
       "                                                texts         title  \\\n",
       "10  Nachrichten. Yuan abgewertet dpa PEKING. China...  Nachrichten.   \n",
       "11  Nachrichten. Ruecktritt abgelehnt afp NEU DELH...  Nachrichten.   \n",
       "12  Nachrichten. Ungarn wertet Forint ab rtr BUDAP...  Nachrichten.   \n",
       "13  Nachrichten. FDP zum Beamtenrecht dpa BONN. Di...  Nachrichten.   \n",
       "14  Nachrichten. Neue Drohungen dpa HAMBURG. Der F...  Nachrichten.   \n",
       "\n",
       "      title_only  word_c  year  \n",
       "10  Nachrichten.      68  1994  \n",
       "11  Nachrichten.      58  1994  \n",
       "12  Nachrichten.      33  1994  \n",
       "13  Nachrichten.      55  1994  \n",
       "14  Nachrichten.      86  1994  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[10:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "980516"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the number of article before pre-processing\n",
    "len(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Light pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first part of the article 327123 contains different symbols (e.g. zw|lf) instead of umlauts. The second part (which I keep) has the same content with the right format. The second part starts with 'NEU DELHI.'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.at[327123, 'texts'] = data['texts'][327123][data['texts'][327123].find('NEU DELHI.'):]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove short articles (<100 words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An important feature of the text is its length. There are a few ways to count the number of words in a text:\n",
    "\n",
    "1. Use the metadata ('worte') of an XML file. Beware that numbers are counted as words. This is the column 'word_c' in the DataFrame. \n",
    "2. Quick and dirty approach: split the tokens by space and calculate the length of the list. Numbers and other non-alphabetic characters are counted as words. If a space is used as a delimiter in large numbers (100 000), this number will be counted as two tokens.\n",
    "3. Use `count_words_mp` function, which counts only words. This is our preferred method because we exclude numbers from the analysis in both sentiment analysis and topic modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the second approach\n",
    "# data['w_count'] = data['texts'].str.split(' ').str.len()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:00:38.085758\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "startTime = datetime.now() # track time\n",
    "\n",
    "import count_words_mp # import the function calculating the number of words in a text\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    pool = mp.Pool(NUM_CORE)\n",
    "    count_results = pool.map(count_words_mp.count_words_mp, [text for text in data['texts']]) \n",
    "    pool.close()\n",
    "    pool.join()\n",
    "    \n",
    "print(datetime.now()-startTime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the result as a new column \"word_count\"\n",
    "data['word_count'] = count_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The difference between the first ('word_c' column) and third ('word_count' column) approaches might be significant. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff = (data['word_c']-data['word_count'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[695, 691, 591, 430, 429, 429, 404, 390, 387, 387]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(diff, reverse=True)[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This happens because the third approach does not count a number as a word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Fu√üball-Bundesliga. \\\\\\\\\\\\\\\\\\\\\\\\\\\\ zu Hause\\\\ ausw√§rts\\\\ Sp.\\\\ g.\\\\ u.\\\\ v.\\\\ Tore\\\\ Diff.\\\\ Pkt.\\\\ Sp.\\\\ g.\\\\ u.\\\\ v.\\\\ Tore\\\\ Pkt.\\\\ Sp.\\\\ g.\\\\ u.\\\\ v.\\\\ Tore\\\\ Pkt.\\\\ 1. ( 1)\\\\ Bayern M√ºnchen 34\\\\ 20\\\\ 11\\\\ 3\\\\ 68:34\\\\ +34\\\\ 71\\\\ 17\\\\ 13\\\\ 4\\\\ 0\\\\ 37:12\\\\ 43\\\\ 17\\\\ 7\\\\ 7\\\\ 3\\\\ 31:22\\\\ 28\\\\ 2. ( 2)\\\\ Bayer Leverkusen 34\\\\ 21\\\\ 6\\\\ 7\\\\ 69:41\\\\ +28\\\\ 69'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['texts'][list(diff).index(387)][:300]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because short texts have sparse semantic features, topic models and BOW-based sentiment tools perform better on longer texts. This is why we keep the texts longer than 100 words.\n",
    "\n",
    "However, at this point, we also keep articles shorter than 100 words if they can potentially be continued on another page and therefore represent part of the chained articles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "continued_strings_short = ['Fortsetzung von Seite', 'FORTSETZUNG VON SEITE', 'Fortsetzung Seite', 'FORTSETZUNG SEITE',\n",
    "                           'Fortsetzung auf Seite', 'FORTSETZUNG AUF SEITE', 'Fortsetzung n√§chste Seite', \n",
    "                           'FORTSETZUNG N√ÑCHSTE SEITE']\n",
    "# keep articles that contain at least one of the strings from the list 'continued_strings_short' and contain \n",
    "# less than 100 words\n",
    "short_cont_ind = list(data[(data['word_count']<100) & \n",
    "    (data.texts.str.contains('|'.join(continued_strings_short)))].index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An example of the articles we keep."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'New York: Anfangs besser. An der Wall Street eroeffnete der Dow-Jones-Index am Montag mit steigenden Notierungen, rutschte aber bis 12.45 Uhr Ortszeit auf 3752,69, 1,40 Punkte unter den Vortragsschluss. Fortsetzung Seite 17'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.iloc[short_cont_ind[0]]['texts']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also keep short articles published on the same day as articles with the `short_cont_ind` indices and with the same beginning of the title. These news reports may be part of longer chained articles as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:00:01.724851\n"
     ]
    }
   ],
   "source": [
    "startTime = datetime.now() # track time\n",
    "# articles with this beginning of the title are not chained articles\n",
    "exceptions = ['Nachrichten', 'Geldticker', 'Wirtschaft', 'Fortsetzun']\n",
    "cont_index = []\n",
    "for ind in short_cont_ind:\n",
    "    # short articles published on the same day as articles with short_cont_ind indices\n",
    "    cont_art = data[(data.day == data.day[ind]) & (data.month == data.month[ind]) & (data.year == data.year[ind]) &\n",
    "                   (data['word_count']<100)]\n",
    "    for i in cont_art.index:\n",
    "        # articles with the same beginning of the title -> potentially chained articles\n",
    "        if (data['texts'][i][:10] == data['texts'][ind][:10]) and (i!=ind) and \\\n",
    "        (data['texts'][ind][:10] not in exceptions):\n",
    "            cont_index.append(i)\n",
    "print(datetime.now()-startTime)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the first part of the chained article published on the page 15 that does not contain any string suggesting that it will be continued on another page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'AUSLAENDISCHE BOERSEN / Alle Indizes verbuchen Gewinne - London geschlossen. Aktienmaerkte in Europa begruessen das neue Jahr mit steigenden Notierungen. cbu DUESSELDORF. Die europaeischen Aktienmaerkte gingen sehr freundlich in das neue Jahr. Alle fuehrenden Indizes lagen im Plus, die wichtige Boerse in London blieb aber geschlossen. In Fernost erreichten Singapur und Hongkong neue Rekordstaende.'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.iloc[cont_index[0]]['texts']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, we can also find the second part of this article including a string 'Continued from page 15' and having the same title as the first part."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'AUSLAENDISCHE BOERSEN / Fortsetzung von Seite 15. Weitere Rekorde in Fernost Tokio: Geschlossen. Wegen eines Feiertages blieben Behoerden, Banken und Maerkte geschlossen.'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.iloc[short_cont_ind[1]]['texts']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "short_keep = list(sorted(set(short_cont_ind + cont_index)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove articles with the number of words<100 unless these are chained articles candidates\n",
    "data = data.loc[short_keep].append(data[data['word_count']>=100])\n",
    "data = data.sort_values(['year', 'month', 'day'], ascending=[True, True, True]) # sort the data in chronological order\n",
    "data.reset_index(inplace=True, drop=True) # reset the index of the DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "673712"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the number of articles after removing short articles\n",
    "len(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove exact duplicates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A few examples of duplicates in our corpus:\n",
    "* The same article enters the corpus twice with different publication dates (e.g., 6.4.1994 and 27.4.1994). In this case, a natural solution is to keep the first entry.\n",
    "* The same article appears twice with a slight variation in the metadata (e.g., one of the duplicates includes the title of the series, or the word count is a little different even though the articles are identical).\n",
    "* The same article enters the corpus twice with the same publication date and metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All the duplicated articles are saved as 'hb_duplicates' for further exploration.\n",
    "hb_duplicates = data[data['texts'].duplicated(keep = False)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>day</th>\n",
       "      <th>kicker</th>\n",
       "      <th>month</th>\n",
       "      <th>newspaper</th>\n",
       "      <th>page</th>\n",
       "      <th>rubrics</th>\n",
       "      <th>series_title</th>\n",
       "      <th>texts</th>\n",
       "      <th>title</th>\n",
       "      <th>title_only</th>\n",
       "      <th>word_c</th>\n",
       "      <th>year</th>\n",
       "      <th>word_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9710</th>\n",
       "      <td>29</td>\n",
       "      <td></td>\n",
       "      <td>4</td>\n",
       "      <td>Handelsblatt</td>\n",
       "      <td></td>\n",
       "      <td>Weltwirtschaft</td>\n",
       "      <td></td>\n",
       "      <td>Importlizenzen im Vergleichsverfahren. HB DUES...</td>\n",
       "      <td>Importlizenzen im Vergleichsverfahren.</td>\n",
       "      <td>Importlizenzen im Vergleichsverfahren.</td>\n",
       "      <td>121</td>\n",
       "      <td>1994</td>\n",
       "      <td>112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9830</th>\n",
       "      <td>2</td>\n",
       "      <td></td>\n",
       "      <td>5</td>\n",
       "      <td>Handelsblatt</td>\n",
       "      <td></td>\n",
       "      <td>Weltwirtschaft</td>\n",
       "      <td></td>\n",
       "      <td>Importlizenzen im Vergleichsverfahren. HB DUES...</td>\n",
       "      <td>Importlizenzen im Vergleichsverfahren.</td>\n",
       "      <td>Importlizenzen im Vergleichsverfahren.</td>\n",
       "      <td>122</td>\n",
       "      <td>1994</td>\n",
       "      <td>112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15338</th>\n",
       "      <td>7</td>\n",
       "      <td></td>\n",
       "      <td>7</td>\n",
       "      <td>Handelsblatt</td>\n",
       "      <td></td>\n",
       "      <td>Finanzzeitung; Geld und Kredit</td>\n",
       "      <td></td>\n",
       "      <td>NACHTREPORT / Dow-Jones-Index steigt um 22 Pun...</td>\n",
       "      <td>NACHTREPORT / Dow-Jones-Index steigt um 22 Pun...</td>\n",
       "      <td>NACHTREPORT / Dow-Jones-Index steigt um 22 Pun...</td>\n",
       "      <td>638</td>\n",
       "      <td>1994</td>\n",
       "      <td>604</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15598</th>\n",
       "      <td>8</td>\n",
       "      <td></td>\n",
       "      <td>7</td>\n",
       "      <td>Handelsblatt</td>\n",
       "      <td></td>\n",
       "      <td>Finanzzeitung; Geld und Kredit</td>\n",
       "      <td></td>\n",
       "      <td>NACHTREPORT / Dow-Jones-Index steigt um 22 Pun...</td>\n",
       "      <td>NACHTREPORT / Dow-Jones-Index steigt um 22 Pun...</td>\n",
       "      <td>NACHTREPORT / Dow-Jones-Index steigt um 22 Pun...</td>\n",
       "      <td>636</td>\n",
       "      <td>1994</td>\n",
       "      <td>604</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16977</th>\n",
       "      <td>28</td>\n",
       "      <td></td>\n",
       "      <td>7</td>\n",
       "      <td>Handelsblatt</td>\n",
       "      <td></td>\n",
       "      <td>Wirtschaft und Politik</td>\n",
       "      <td></td>\n",
       "      <td>Glossar. Das Verarbeitende Gewerbe ist in West...</td>\n",
       "      <td>Glossar.</td>\n",
       "      <td>Glossar.</td>\n",
       "      <td>139</td>\n",
       "      <td>1994</td>\n",
       "      <td>130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18493</th>\n",
       "      <td>19</td>\n",
       "      <td></td>\n",
       "      <td>8</td>\n",
       "      <td>Handelsblatt</td>\n",
       "      <td></td>\n",
       "      <td>Karriere</td>\n",
       "      <td></td>\n",
       "      <td>Solide Basisausbildung zu DDR-Zeiten. Ost-Inge...</td>\n",
       "      <td>Solide Basisausbildung zu DDR-Zeiten. Ost-Inge...</td>\n",
       "      <td>Solide Basisausbildung zu DDR-Zeiten. Ost-Inge...</td>\n",
       "      <td>176</td>\n",
       "      <td>1994</td>\n",
       "      <td>169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18627</th>\n",
       "      <td>19</td>\n",
       "      <td></td>\n",
       "      <td>8</td>\n",
       "      <td>Handelsblatt</td>\n",
       "      <td></td>\n",
       "      <td>Karriere</td>\n",
       "      <td></td>\n",
       "      <td>Solide Basisausbildung zu DDR-Zeiten. Ost-Inge...</td>\n",
       "      <td>Solide Basisausbildung zu DDR-Zeiten. Ost-Inge...</td>\n",
       "      <td>Solide Basisausbildung zu DDR-Zeiten. Ost-Inge...</td>\n",
       "      <td>176</td>\n",
       "      <td>1994</td>\n",
       "      <td>169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20943</th>\n",
       "      <td>20</td>\n",
       "      <td></td>\n",
       "      <td>9</td>\n",
       "      <td>Handelsblatt</td>\n",
       "      <td></td>\n",
       "      <td>Unternehmen und Maerkte</td>\n",
       "      <td></td>\n",
       "      <td>Hochtief will seine Beteiligung bei Holzmann e...</td>\n",
       "      <td>Hochtief will seine Beteiligung bei Holzmann e...</td>\n",
       "      <td>Hochtief will seine Beteiligung bei Holzmann e...</td>\n",
       "      <td>738</td>\n",
       "      <td>1994</td>\n",
       "      <td>715</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20947</th>\n",
       "      <td>20</td>\n",
       "      <td></td>\n",
       "      <td>9</td>\n",
       "      <td>Handelsblatt</td>\n",
       "      <td></td>\n",
       "      <td>Unternehmen und Maerkte</td>\n",
       "      <td></td>\n",
       "      <td>Hochtief will seine Beteiligung bei Holzmann e...</td>\n",
       "      <td>Hochtief will seine Beteiligung bei Holzmann e...</td>\n",
       "      <td>Hochtief will seine Beteiligung bei Holzmann e...</td>\n",
       "      <td>738</td>\n",
       "      <td>1994</td>\n",
       "      <td>715</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21266</th>\n",
       "      <td>22</td>\n",
       "      <td></td>\n",
       "      <td>9</td>\n",
       "      <td>Handelsblatt</td>\n",
       "      <td></td>\n",
       "      <td>Wirtschaft und Politik; Konjunkturbarometer</td>\n",
       "      <td></td>\n",
       "      <td>Glossar. Das Verarbeitende Gewerbe ist in West...</td>\n",
       "      <td>Glossar.</td>\n",
       "      <td>Glossar.</td>\n",
       "      <td>133</td>\n",
       "      <td>1994</td>\n",
       "      <td>126</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       day kicker  month     newspaper page  \\\n",
       "9710    29             4  Handelsblatt        \n",
       "9830     2             5  Handelsblatt        \n",
       "15338    7             7  Handelsblatt        \n",
       "15598    8             7  Handelsblatt        \n",
       "16977   28             7  Handelsblatt        \n",
       "18493   19             8  Handelsblatt        \n",
       "18627   19             8  Handelsblatt        \n",
       "20943   20             9  Handelsblatt        \n",
       "20947   20             9  Handelsblatt        \n",
       "21266   22             9  Handelsblatt        \n",
       "\n",
       "                                           rubrics series_title  \\\n",
       "9710                                Weltwirtschaft                \n",
       "9830                                Weltwirtschaft                \n",
       "15338               Finanzzeitung; Geld und Kredit                \n",
       "15598               Finanzzeitung; Geld und Kredit                \n",
       "16977                       Wirtschaft und Politik                \n",
       "18493                                     Karriere                \n",
       "18627                                     Karriere                \n",
       "20943                      Unternehmen und Maerkte                \n",
       "20947                      Unternehmen und Maerkte                \n",
       "21266  Wirtschaft und Politik; Konjunkturbarometer                \n",
       "\n",
       "                                                   texts  \\\n",
       "9710   Importlizenzen im Vergleichsverfahren. HB DUES...   \n",
       "9830   Importlizenzen im Vergleichsverfahren. HB DUES...   \n",
       "15338  NACHTREPORT / Dow-Jones-Index steigt um 22 Pun...   \n",
       "15598  NACHTREPORT / Dow-Jones-Index steigt um 22 Pun...   \n",
       "16977  Glossar. Das Verarbeitende Gewerbe ist in West...   \n",
       "18493  Solide Basisausbildung zu DDR-Zeiten. Ost-Inge...   \n",
       "18627  Solide Basisausbildung zu DDR-Zeiten. Ost-Inge...   \n",
       "20943  Hochtief will seine Beteiligung bei Holzmann e...   \n",
       "20947  Hochtief will seine Beteiligung bei Holzmann e...   \n",
       "21266  Glossar. Das Verarbeitende Gewerbe ist in West...   \n",
       "\n",
       "                                                   title  \\\n",
       "9710              Importlizenzen im Vergleichsverfahren.   \n",
       "9830              Importlizenzen im Vergleichsverfahren.   \n",
       "15338  NACHTREPORT / Dow-Jones-Index steigt um 22 Pun...   \n",
       "15598  NACHTREPORT / Dow-Jones-Index steigt um 22 Pun...   \n",
       "16977                                           Glossar.   \n",
       "18493  Solide Basisausbildung zu DDR-Zeiten. Ost-Inge...   \n",
       "18627  Solide Basisausbildung zu DDR-Zeiten. Ost-Inge...   \n",
       "20943  Hochtief will seine Beteiligung bei Holzmann e...   \n",
       "20947  Hochtief will seine Beteiligung bei Holzmann e...   \n",
       "21266                                           Glossar.   \n",
       "\n",
       "                                              title_only  word_c  year  \\\n",
       "9710              Importlizenzen im Vergleichsverfahren.     121  1994   \n",
       "9830              Importlizenzen im Vergleichsverfahren.     122  1994   \n",
       "15338  NACHTREPORT / Dow-Jones-Index steigt um 22 Pun...     638  1994   \n",
       "15598  NACHTREPORT / Dow-Jones-Index steigt um 22 Pun...     636  1994   \n",
       "16977                                           Glossar.     139  1994   \n",
       "18493  Solide Basisausbildung zu DDR-Zeiten. Ost-Inge...     176  1994   \n",
       "18627  Solide Basisausbildung zu DDR-Zeiten. Ost-Inge...     176  1994   \n",
       "20943  Hochtief will seine Beteiligung bei Holzmann e...     738  1994   \n",
       "20947  Hochtief will seine Beteiligung bei Holzmann e...     738  1994   \n",
       "21266                                           Glossar.     133  1994   \n",
       "\n",
       "       word_count  \n",
       "9710          112  \n",
       "9830          112  \n",
       "15338         604  \n",
       "15598         604  \n",
       "16977         130  \n",
       "18493         169  \n",
       "18627         169  \n",
       "20943         715  \n",
       "20947         715  \n",
       "21266         126  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hb_duplicates[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop the exact duplicates, keep the article with the earlier publication date ('first')\n",
    "data.drop_duplicates(['texts'], keep = 'first', inplace=True)\n",
    "data.reset_index(inplace=True, drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "670961"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the number of articles after removing exact duplicates\n",
    "len(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filtering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Handelsblatt's articles are organized into 2073 unique sections/subsections. We investigate the most frequently met ones and remove articles published within a few of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Unternehmen und M√§rkte', 87617), ('Wirtschaft und Politik', 33223), ('Deutschland', 29652), ('Finanzzeitung', 28435), ('Unternehmen & M√§rkte', 19289), ('Meinung und Analyse', 17238), ('Titelseite', 16957), ('Beilage oder Sonderseite', 16278), ('International', 16252), ('Europa', 15709)]\n"
     ]
    }
   ],
   "source": [
    "# Sections and the number of articles per section\n",
    "import collections\n",
    "counter_sections = collections.Counter(data.rubrics)\n",
    "print(counter_sections.most_common(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To ensure consistency in topical content over time, we remove articles from a few sections that were covered only within a limited time period:\n",
    "\n",
    "* 1) 'Karriere': news related to work, career; due to the organizational changes, starting from 2014, this section is only present in a weekend edition of the newspaper.\n",
    "* 2) 'Weekend Journal': the section was introduced in 2002 and did not receive any coverage after 2008. Moreover, the news discussed within this section is on average longer (663 words) than the news from the main section of interest \"Wirtschaft und Politik\" (436 words).\n",
    "* 3) 'Panorama': news from around the world, issued in 1997, existed until 2001.\n",
    "* 4) 'Business-Travel': travel-related news, e.g. weather, 2001-2006, only 366 articles in total.\n",
    "* 5) 'Fortschritt': scientific findings, new devices, 2001-2003.\n",
    "* 6) 'Wochenende': weekend news, 2012-2018, average article length is 1579.\n",
    "* 7) 'perspektiven': perspectives, non-economic topics like a prize for women in science, working abroad, or new beauty standards popularized by Dove's advertising campaign; 2007-2009."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "abandoned_sections = ['Karriere', 'Weekend Journal', 'Panorama', 'Business-Travel', \n",
    "                    'Fortschritt', 'Wochenende', 'perspektiven']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data[~data['rubrics'].isin(abandoned_sections)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "658760"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the number of articles after removing articles from the sections covered only within a certain time period\n",
    "len(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we want to exclude the articles from non-economic sections. The list of irrelevant sections follows.\n",
    "\n",
    "* 1) 'Sportreport': sport news.\n",
    "* 2) 'Kunstmarkt': art market.\n",
    "* 3) 'Sportreport      Nachrichten': sport news.\n",
    "* 4) 'Neue B√ºcher': an advertisement for new books.\n",
    "* 5) 'Wissenschaft & Debatte': science and debate.\n",
    "* 6) 'Literatur': literature.\n",
    "* 7) 'Leserbriefe': letters to the editor; starting from 2013, there are almost no letters/comments from the readers, because all the comments became digital.\n",
    "* 8) 'Reisen und Tagen': travel.\n",
    "* 9) 'Kultur': culture.\n",
    "* 10) 'Termin- und Optionsm√§rkte': futures and options markets, mostly quantitative info.\n",
    "* 11) 'Galerie des Stils': style and fashion.\n",
    "* 12) 'Galerie': non-economic news about a pop band, Nutella restaurant etc.\n",
    "* 13) 'Neue Buecher': new books.\n",
    "* 14) 'Online': news about the websites, digitalization, online services.\n",
    "* 15) 'Forschung und Technik': research and technology.\n",
    "* 16) 'Business-Service': news unrelated to the economy, e.g. telephone service for weather forecast, travel information etc.\n",
    "* 17) 'Computer und Kommunikation': computers and communication.\n",
    "* 18) '√ñkonomie & Bildung': economics and education, Handelsblatt's materials for school lessons, MBA studies, educating articles about how the economy works.\n",
    "* 19) 'Auto-Mobil': new car models.\n",
    "* 20) 'Die Handelsblatt-Woche': announcement of events taking place this week.\n",
    "* 21) 'Wirtschaftsbuch': economics books."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_economic_sections = ['Kunstmarkt', 'Sportreport', 'Sportreport      Nachrichten', \n",
    "                  'Neue B√ºcher', 'Wissenschaft & Debatte',  \n",
    "                  'Literatur', 'Leserbriefe', 'Reisen und Tagen', \n",
    "                  'Kultur',  'Termin- und Optionsm√§rkte',\n",
    "                  'Galerie des Stils', 'Galerie', 'Neue Buecher', 'Online', 'Forschung und Technik', \n",
    "                  'Business-Service', 'Computer und Kommunikation', '√ñkonomie & Bildung', 'Auto-Mobil', \n",
    "                  'Die Handelsblatt-Woche', 'Wirtschaftsbuch']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data[~data['rubrics'].isin(non_economic_sections)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "621597"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the number of articles after removing articles from the non-economic sections\n",
    "len(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Additionally, we remove articles from the related sections and subsections. \n",
    "\n",
    "* This means that we not only remove articles from a section 'Kunstmarkt', but also from the subsections 'Kunstmarkt      Anleihen', 'Kunstmarkt      Aus den Galerien' etc. \n",
    "\n",
    "* We consider 'Kunstmarkt' and 'Themen und Trends      Kunstmarkt' as two close sections. \n",
    "\n",
    "* The full list of sections/subsections we remove can be found in 'subsections_hb.txt'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this function loads the dictionary with the related sections and subsections we want to exclude\n",
    "from os import path\n",
    "import codecs\n",
    "\n",
    "def dictionary_open(name):\n",
    "    with codecs.open(path.join(os.getcwd(), name),\n",
    "               'r',  'utf-8') as f:\n",
    "          dictionary = set(f.read().splitlines()[1:-1])\n",
    "    return dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "subsections_clean = dictionary_open('subsections_hb.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data[~data['rubrics'].isin(subsections_clean)]\n",
    "data.reset_index(inplace=True, drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "589117"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the number of articles after removing articles from the related subsections\n",
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Unternehmen und M√§rkte', 87617), ('Wirtschaft und Politik', 33223), ('Deutschland', 29652), ('Finanzzeitung', 28435), ('Unternehmen & M√§rkte', 19289), ('Meinung und Analyse', 17238), ('Titelseite', 16957), ('Beilage oder Sonderseite', 16278), ('International', 16252), ('Europa', 15709)]\n"
     ]
    }
   ],
   "source": [
    "# Sections and the number of articles per section\n",
    "counter_sections=collections.Counter(data['rubrics'])\n",
    "print(counter_sections.most_common(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section + text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove articles for which the following two conditions are met: a section is '' (unclassified), and a text contains a string 'KARRIERE'. As discussed, this type of articles is removed due to organizational changes (see above)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data[~((data['rubrics']=='') & (data['texts'].str.contains('KARRIERE')))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove articles that are characterized by the following conditions: a section contains a string \"Recht und Steuern\" (Law and taxes), and a text contains one of the legal terms from the list `legal_terms`.\n",
    "\n",
    "* Az./AZ/Aktenzeichen/AKTENZEICHEN - docket number\n",
    "* LAG/Landesarbeitsgericht - Regional Labour Court\n",
    "* BAG/Bundesarbeitsgericht - Federal Labour Court\n",
    "* VI R/IV A - case/law reference\n",
    "* BFH - Bundesfinanzhof, German Federal Fiscal court\n",
    "* Abs. - section\n",
    "* BGH/Bundesgerichtshof - Federal Supreme Court\n",
    "* BGB - B√ºrgerliches Gesetzbuch, Civil Code\n",
    "* BSG - Bundessozialgericht, Federal Social Court\n",
    "* EStG/Einkommensteuergesetz - Income Tax Act\n",
    "* OLG/Oberlandesgericht - Higher Regional Court\n",
    "* Anwaltgerichtshof - Lawyers' Court\n",
    "* Finanzgericht - Tax Court\n",
    "\n",
    "We remove these articles because they contain very detailed explanations of laws/tax deduction rules/court decisions which have a low chance to be relevant for forecasting economic activity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "legal_terms = ['Az\\\\.', 'AZ', 'Az\\\\:', 'Aktenzeichen', 'AKTENZEICHEN', 'LAG', 'Landesarbeitsgericht', 'Bundesarbeitsgericht', 'BAG', 'Arbeitsgericht', 'VI R','IV A','BFH','Abs\\\\.','BGH', 'Bundesgerichtshof', 'BGB','BSG','EStG','Einkommensteuergesetz','OLG', 'Oberlandesgericht', 'Anwaltgerichtshof', 'Finanzgericht']\n",
    "data = data[~((data.rubrics.str.contains('Recht und Steuern')) & (data.texts.str.contains('|'.join(legal_terms))))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "584047"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the number of articles after filtering out articles based on the section and text\n",
    "len(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section + title"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove articles satisfying two conditions: section is 'Inhalt' (content), and the title is not 'Termine des Tages.' Section 'Inhalt' mainly includes news reports about the Handelsblatt and its organization. The aritlces with the title 'Termine des Tages.' may include short news on the Economy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data[~(((data['rubrics']=='Inhalt') | (data['rubrics']=='Inhalt ;') | (data['rubrics']=='Inhalt      Der Werber-Rat'))\n",
    "              & (data['title']!='Termine des Tages.'))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "583278"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the number of articles after removing articles from the section 'Inhalt'\n",
    "len(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Title"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exclude articles with the following title patterns:\n",
    "\n",
    "* book suggestions (Buchtip)\n",
    "* some very short news on a few people (BUSINESS LOUNGE.)\n",
    "* articles about Handelsblatt and its organization (Liebe Leser)\n",
    "* articles with comments from the readers (DIE MEINUNG UNSERER LESER.)\n",
    "* announcement of events taking place this week (DIE HANDELSBLATT-WOCHE.)\n",
    "* non-economic news related to numbers (e.g., how many people watched the movie this week); Bilanz des Wochenendes.\n",
    "* sport news (SPORT TELEGRAMM.) \n",
    "* news related to the project 'Handelsblatt macht Schule' promoting economic education"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "title_patterns = ['BUCHTIP', 'Buchtip', 'BUSINESS LOUNGE\\\\.', 'Liebe Leser', 'DIE MEINUNG UNSERER LESER\\\\.', \n",
    "                 'DIE HANDELSBLATT\\\\-WOCHE\\\\.', 'Die Handelsblatt\\\\-Woche\\\\.', 'Die Handelsblattwoche\\\\.',\n",
    "                 'DIE HANDELSBLATTWOCHE\\\\.', 'Bilanz des Wochenendes\\\\.', 'BILANZ DES WOCHENENDES\\\\.',\n",
    "                 'SPORT TELEGRAMM\\\\.']\n",
    "data = data[~data['title_only'].str.contains('|'.join(title_patterns))]\n",
    "\n",
    "kicker_title_patterns = ['Handelsblatt macht Schule', 'HANDELSBLATT MACHT SCHULE\\\\.', \n",
    "                         'Aktuelles Wirtschaftswissen f√ºr den Unterricht', \n",
    "                         'Wirtschaftswissen f√ºr Sch√ºler leicht gemacht',\n",
    "                        '√ñkonomie ganz leicht gemacht f√ºr Jugendliche',\n",
    "                        'Aktuelles Wirtschaftswissen f√ºr Jugendliche',\n",
    "                        'Neuer Stoff f√ºr den Unterricht im Fach Wirtschaft',\n",
    "                        'Handelsblatt bringt Praxis in die Schulen',\n",
    "                        'Experten aus der Praxis in den Schulen',\n",
    "                        'Der neue Newcomer ist da',\n",
    "                        'Die neue Zeitung f√ºr Sch√ºler ist da',\n",
    "                        'Wettbewerb\\\\: Eine Welt ohne Geld\\\\?',\n",
    "                        'Wettbewerb zum Thema Geld',\n",
    "                        'Wettbewerb f√ºr Sch√ºler',\n",
    "                        'Schlauere Azubis dank Zeitunglesen',\n",
    "                        'F√∂rderung f√ºr Auszubildende'\n",
    "                        ]\n",
    "data = data[~data.title.str.contains('|'.join(kicker_title_patterns))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exclude articles with the following titles:\n",
    "\n",
    "* Kontakte. (contacts of organizations: addresses and phone numbers)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data[~(data['title_only']=='Kontakte.')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "580876"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the number of articles after excluding articles based on the title patterns\n",
    "len(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Series"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exclude articles from the following non-economic series:\n",
    "\n",
    "* Neue Wirtschaftsliteratur (Handelsblatt-Beilage): economic books;\n",
    "* Golf (Handelsblatt-Beilage): golf;\n",
    "* Literatur (Handelsblatt-Beilage): literature;\n",
    "* Macher des Handelsblatts (Handelsblatt-Serie): about journalists working for Handelsblatt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "series_exclude = ['Neue Wirtschaftsliteratur (Handelsblatt-Beilage)', \n",
    "                  'Golf (Handelsblatt-Beilage)', 'Literatur (Handelsblatt-Beilage)', \n",
    "                  'Literatur (Handelsblatt - Beilage)', 'Macher des Handelsblatts (Handelsblatt-Serie)']\n",
    "data = data[~data['series_title'].isin(series_exclude)]\n",
    "data.reset_index(inplace=True, drop=True) # reset the index of the DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "580228"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the number of articles after excluding articles from the non-economic series\n",
    "len(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exclude articles that contain the following strings:\n",
    "\n",
    "* TERMINM√ÑRKTE / Tagesbericht: options trading, mainly quantitative information;\n",
    "* Deutsche B√∂rsen D√ºsseldorf: stock market, mainly quantitative information;\n",
    "* DIGIX. Der E-Business-Index. or DIGIX. DIGIX Der E-Business-Index.: text corresponding to a graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_strings = ['TERMINM√ÑRKTE \\\\/ Tagesbericht', 'TERMINMAERKTE \\\\/ Tagesbericht', \n",
    "                'Deutsche B√∂rsen D√ºsseldorf', 'Deutsche Boersen Duesseldorf', 'DIGIX\\\\. Der E\\\\-Business\\\\-Index.',\n",
    "               'DIGIX\\\\. DIGIX Der E\\\\-Business\\\\-Index\\\\.']\n",
    "data = data[~(data.texts.str.contains('|'.join(text_strings)))]\n",
    "data.reset_index(inplace=True, drop=True) # reset the index of the DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "580102"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the number of articles after excluding articles based on the text patterns\n",
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({2000: 33949, 1999: 32861, 2001: 30730, 1996: 27326, 1997: 27086, 1998: 26912, 2004: 26432, 1995: 26170, 1994: 25349, 2002: 25083, 2005: 24726, 2003: 24350, 2006: 24167, 2007: 23816, 2010: 23811, 2008: 23520, 2009: 23329, 2011: 22196, 2012: 20667, 2013: 17506, 2014: 17397, 2015: 16297, 2016: 13102, 2017: 12152, 2018: 11168})\n"
     ]
    }
   ],
   "source": [
    "counter=collections.Counter(data['year'])\n",
    "print(counter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Umlauts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To fix the issue with umlauts, we use the notebook Umlauts_fix written in Python 2. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "umlauts = ['√§', '√∂', '√º', '√ü', '√Ñ', '√ñ', '√ú']\n",
    "umlauts_replace = ['ae', 'oe', 'ue', 'ss', 'AE', 'OE', 'UE']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "hb_umlauts_fix = data[(data.texts.str.contains('|'.join(umlauts_replace))) & (~data.texts.str.contains('|'.join(umlauts))) & (data.year<1999)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1994 wird ein Jahr des Wandels. Auf ein Neues!. Steuern und Beitraege steigen, staatliche Leistungen werden gesenkt - wahrlich kein berauschender Auftakt fuer das neue Jahr. Ohnehin sind die Erwartungen nicht sehr rosig. Zwar wird sich die Wirtschaft erholen. Ob es zu einem kraeftigen Aufschwung reicht, ist ungewiss. Sicher ist dagegen der Anstieg der Arbeitslosigkeit. Fuer Spannung sorgt der Wahlmarathon, der erst mit den Bundestagswahlen im Oktober entschieden wird. Kein Zweifel: 1994 wird ein Jahr der Unsicherheit und des Wandels. Ob es auch ein gutes Jahr wird, haengt davon ab, wie dieser Wandel bewaeltigt wird. 1993 war das Jahr der Krisenerkenntnis. Die Einsicht in die Kluft zwischen wirtschaftlich Machbarem und Anspruchsdenken hat erste Ansaetze zur Krisenbewaeltigung ermoeglicht. In der Tarifpolitik und bei den Sozialleistungen hat es - noch vor kurzem undenkbare - Einschnitte gegeben. Auch 1994 wird es Kuerzungen geben. Nur: Mit dem Rotstift allein kann kein Ausweg aus der Krise gezeichnet werden. In den Unternehmen sind innovative Strategien gefragt. Anstatt den Guertel immer wieder ein wenig enger zu schnallen, sollte der Staat seine Rolle grundsaetzlich ueberdenken. Ein Rueckzug auf die Kernaufgaben schafft verlaessliche Rahmendaten. Und die sind in unsicheren Zeiten besonders wichtig. Die Krise muss als Herausforderung, Wandel als Chance zum Fortschritt aufgefasst werden. Dann kann 1994 ein gutes Jahr werden - selbst wenn es zu Sylvester im Portemonnaie noch nicht messbar ist.'"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# example of the text that we try to fix with a spellchecker\n",
    "hb_umlauts_fix.texts[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "hb_umlauts_fix.to_csv('hb_umlauts_fix.csv', encoding='utf-8-sig', sep = ';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "X:\\conda\\envs\\py3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:2785: DtypeWarning: Columns (3) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "hb_umlauts_fixed = pd.read_csv('hb_umlauts_fixed.csv', encoding = 'utf-8', sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.loc[hb_umlauts_fixed['Unnamed: 0.1'], 'texts'] = hb_umlauts_fixed.texts.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1994 wird ein Jahr des Wandels. Auf ein Neues!. Steuern und Beitr√§ge steigen, staatliche Leistungen werden gesenkt - wahrlich kein berauschender Auftakt f√ºr das neue Jahr. Ohnehin sind die Erwartungen nicht sehr rosig. Zwar wird sich die Wirtschaft erholen. Ob es zu einem kr√§ftigen Aufschwung reicht, ist ungewiss. Sicher ist dagegen der Anstieg der Arbeitslosigkeit. F√ºr Spannung sorgt der Wahlmarathon, der erst mit den Bundestagswahlen im Oktober entschieden wird. Kein Zweifel: 1994 wird ein Jahr der Unsicherheit und des Wandels. Ob es auch ein gutes Jahr wird, h√§ngt davon ab, wie dieser Wandel bew√§ltigt wird. 1993 war das Jahr der Krisenerkenntnis. Die Einsicht in die Kluft zwischen wirtschaftlich Machbarem und Anspruchsdenken hat erste Ans√§tze zur Krisenbew√§ltigung erm√∂glicht. In der Tarifpolitik und bei den Sozialleistungen hat es - noch vor kurzem undenkbare - Einschnitte gegeben. Auch 1994 wird es K√ºrzungen geben. Nur: Mit dem Rotstift allein kann kein Ausweg aus der Krise gezeichnet werden. In den Unternehmen sind innovative Strategien gefragt. Anstatt den G√ºrtel immer wieder ein wenig enger zu schnallen, sollte der Staat seine Rolle grunds√§tzlich √ºberdenken. Ein R√ºckzug auf die Kernaufgaben schafft verl√§ssliche Rahmendaten. Und die sind in unsicheren Zeiten besonders wichtig. Die Krise muss als Herausforderung, Wandel als Chance zum Fortschritt aufgefasst werden. Dann kann 1994 ein gutes Jahr werden - selbst wenn es zu Sylvester im Portemonnaie noch nicht messbar ist.'"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fixed version\n",
    "data.texts[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove English articles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the next pre-processing step, we filter out news articles written in English. To do that, we use a **langdetect** library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:32:14.876433\n"
     ]
    }
   ],
   "source": [
    "startTime = datetime.now()\n",
    "\n",
    "import identify_eng_2\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    pool = mp.Pool(NUM_CORE)\n",
    "    eng_results = pool.map(identify_eng_2.identify_eng_2, [text for text in data['texts']]) \n",
    "    pool.close()\n",
    "    pool.join()\n",
    "\n",
    "print(datetime.now()-startTime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['language'] = eng_results\n",
    "data = data[data.language==0]\n",
    "data.reset_index(inplace=True, drop=True) # reset the index of the DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "580044"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the number of articles after excluding English articles\n",
    "len(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove URLs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove URLs and html file references.\n",
    "E.g.: 'de/studie99.html', 'Amazon.com', etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:02:39.402897\n"
     ]
    }
   ],
   "source": [
    "startTime = datetime.now()\n",
    "\n",
    "import correct_url\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    pool = mp.Pool(NUM_CORE)\n",
    "    url_corrected = pool.map(correct_url.correct_url, [text for text in data['texts']]) \n",
    "    pool.close()\n",
    "    pool.join()\n",
    "\n",
    "print(datetime.now()-startTime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['texts'] = url_corrected"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## O instead of 0 problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In some cases, Optical Character Recognition (OCR) can not distinguish between '0' and 'O' ('o'). As a result, there are tokens like '1OO'. Using regular expressions, we identify problematic tokens and replace 'O' ('o') with '0'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:00:15.624922\n"
     ]
    }
   ],
   "source": [
    "startTime = datetime.now()\n",
    "\n",
    "import ocr_replace\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    pool = mp.Pool(NUM_CORE)\n",
    "    ocr_corrected = pool.map(ocr_replace.ocr_replace, [text for text in data['texts']]) \n",
    "    pool.close()\n",
    "    pool.join()\n",
    "\n",
    "print(datetime.now()-startTime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['texts'] = ocr_corrected"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fixing tokens containing a number and a word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In quite a few cases, a number and a word are erroneously merged together into a single token. Splitting these tokens into two tokens helps us to deal with the following problems:\n",
    "\n",
    "1. Hyphen-separated words will have the same spelling across the whole corpus. Examples: 20J√§hrige/20-J√§hrige, 100prozentig/100-prozentig, 30min√ºtigen/30-min√ºtigen etc.\n",
    "\n",
    "2. Numbers and the names of the currencies will represent two different tokens. Examples: 100DM/100 DM, 30Euro/30 Euro.\n",
    "\n",
    "3. Numbers and the measures of time/weight/distance etc. will be split into two tokens. Examples: 100km/100 km, 1Mill/1 Mill, 16Uhr/16 Uhr.\n",
    "\n",
    "4. Simple mistakes will be corrected: 30bis 40/30 bis 40, 10Fahrzeuge/10 Fahrzeuge, 10und/10 und.\n",
    "\n",
    "5. Mistakes in the beginnning of the sentences will be corrected: 10Welche/10 Welche, 8Wie/8 Wie, etc.\n",
    "\n",
    "The most frequently met exceptions from this rule are taken into account:\n",
    "\n",
    "1. Names of the companies/organizations: 1822direkt, 3Sat, 4MBO etc.\n",
    "2. Names of smartphone/airplane/satellite models: 4S, 1B, 1k, 328Jet, 5C, 777X etc.\n",
    "3. German nouns like 90er and English adjectives like 21st, 3rd."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:00:24.375525\n"
     ]
    }
   ],
   "source": [
    "startTime = datetime.now()\n",
    "\n",
    "import split_number_word\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    pool = mp.Pool(NUM_CORE)\n",
    "    split_corrected = pool.map(split_number_word.split_number_word, [text for text in data['texts']]) \n",
    "    pool.close()\n",
    "    pool.join()\n",
    "\n",
    "print(datetime.now()-startTime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['texts'] = split_corrected"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove fuzzy duplicates\n",
    "\n",
    "By fuzzy duplicates we understand nearly duplicated articles. These are:\n",
    "\n",
    "* drafts/minor revisions of the articles saved in the database;\n",
    "* slightly changed advertisements which are published several times during a month;\n",
    "* 'NACHT - REPORT' news reports about stock market which are published a few hours earlier than almost identical 'WALL STREET' articles.\n",
    "\n",
    "We identify 'fuzzy' duplicates using cosine similarity and choose a threshold of 93% based on some visual exploration. Here is the article by Ryan Basques we used as a reference: [Link](https://towardsdatascience.com/a-laymans-guide-to-fuzzy-document-deduplication-a3b3cf9a05a7). \n",
    "\n",
    "There are also a few exceptions that we take into account:\n",
    "\n",
    "* 'DEUTSCHE AKTIEN' articles are often erroneously classified as duplicates because they contain very similar information;\n",
    "* 'Fortsetzung von Seite' (continuation of an article from another page): not surprisingly, these documents are also very close to each other. Later we concatenate them into one article;\n",
    "* cosine similarity approach does not work perfectly for extremely long articles. A few of them (e.g., 'Stuttgart 21: Ist das Projekt noch zu stoppen?') we list as exceptions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Required input for the function 'fuzzy_duplicates': a dataframe for each month-year combination.\n",
    "# List with a year\n",
    "inputs_year = []\n",
    "# List with a month\n",
    "inputs_month = []\n",
    "# List with the dataframes containing 'year', 'month', and 'texts' columns\n",
    "inputs_month_year = []\n",
    "for year in list(set(data['year'])):\n",
    "    for month in list(set(data['month'])):\n",
    "        inputs_year.append(year)\n",
    "        inputs_month.append(month)\n",
    "        inputs_month_year.append(data[(data['year'] == year) & (data['month'] == month)][[\"month\", \"year\", \"texts\"]])\n",
    "        \n",
    "inputs = list(zip(inputs_year, inputs_month, inputs_month_year))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "X:\\conda\\envs\\py3\\lib\\site-packages\\gensim\\utils.py:1209: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:08:12.212528\n"
     ]
    }
   ],
   "source": [
    "startTime = datetime.now() # track time\n",
    "\n",
    "import fuzzy_duplicates # import a function that outputs the indices of fuzzy duplicates \n",
    "delete_indices = []\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    pool = mp.Pool(NUM_CORE)\n",
    "    # apply function to all combinations of month-year in parallel\n",
    "    delete_intermediate = pool.map(fuzzy_duplicates.fuzzy_duplicates, inputs)\n",
    "    delete_indices = delete_indices + delete_intermediate # create one list of indices\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "    \n",
    "print(datetime.now()-startTime)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Duplicates exploration\n",
    "\n",
    "In case you want to have a look at the identified duplicates, use the function 'fuzzy_duplicates_test' from the file 'fuzzy_duplicates_test_all'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:08:10.616491\n"
     ]
    }
   ],
   "source": [
    "startTime = datetime.now() # track time\n",
    "\n",
    "import fuzzy_duplicates_test_all \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    pool = mp.Pool(NUM_CORE)\n",
    "    dup_intermediate = pool.map(fuzzy_duplicates_test_all.fuzzy_duplicates_test, inputs) \n",
    "    duplicates = pd.concat(dup_intermediate) \n",
    "    pool.close()\n",
    "    pool.join()\n",
    "    \n",
    "print(datetime.now()-startTime)\n",
    "\n",
    "duplicates.to_csv('duplicates.csv', encoding = 'utf-8-sig', sep = ';')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drop the duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Free memory\n",
    "inputs = None\n",
    "# List of indices corresponding to the duplicated articles\n",
    "delete_indices = [item for sublist in delete_indices for item in sublist]\n",
    "# List of unique indices\n",
    "delete_indices = list(set(delete_indices))\n",
    "# Drop the fuzzy duplicates\n",
    "data.drop(data.index[delete_indices], inplace = True)\n",
    "data.reset_index(inplace=True, drop=True) # reset the index of the DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_csv('before_chained.csv', encoding = 'utf-8-sig', sep = ';')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Concatenate chained articles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By the chained articles we mean the news reports that start on one page and continue on another page of the newspaper. Since the news database is prepared using OCR technology, each part of a chained news article represents a separate entry in the database, i.e. a separate news report.\n",
    "\n",
    "Ideally, all the parts of a chained article must be combined into one news report. However, as nothing in the metadata helps us identify chained articles, the merging process is not straightforward. We do our best to merge at least some chained articles based on the text alone."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data = pd.read_csv('before_chained.csv', encoding = 'utf-8-sig', sep=';', index_col = 0, dtype = {'kicker': 'str',\n",
    "                                                                                                 'page': 'str',\n",
    "                                                                                                 'series_title': 'str',\n",
    "                                                                                                 'rubrics': 'str'})\n",
    "data.page = data.page.fillna('')\n",
    "data.series_title = data.series_title.fillna('')\n",
    "data.kicker = data.kicker.fillna('')\n",
    "data.rubrics = data.rubrics.fillna('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second part of a chained article normally contains one of the following strings: 'Fortsetzung von Seite' (continuantion from page) or 'FORTSEZUNG VON SEITE'. We select the artilces that meet this critetion and try to merge them with their beginnings.\n",
    "\n",
    "However, we also exclude the articles that contain strings from both lists, `continued_strings_1` and `continued_strings_2`. This is because these chained articles have already been merged by the Handelsblatt team."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "continued_strings_1 = ['Fortsetzung S\\\\.', 'Fortsetzung auf Seite', 'Fortsetzung Seite', 'Fortsetzung n√§chste Seite',\n",
    "                       'Fortsetzung Seiten', 'FORTSETZUNG S\\\\.', 'FORTSETZUNG AUF SEITE', 'FORTSETZUNG SEITE', \n",
    "                       'FORTSETZUNG N√ÑCHSTE SEITE', 'FORTSETZUNG SEITEN']\n",
    "continued_strings_2 = ['Fortsetzung von Seite', 'FORTSETZUNG VON SEITE']\n",
    "\n",
    "# Articles that contain one of the strings from 'continued_strings_1' and one of the strings from \n",
    "# 'continued_strings_2' are examples of chained articles that have already been merged. \n",
    "continued_articles = data[(data.texts.str.contains('|'.join(continued_strings_2))) & \n",
    "                         (~data.texts.str.contains('|'.join(continued_strings_1)))]\n",
    "continued_articles = continued_articles.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "844"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# There are 844 articles that could potentially be part of the chained articles.\n",
    "len(continued_articles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An input of the function 'chained_articles' is a tuple, where the first element is a row of the dataframe corresponding to the second part of the chained article. The second element of the tuple is a dataframe containing all articles published on the same day as the second part of the chained article, i.e. all articles that could potentially be the first part of the chained article."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Required input for the function 'chained_articles': a row of the 'continued_articles' df, and a subset of the\n",
    "# 'data' df corresponding to the articles published on the same day as the considered chained article.\n",
    "\n",
    "# List with the potentially chained articles\n",
    "cont_input = []\n",
    "# List with the dataframes containing articles published on the same day as the considered chained article\n",
    "data_input = []\n",
    "\n",
    "# To merge the articles from 'chain_exceptions' with the corresponding first parts, more complicated rules \n",
    "# are required than those we use in the funciton 'chained_articles'. Therefore, we list them as exceptions.\n",
    "chain_exceptions = [139, 140, 185]\n",
    "# An input for the function 'chained_articles'.\n",
    "for ind in continued_articles.index:\n",
    "    if ind not in chain_exceptions:\n",
    "        cont_input.append(continued_articles.iloc[[ind]])\n",
    "        data_input.append(data[(data.day == continued_articles['day'][ind]) & \n",
    "                      (data.month == continued_articles['month'][ind]) &\n",
    "                     (data.year == continued_articles['year'][ind])])\n",
    "    \n",
    "inputs_cont = list(zip(cont_input, data_input))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function 'chained_articles' returns a list with two tuples. The first tuple contains two indices corresponging to the first and second part of a chained article. These two parts must be merged into one article.\n",
    "    \n",
    "The second tuple contains two indices as well. The first index corresponds to a part of the chained article that is a duplicate because the merged version of this chained article already exists in the database. The article with this index must be deleted. The second index corresponds to the merged version of the chained article. This article must be kept in the database.\n",
    "\n",
    "All the rules we use to find the first part of a chained article are described in detail in the function 'chained_articles'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of cores that will be used: 12\n"
     ]
    }
   ],
   "source": [
    "import multiprocessing as mp # use multiprocessing module for parallel computing\n",
    "\n",
    "NUM_CORE = mp.cpu_count()-4 # set the number of cores to use\n",
    "\n",
    "print(\"The number of cores that will be used: {}\".format(NUM_CORE))\n",
    "\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:00:04.064377\n"
     ]
    }
   ],
   "source": [
    "startTime = datetime.now() # track time\n",
    "\n",
    "import chained_articles # import a function that outputs two parts of a chained article and duplicated articles\n",
    "chained_pair = []\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    pool = mp.Pool(NUM_CORE)\n",
    "    chained_intermediate = pool.map(chained_articles.chained_articles, inputs_cont)\n",
    "    chained_pair = chained_pair + chained_intermediate \n",
    "    pool.close()\n",
    "    pool.join()\n",
    "    \n",
    "print(datetime.now()-startTime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "206"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The list of unique tuples, where the first element is a duplicate and \n",
    "# the second element is a merged chained article.\n",
    "duplicates = list(set([sublist[1] for sublist in chained_pair if sublist[1] != (-1, -1)]))\n",
    "# The list with indices of all the duplicated/merged articles.\n",
    "dup_merged = [tup[0] for tup in duplicates]+[tup[1] for tup in duplicates]\n",
    "# The list with all the chained articles that we would like to merge.\n",
    "chained_pair_list = [sublist[0] for sublist in chained_pair if sublist[0] != ()]\n",
    "# Exclude duplicated/merged articles from the list of the chained articles.\n",
    "chained_pair_list = [pair for pair in chained_pair_list if (pair[0] not in dup_merged) and \\\n",
    "                     (pair[1] not in dup_merged)]\n",
    "# The rules we use allow us to merge 206 chained articles\n",
    "len(chained_pair_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'AUSL√ÑNDISCHE B√ñRSEN / Alle Indizes verbuchen Gewinne - London geschlossen. Aktienm√§rkte in Europa begr√ºssen das neue Jahr mit steigenden Notierungen. cbu D√úSSELDORF. Die europ√§ischen Aktienm√§rkte gingen sehr freundlich in das neue Jahr. Alle f√ºhrenden Indizes lagen im Plus, die wichtige B√∂rse in London blieb aber geschlossen. In Fernost erreichten Singapur und Hongkong neue Rekordst√§nde.'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# An example of the first part\n",
    "data['texts'][chained_pair_list[0][0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'AUSL√ÑNDISCHE B√ñRSEN / Fortsetzung von Seite 15. Weitere Rekorde in Fernost Tokio: Geschlossen. Wegen eines Feiertages blieben Beh√∂rden, Banken und M√§rkte geschlossen.'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# An example of the second part\n",
    "data['texts'][chained_pair_list[0][1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The indices of the first part of the chained articles.\n",
    "first_part_ind = [pair[0] for pair in chained_pair_list]\n",
    "# The indices of the second part of the chained articles.\n",
    "second_part_ind = [pair[1] for pair in chained_pair_list]\n",
    "# Merge the first and the second parts.\n",
    "data.loc[first_part_ind, 'texts'] = data.loc[first_part_ind, 'texts'].values + \\\n",
    "\" \" + data.loc[second_part_ind, 'texts'].values\n",
    "# Drop the second part.\n",
    "data.drop(data.index[second_part_ind], inplace = True)\n",
    "# The indices of the duplicates.\n",
    "dup_indices = [tup[0] for tup in duplicates]\n",
    "# Drop the duplicates.\n",
    "data.drop(data.index[dup_indices], inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:00:00.446067\n"
     ]
    }
   ],
   "source": [
    "import count_words_mp # import the function calculating the number of words in a text\n",
    "# Calculate the word count of the merged articles.\n",
    "\n",
    "startTime = datetime.now() # track time\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    pool = mp.Pool(NUM_CORE)\n",
    "    count_results = pool.map(count_words_mp.count_words_mp, [text for text in data.loc[first_part_ind, 'texts']]) \n",
    "    pool.close()\n",
    "    pool.join()\n",
    "    \n",
    "print(datetime.now()-startTime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace the \"word_count\" of the merged articles.\n",
    "data.loc[first_part_ind, 'word_count'] = count_results\n",
    "# Drop short articles.\n",
    "data = data[data['word_count']>=100]\n",
    "data.reset_index(inplace=True, drop=True) # reset the index of the DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handelsblatt-specific problems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a few small problems that are specific to Handelsblatt and that we have noticed while fixing other issues. \n",
    "\n",
    "&nbsp; &nbsp; 1. The end of the text with the index 205950 contains unusual symbols. We have removed this part from the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' k{ ! 4 6 8 L v t v x √ó √ô √ë √ì √ö √ú qqjjjjjjjjjjjjjjjjjj ¬∑ n# √ú ¬∞ ¬≤ √å √é - ` b d f xxxxxxxxxxx f ) √ú f * + Times New Roman √Ö ¬• n √ÜA . √Ö √Ö n √ÜA . √Ö √Ö K C o m p O b j √ø√ø√ø√ø√ø√ø√ø√ø√ø√ø√ø√ø U √ø√ø√ø√ø√ø√ø√ø√ø√ø√ø√ø√ø √ø√ø√ø√ø√ø√ø√ø√ø√ø√ø√ø√ø √ø√ø√ø√ø√ø√ø√ø√ø√ø√ø√ø√ø'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This is the part we delete.\n",
    "data['texts'][205950][data['texts'][205950].find(' k{'):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.at[205950, 'texts'] = data['texts'][205950][:data['texts'][205950].find(' k{')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&nbsp; &nbsp; 2. Some texts contain unicode errors. For example, the name Jaros≈Çaw is written as Jaros√Ö‚Äöaw. We fix these errors using the function `unicode_correct`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:00:01.206759\n"
     ]
    }
   ],
   "source": [
    "import unicode_correct # import the function correcting the unicode errors present in the database\n",
    "\n",
    "startTime = datetime.now() # track time\n",
    "\n",
    "to_correct = list(data[data.texts.str.contains('√Ö')].index)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    pool = mp.Pool(NUM_CORE)\n",
    "    unicode_corrected = pool.map(unicode_correct.unicode_correct, [text for text in data.loc[to_correct, 'texts']]) \n",
    "    pool.close()\n",
    "    pool.join()\n",
    "    \n",
    "print(datetime.now()-startTime)\n",
    "\n",
    "# Replace the texts with the corrected version of the articles.\n",
    "data.loc[to_correct, 'texts'] = unicode_corrected"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&nbsp; &nbsp; 3. We drop two texts that contain a string 'MMHANDELSBLATT' because they include a lot of errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'MMHANDELSBLATT, Sa./So., 28./29.9.96√ú PP3.6HECX0SV19,102,-S6VZS8 BH196.0 KO 73.7 MMhjk BERLIN. Gegen eine Aufwei√ú SC.3 chung der Maastrichter Konver\" genzkriterien hat sich der Vorsitzen\" de der CDU/CSU-Bundestagsfrak\" tion, Wolfgang Sch√§uble, in Berlin \" ausgesprochen. Deutschland, das \" sich dieses Jahr relativ weit von dem \" H√∂chstwert der √∂ffentlichen Neuver\" schuldung von 3 % des Bruttoin\" landsprodukts entfernt habe, k√∂nne \" sich dies im Referenzjahr 1997 nicht \" mehr leisten. Nach dem gerade ver\" abschiedeten Sparpaket seien daher \" weitere Sparma√ünahmen erforder\" lich. Es gebe keinerlei politischen \" Spielraum zur Interpretation der \" Maastrichter Kriterien. Bonn werde \" alles tun, damit Deutschland an der \" EWWU ab Anfang 1999 teilnehmen \" k√∂nne. ZS Seite 9\" SC0PP73.7HECX0ZB132VE0 SV19,S7,-S6VZS8 BLGS000 000 000 000 000 000 000 000BLGS116 116 000 000 000 000 000 000BLBT000 000 000 000 000 000 000 000 bla,2,116 mbl Erschwerniszulage bei Rationalisierung CX0SV19,69,-S6VZS8 BH9.5'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['texts'][71133][data['texts'][71133].find('MMHANDELSBLATT'):(data['texts'][71133].find('MMHANDELSBLATT')+1000)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices_drop = list(data[data.texts.str.contains('MMHANDELSBLATT')].index)\n",
    "data.drop(data.index[indices_drop], inplace = True)\n",
    "data.reset_index(inplace=True, drop=True) # reset the index of the DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&nbsp; &nbsp; 4. We drop 12 texts with broken umlauts and too many non-systematic mistakes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:00:43.859227\n"
     ]
    }
   ],
   "source": [
    "startTime = datetime.now()\n",
    "\n",
    "# Find the words like '}ber' and 'gel|st' in the articles' texts.\n",
    "import find_umlaut\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    pool = mp.Pool(NUM_CORE)\n",
    "    problem_umlaut = pool.map(find_umlaut.find_umlaut, [text for text in data['texts']]) \n",
    "    pool.close()\n",
    "    pool.join()\n",
    "\n",
    "print(datetime.now()-startTime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Indices of potentially problematic articles.\n",
    "problem_umlaut_list = [tup[0] for tup in enumerate(problem_umlaut) if tup[1] != []]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The last five articles contain unusual words with '|' inside. However, in these cases, '|' does not correspond\n",
    "# to the umlaut '√∂'. Example: 'netzwerk|nordbayern'.\n",
    "data.drop(data.index[problem_umlaut_list[:-5]], inplace = True)\n",
    "data.reset_index(inplace=True, drop=True) # reset the index of the DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&nbsp; &nbsp; 5. Drop the text with an index 43024 because it is written in French (we have tested that all the other articles are in German)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.drop(data.index[43024], inplace = True)\n",
    "data.reset_index(inplace=True, drop=True) # reset the index of the DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean articles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Handelsblatt articles include some text passages that are unlikely to be relevant for either topic modeling or sentiment analysis. We decided to clean the affected articles from these text passages to make the analysis easier for our models.\n",
    "\n",
    "We remove the following information from the texts:\n",
    "\n",
    "   * 1) additional information in the text meant for the author\n",
    "   * 2) the list of articles on a particular topic\n",
    "   * 3) reference to additional information\n",
    "   * 4) the calendar of events\n",
    "   * 5) the composition of an index\n",
    "   * 6) the box with additional information on the topic\n",
    "   * 7) the addresses/Internet addresses\n",
    "   * 8) the links\n",
    "   * 9) the word game for investors\n",
    "   * 10) information on how special pages on the topic can be ordered\n",
    "   * 11) educational articles on how to understand financial news\n",
    "   * 12) announcement of premieres\n",
    "   * 13) announcement of trade fairs\n",
    "   * 14) announcement of events\n",
    "   * 15) place and time/organizer of the event"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:06:04.096771\n"
     ]
    }
   ],
   "source": [
    "startTime = datetime.now()\n",
    "\n",
    "import clean_hb_articles\n",
    "if __name__ == \"__main__\":\n",
    "    pool = mp.Pool(NUM_CORE)\n",
    "    cleaned_articles = pool.map(clean_hb_articles.clean_hb_articles, [text for text in data['texts']]) \n",
    "    pool.close()\n",
    "    pool.join()\n",
    "    \n",
    "print(datetime.now()-startTime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['texts'] = cleaned_articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:00:26.280382\n"
     ]
    }
   ],
   "source": [
    "import count_words_mp # import the function calculating the number of words in a text\n",
    "# Calculate an updated word count of the cleaned articles.\n",
    "\n",
    "startTime = datetime.now() # track time\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    pool = mp.Pool(NUM_CORE)\n",
    "    count_results = pool.map(count_words_mp.count_words_mp, [text for text in data['texts']]) \n",
    "    pool.close()\n",
    "    pool.join()\n",
    "    \n",
    "print(datetime.now()-startTime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update the word count in the data frame.\n",
    "data['word_count'] = count_results\n",
    "# Drop short articles.\n",
    "data = data[data['word_count']>=100]\n",
    "data.reset_index(inplace=True, drop=True) # reset the index of the DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "578573"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the number of articles after excluding articles shorter than 100 words\n",
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data.to_csv('hb_prepro_final.csv', encoding = 'utf-8-sig', sep = ';')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py3",
   "language": "python",
   "name": "py3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
