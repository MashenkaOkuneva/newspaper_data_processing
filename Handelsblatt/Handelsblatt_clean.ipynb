{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Handelsblatt data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Handelsblatt* is a popular German daily newspaper. According to the IVW, Informationsgemeinscahft zur Feststellung der Verbreitung von Werbeträgern (Information Community for the Assessment of the Circulation of Media), it had a circulation of 140612 daily copies in the first quarter of 2021. An appealing feature of Handelsblatt for forecasting economic activity is its focus on the economy.\n",
    "\n",
    "We purchased Handelsblatt data from Genios, a German provider of business information. The corpus includes **980516** articles from January 1994 to November 2018. The data was purchased in February 2019. \n",
    "\n",
    "The data set includes 25 subfolders corresponding to a particular year (e.g., HB_1994). Each subfolder contains a few XML files which we parse to extract information relevant to our research project. Unfortunately, the copyright prevents us from publishing the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we need to read in the data. We create the list including the names of the 25 subfolders (`folder_list`) and apply the function `hb_load` to them in parallel by exploiting Python's `multiprocessing` library."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We extract the following XML elements:\n",
    "\n",
    "* datum - publication date\n",
    "* worte - the word count\n",
    "* ressort - section/subsection of the newspaper\n",
    "* titel-liste/serientitel - the name of the series\n",
    "* quelle/name - the name of the newspaper\n",
    "* titel-liste/titel - article's title\n",
    "* titel-liste/dachzeile - article's kicker\n",
    "* titel-liste/untertitel - article's subheading\n",
    "* inhalt/vorspann - annotation\n",
    "* inhalt/text - text of the article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Handelsblatt is the main folder with 25 subfolders in it\n",
    "path = 'E:\\\\Userhome\\\\mokuneva\\\\Handelsblatt' # your path here\n",
    "\n",
    "# Create the list of all subfolders within Hb main folder.\n",
    "folder_list=[]\n",
    "\n",
    "for f in [f for f in os.listdir(path) ]: \n",
    "    # os.listdir(path) - names of directories                                         \n",
    "    folder_list.append(path + '\\\\' + f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of cores that will be used: 12\n"
     ]
    }
   ],
   "source": [
    "import multiprocessing as mp # use multiprocessing module for parallel computing\n",
    "\n",
    "NUM_CORE = mp.cpu_count()-4 # set the number of cores to use\n",
    "\n",
    "print(\"The number of cores that will be used: {}\".format(NUM_CORE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:01:16.444038\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "startTime = datetime.now() # track time\n",
    "\n",
    "import pandas as pd # load pandas: python data analysis library\n",
    "import hb_load # import a function that loads the data from one folder (see hb_load.py file for details)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    pool = mp.Pool(NUM_CORE)\n",
    "    data_intermediate = pool.map(hb_load.hb_load, folder_list) # load data from each folder in parallel\n",
    "    data = pd.concat(data_intermediate) # concatenate DataFrames corresponding to different folders\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "    \n",
    "print(datetime.now()-startTime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.sort_values(['year', 'month', 'day'], ascending=[True, True, True]) # sort the data in chronological order\n",
    "data = data.reset_index() # reset the index of the DataFrame\n",
    "del data['index'] # delete a column with an old index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>day</th>\n",
       "      <th>kicker</th>\n",
       "      <th>month</th>\n",
       "      <th>newspaper</th>\n",
       "      <th>rubrics</th>\n",
       "      <th>series_title</th>\n",
       "      <th>texts</th>\n",
       "      <th>title</th>\n",
       "      <th>title_only</th>\n",
       "      <th>word_c</th>\n",
       "      <th>year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>3</td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "      <td>Handelsblatt</td>\n",
       "      <td>Nachrichten</td>\n",
       "      <td></td>\n",
       "      <td>Nachrichten. Yuan abgewertet dpa PEKING. China...</td>\n",
       "      <td>Nachrichten.</td>\n",
       "      <td>Nachrichten.</td>\n",
       "      <td>68</td>\n",
       "      <td>1994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>3</td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "      <td>Handelsblatt</td>\n",
       "      <td>Nachrichten</td>\n",
       "      <td></td>\n",
       "      <td>Nachrichten. Ruecktritt abgelehnt afp NEU DELH...</td>\n",
       "      <td>Nachrichten.</td>\n",
       "      <td>Nachrichten.</td>\n",
       "      <td>58</td>\n",
       "      <td>1994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>3</td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "      <td>Handelsblatt</td>\n",
       "      <td>Nachrichten</td>\n",
       "      <td></td>\n",
       "      <td>Nachrichten. Ungarn wertet Forint ab rtr BUDAP...</td>\n",
       "      <td>Nachrichten.</td>\n",
       "      <td>Nachrichten.</td>\n",
       "      <td>33</td>\n",
       "      <td>1994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>3</td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "      <td>Handelsblatt</td>\n",
       "      <td>Nachrichten</td>\n",
       "      <td></td>\n",
       "      <td>Nachrichten. FDP zum Beamtenrecht dpa BONN. Di...</td>\n",
       "      <td>Nachrichten.</td>\n",
       "      <td>Nachrichten.</td>\n",
       "      <td>55</td>\n",
       "      <td>1994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>3</td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "      <td>Handelsblatt</td>\n",
       "      <td>Nachrichten</td>\n",
       "      <td></td>\n",
       "      <td>Nachrichten. Neue Drohungen dpa HAMBURG. Der F...</td>\n",
       "      <td>Nachrichten.</td>\n",
       "      <td>Nachrichten.</td>\n",
       "      <td>86</td>\n",
       "      <td>1994</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    day kicker  month     newspaper      rubrics series_title  \\\n",
       "10    3             1  Handelsblatt  Nachrichten                \n",
       "11    3             1  Handelsblatt  Nachrichten                \n",
       "12    3             1  Handelsblatt  Nachrichten                \n",
       "13    3             1  Handelsblatt  Nachrichten                \n",
       "14    3             1  Handelsblatt  Nachrichten                \n",
       "\n",
       "                                                texts         title  \\\n",
       "10  Nachrichten. Yuan abgewertet dpa PEKING. China...  Nachrichten.   \n",
       "11  Nachrichten. Ruecktritt abgelehnt afp NEU DELH...  Nachrichten.   \n",
       "12  Nachrichten. Ungarn wertet Forint ab rtr BUDAP...  Nachrichten.   \n",
       "13  Nachrichten. FDP zum Beamtenrecht dpa BONN. Di...  Nachrichten.   \n",
       "14  Nachrichten. Neue Drohungen dpa HAMBURG. Der F...  Nachrichten.   \n",
       "\n",
       "      title_only  word_c  year  \n",
       "10  Nachrichten.      68  1994  \n",
       "11  Nachrichten.      58  1994  \n",
       "12  Nachrichten.      33  1994  \n",
       "13  Nachrichten.      55  1994  \n",
       "14  Nachrichten.      86  1994  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[10:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "980516"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the number of article before pre-processing\n",
    "len(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Light pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove short articles (<100 words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first part of the article 327123 contains different symbols (e.g. zw|lf) instead of umlauts. The second part (which I keep) has the same content with the right format. The second part starts with 'NEU DELHI.'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.at[327123, 'texts'] = data['texts'][327123][data['texts'][327123].find('NEU DELHI.'):]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An important feature of the text is its length. There are a few ways to count the number of words in a text:\n",
    "\n",
    "1. Use the metadata ('worte') of an XML file. Beware that numbers are counted as words. This is the column 'word_c' in the DataFrame. \n",
    "2. Quick and dirty approach: split the tokens by space and calculate the length of the list. Numbers and other non-alphabetic characters are counted as words. If a space is used as a delimiter in large numbers (100 000), this number will be counted as two tokens.\n",
    "3. Use `count_words_mp` function, which counts only words. This is our preferred method because we exclude numbers from the analysis in both sentiment analysis and topic modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the second approach\n",
    "# data['w_count'] = data['texts'].str.split(' ').str.len()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:00:40.811674\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "startTime = datetime.now() # track time\n",
    "\n",
    "import count_words_mp # import the function calculating the number of words in a text\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    pool = mp.Pool(NUM_CORE)\n",
    "    count_results = pool.map(count_words_mp.count_words_mp, [text for text in data['texts']]) \n",
    "    pool.close()\n",
    "    pool.join()\n",
    "    \n",
    "print(datetime.now()-startTime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the result as a new column \"word_count\"\n",
    "data['word_count'] = count_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The difference between the first ('word_c' column) and third ('word_count' column) approaches might be significant. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff = (data['word_c']-data['word_count'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[695, 691, 591, 430, 429, 429, 404, 390, 387, 387]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(diff, reverse=True)[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This happens because the third approach does not count a number as a word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Fußball-Bundesliga. \\\\\\\\\\\\\\\\\\\\\\\\\\\\ zu Hause\\\\ auswärts\\\\ Sp.\\\\ g.\\\\ u.\\\\ v.\\\\ Tore\\\\ Diff.\\\\ Pkt.\\\\ Sp.\\\\ g.\\\\ u.\\\\ v.\\\\ Tore\\\\ Pkt.\\\\ Sp.\\\\ g.\\\\ u.\\\\ v.\\\\ Tore\\\\ Pkt.\\\\ 1. ( 1)\\\\ Bayern München 34\\\\ 20\\\\ 11\\\\ 3\\\\ 68:34\\\\ +34\\\\ 71\\\\ 17\\\\ 13\\\\ 4\\\\ 0\\\\ 37:12\\\\ 43\\\\ 17\\\\ 7\\\\ 7\\\\ 3\\\\ 31:22\\\\ 28\\\\ 2. ( 2)\\\\ Bayer Leverkusen 34\\\\ 21\\\\ 6\\\\ 7\\\\ 69:41\\\\ +28\\\\ 69'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['texts'][list(diff).index(387)][:300]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because short texts have sparse semantic features, topic models and BOW-based sentiment tools perform better on longer texts. This is why we keep the texts longer than 100 words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove articles with the number of words<100\n",
    "data = data[data['word_count']>=100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "673430"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the number of articles after removing short articles\n",
    "len(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove exact duplicates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A few examples of duplicates in our corpus:\n",
    "* The same article enters the corpus twice with different publication dates (e.g., 6.4.1994 and 27.4.1994). In this case, a natural solution is to keep the first entry.\n",
    "* The same article appears twice with a slight variation in the metadata (e.g., one of the duplicates includes the title of the series, or the word count is a little different even though the articles are identical).\n",
    "* The same article enters the corpus twice with the same publication date and metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All the duplicated articles are saved as 'hb_duplicates' for further exploration.\n",
    "hb_duplicates = data[data['texts'].duplicated(keep = False)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>day</th>\n",
       "      <th>kicker</th>\n",
       "      <th>month</th>\n",
       "      <th>newspaper</th>\n",
       "      <th>rubrics</th>\n",
       "      <th>series_title</th>\n",
       "      <th>texts</th>\n",
       "      <th>title</th>\n",
       "      <th>title_only</th>\n",
       "      <th>word_c</th>\n",
       "      <th>year</th>\n",
       "      <th>word_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>18404</th>\n",
       "      <td>29</td>\n",
       "      <td></td>\n",
       "      <td>4</td>\n",
       "      <td>Handelsblatt</td>\n",
       "      <td>Weltwirtschaft</td>\n",
       "      <td></td>\n",
       "      <td>Importlizenzen im Vergleichsverfahren. HB DUES...</td>\n",
       "      <td>Importlizenzen im Vergleichsverfahren.</td>\n",
       "      <td>Importlizenzen im Vergleichsverfahren.</td>\n",
       "      <td>121</td>\n",
       "      <td>1994</td>\n",
       "      <td>112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18618</th>\n",
       "      <td>2</td>\n",
       "      <td></td>\n",
       "      <td>5</td>\n",
       "      <td>Handelsblatt</td>\n",
       "      <td>Weltwirtschaft</td>\n",
       "      <td></td>\n",
       "      <td>Importlizenzen im Vergleichsverfahren. HB DUES...</td>\n",
       "      <td>Importlizenzen im Vergleichsverfahren.</td>\n",
       "      <td>Importlizenzen im Vergleichsverfahren.</td>\n",
       "      <td>122</td>\n",
       "      <td>1994</td>\n",
       "      <td>112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29129</th>\n",
       "      <td>7</td>\n",
       "      <td></td>\n",
       "      <td>7</td>\n",
       "      <td>Handelsblatt</td>\n",
       "      <td>Finanzzeitung; Geld und Kredit</td>\n",
       "      <td></td>\n",
       "      <td>NACHTREPORT / Dow-Jones-Index steigt um 22 Pun...</td>\n",
       "      <td>NACHTREPORT / Dow-Jones-Index steigt um 22 Pun...</td>\n",
       "      <td>NACHTREPORT / Dow-Jones-Index steigt um 22 Pun...</td>\n",
       "      <td>638</td>\n",
       "      <td>1994</td>\n",
       "      <td>604</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29637</th>\n",
       "      <td>8</td>\n",
       "      <td></td>\n",
       "      <td>7</td>\n",
       "      <td>Handelsblatt</td>\n",
       "      <td>Finanzzeitung; Geld und Kredit</td>\n",
       "      <td></td>\n",
       "      <td>NACHTREPORT / Dow-Jones-Index steigt um 22 Pun...</td>\n",
       "      <td>NACHTREPORT / Dow-Jones-Index steigt um 22 Pun...</td>\n",
       "      <td>NACHTREPORT / Dow-Jones-Index steigt um 22 Pun...</td>\n",
       "      <td>636</td>\n",
       "      <td>1994</td>\n",
       "      <td>604</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32437</th>\n",
       "      <td>28</td>\n",
       "      <td></td>\n",
       "      <td>7</td>\n",
       "      <td>Handelsblatt</td>\n",
       "      <td>Wirtschaft und Politik</td>\n",
       "      <td></td>\n",
       "      <td>Glossar. Das Verarbeitende Gewerbe ist in West...</td>\n",
       "      <td>Glossar.</td>\n",
       "      <td>Glossar.</td>\n",
       "      <td>139</td>\n",
       "      <td>1994</td>\n",
       "      <td>130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35680</th>\n",
       "      <td>19</td>\n",
       "      <td></td>\n",
       "      <td>8</td>\n",
       "      <td>Handelsblatt</td>\n",
       "      <td>Karriere</td>\n",
       "      <td></td>\n",
       "      <td>Solide Basisausbildung zu DDR-Zeiten. Ost-Inge...</td>\n",
       "      <td>Solide Basisausbildung zu DDR-Zeiten. Ost-Inge...</td>\n",
       "      <td>Solide Basisausbildung zu DDR-Zeiten. Ost-Inge...</td>\n",
       "      <td>176</td>\n",
       "      <td>1994</td>\n",
       "      <td>169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35962</th>\n",
       "      <td>19</td>\n",
       "      <td></td>\n",
       "      <td>8</td>\n",
       "      <td>Handelsblatt</td>\n",
       "      <td>Karriere</td>\n",
       "      <td></td>\n",
       "      <td>Solide Basisausbildung zu DDR-Zeiten. Ost-Inge...</td>\n",
       "      <td>Solide Basisausbildung zu DDR-Zeiten. Ost-Inge...</td>\n",
       "      <td>Solide Basisausbildung zu DDR-Zeiten. Ost-Inge...</td>\n",
       "      <td>176</td>\n",
       "      <td>1994</td>\n",
       "      <td>169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40771</th>\n",
       "      <td>20</td>\n",
       "      <td></td>\n",
       "      <td>9</td>\n",
       "      <td>Handelsblatt</td>\n",
       "      <td>Unternehmen und Maerkte</td>\n",
       "      <td></td>\n",
       "      <td>Hochtief will seine Beteiligung bei Holzmann e...</td>\n",
       "      <td>Hochtief will seine Beteiligung bei Holzmann e...</td>\n",
       "      <td>Hochtief will seine Beteiligung bei Holzmann e...</td>\n",
       "      <td>738</td>\n",
       "      <td>1994</td>\n",
       "      <td>715</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40776</th>\n",
       "      <td>20</td>\n",
       "      <td></td>\n",
       "      <td>9</td>\n",
       "      <td>Handelsblatt</td>\n",
       "      <td>Unternehmen und Maerkte</td>\n",
       "      <td></td>\n",
       "      <td>Hochtief will seine Beteiligung bei Holzmann e...</td>\n",
       "      <td>Hochtief will seine Beteiligung bei Holzmann e...</td>\n",
       "      <td>Hochtief will seine Beteiligung bei Holzmann e...</td>\n",
       "      <td>738</td>\n",
       "      <td>1994</td>\n",
       "      <td>715</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41355</th>\n",
       "      <td>22</td>\n",
       "      <td></td>\n",
       "      <td>9</td>\n",
       "      <td>Handelsblatt</td>\n",
       "      <td>Wirtschaft und Politik; Konjunkturbarometer</td>\n",
       "      <td></td>\n",
       "      <td>Glossar. Das Verarbeitende Gewerbe ist in West...</td>\n",
       "      <td>Glossar.</td>\n",
       "      <td>Glossar.</td>\n",
       "      <td>133</td>\n",
       "      <td>1994</td>\n",
       "      <td>126</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       day kicker  month     newspaper  \\\n",
       "18404   29             4  Handelsblatt   \n",
       "18618    2             5  Handelsblatt   \n",
       "29129    7             7  Handelsblatt   \n",
       "29637    8             7  Handelsblatt   \n",
       "32437   28             7  Handelsblatt   \n",
       "35680   19             8  Handelsblatt   \n",
       "35962   19             8  Handelsblatt   \n",
       "40771   20             9  Handelsblatt   \n",
       "40776   20             9  Handelsblatt   \n",
       "41355   22             9  Handelsblatt   \n",
       "\n",
       "                                           rubrics series_title  \\\n",
       "18404                               Weltwirtschaft                \n",
       "18618                               Weltwirtschaft                \n",
       "29129               Finanzzeitung; Geld und Kredit                \n",
       "29637               Finanzzeitung; Geld und Kredit                \n",
       "32437                       Wirtschaft und Politik                \n",
       "35680                                     Karriere                \n",
       "35962                                     Karriere                \n",
       "40771                      Unternehmen und Maerkte                \n",
       "40776                      Unternehmen und Maerkte                \n",
       "41355  Wirtschaft und Politik; Konjunkturbarometer                \n",
       "\n",
       "                                                   texts  \\\n",
       "18404  Importlizenzen im Vergleichsverfahren. HB DUES...   \n",
       "18618  Importlizenzen im Vergleichsverfahren. HB DUES...   \n",
       "29129  NACHTREPORT / Dow-Jones-Index steigt um 22 Pun...   \n",
       "29637  NACHTREPORT / Dow-Jones-Index steigt um 22 Pun...   \n",
       "32437  Glossar. Das Verarbeitende Gewerbe ist in West...   \n",
       "35680  Solide Basisausbildung zu DDR-Zeiten. Ost-Inge...   \n",
       "35962  Solide Basisausbildung zu DDR-Zeiten. Ost-Inge...   \n",
       "40771  Hochtief will seine Beteiligung bei Holzmann e...   \n",
       "40776  Hochtief will seine Beteiligung bei Holzmann e...   \n",
       "41355  Glossar. Das Verarbeitende Gewerbe ist in West...   \n",
       "\n",
       "                                                   title  \\\n",
       "18404             Importlizenzen im Vergleichsverfahren.   \n",
       "18618             Importlizenzen im Vergleichsverfahren.   \n",
       "29129  NACHTREPORT / Dow-Jones-Index steigt um 22 Pun...   \n",
       "29637  NACHTREPORT / Dow-Jones-Index steigt um 22 Pun...   \n",
       "32437                                           Glossar.   \n",
       "35680  Solide Basisausbildung zu DDR-Zeiten. Ost-Inge...   \n",
       "35962  Solide Basisausbildung zu DDR-Zeiten. Ost-Inge...   \n",
       "40771  Hochtief will seine Beteiligung bei Holzmann e...   \n",
       "40776  Hochtief will seine Beteiligung bei Holzmann e...   \n",
       "41355                                           Glossar.   \n",
       "\n",
       "                                              title_only  word_c  year  \\\n",
       "18404             Importlizenzen im Vergleichsverfahren.     121  1994   \n",
       "18618             Importlizenzen im Vergleichsverfahren.     122  1994   \n",
       "29129  NACHTREPORT / Dow-Jones-Index steigt um 22 Pun...     638  1994   \n",
       "29637  NACHTREPORT / Dow-Jones-Index steigt um 22 Pun...     636  1994   \n",
       "32437                                           Glossar.     139  1994   \n",
       "35680  Solide Basisausbildung zu DDR-Zeiten. Ost-Inge...     176  1994   \n",
       "35962  Solide Basisausbildung zu DDR-Zeiten. Ost-Inge...     176  1994   \n",
       "40771  Hochtief will seine Beteiligung bei Holzmann e...     738  1994   \n",
       "40776  Hochtief will seine Beteiligung bei Holzmann e...     738  1994   \n",
       "41355                                           Glossar.     133  1994   \n",
       "\n",
       "       word_count  \n",
       "18404         112  \n",
       "18618         112  \n",
       "29129         604  \n",
       "29637         604  \n",
       "32437         130  \n",
       "35680         169  \n",
       "35962         169  \n",
       "40771         715  \n",
       "40776         715  \n",
       "41355         126  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hb_duplicates[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop the exact duplicates, keep the article with the earlier publication date ('first')\n",
    "data.drop_duplicates(['texts'], keep = 'first', inplace=True)\n",
    "data = data.reset_index() # reset the index of the DataFrame\n",
    "del data['index'] # delete a column with an old index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "670682"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the number of articles after removing exact duplicates\n",
    "len(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filtering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Handelsblatt's articles are organized into 2073 unique sections/subsections. We investigate the most frequently met ones and remove artiles published within a few of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Unternehmen und Märkte', 87617), ('Wirtschaft und Politik', 33223), ('Deutschland', 29652), ('Finanzzeitung', 28197), ('Unternehmen & Märkte', 19289), ('Meinung und Analyse', 17238), ('Titelseite', 16957), ('Beilage oder Sonderseite', 16278), ('International', 16252), ('Europa', 15709)]\n"
     ]
    }
   ],
   "source": [
    "# Sections and the number of articles per section\n",
    "import collections\n",
    "counter_sections = collections.Counter(data.rubrics)\n",
    "print(counter_sections.most_common(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To ensure consistency in topical content over time, we remove articles from a few sections that were covered only within a limited time period:\n",
    "\n",
    "* 1) 'Karriere': news related to work, career; due to the organizational changes, starting from 2014, this section is only present in a weekend edition of the newspaper.\n",
    "* 2) 'Weekend Journal': the section was introduced in 2002 and did not receive any coverage after 2008. Moreover, the news discussed within this section is on average longer (663 words) than the news from the main section of interest \"Wirtschaft und Politik\" (436 words).\n",
    "* 3) 'Panorama': news from around the world, issued in 1997, existed until 2001.\n",
    "* 4) 'Business-Travel': travel-related news, e.g. weather, 2001-2006, only 366 articles in total.\n",
    "* 5) 'Fortschritt': scientific findings, new devices, 2001-2003.\n",
    "* 6) 'Wochenende': weekend news, 2012-2018, average article length is 1579.\n",
    "* 7) 'perspektiven': perspectives, non-economic topics like a prize for women in science, working abroad, or new beauty standards popularized by Dove's advertising campaign; 2007-2009."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "abandoned_sections = ['Karriere', 'Weekend Journal', 'Panorama', 'Business-Travel', \n",
    "                    'Fortschritt', 'Wochenende', 'perspektiven']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data[~data['rubrics'].isin(abandoned_sections)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "658481"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the number of articles after removing articles from the sections covered only within a certain time period\n",
    "len(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we want to exclude the articles from non-economic sections. The list of irrelevant sections follows.\n",
    "\n",
    "* 1) 'Sportreport': sport news.\n",
    "* 2) 'Kunstmarkt': art market.\n",
    "* 3) 'Sportreport      Nachrichten': sport news.\n",
    "* 4) 'Neue Bücher': an advertisement for new books.\n",
    "* 5) 'Wissenschaft & Debatte': science and debate.\n",
    "* 6) 'Literatur': literature.\n",
    "* 7) 'Leserbriefe': letters to the editor; starting from 2013, there are almost no letters/comments from the readers, because all the comments became digital.\n",
    "* 8) 'Reisen und Tagen': travel.\n",
    "* 9) 'Kultur': culture.\n",
    "* 10) 'Termin- und Optionsmärkte': futures and options markets, mostly quantitative info.\n",
    "* 11) 'Galerie des Stils': style and fashion.\n",
    "* 12) 'Galerie': non-economic news about a pop band, Nutella restaurant etc.\n",
    "* 13) 'Neue Buecher': new books.\n",
    "* 14) 'Online': news about the websites, digitalization, online services.\n",
    "* 15) 'Forschung und Technik': research and technology.\n",
    "* 16) 'Business-Service': news unrelated to the economy, e.g. telephone service for weather forecast, travel information etc.\n",
    "* 17) 'Computer und Kommunikation': computers and communication.\n",
    "* 18) 'Ökonomie & Bildung': economics and education, Handelsblatt's materials for school lessons, MBA studies, educating articles about how the economy works.\n",
    "* 19) 'Auto-Mobil': new car models.\n",
    "* 20) 'Die Handelsblatt-Woche': announcement of events taking place this week.\n",
    "* 21) 'Wirtschaftsbuch': economics books."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_economic_sections = ['Kunstmarkt', 'Sportreport', 'Sportreport      Nachrichten', \n",
    "                  'Neue Bücher', 'Wissenschaft & Debatte',  \n",
    "                  'Literatur', 'Leserbriefe', 'Reisen und Tagen', \n",
    "                  'Kultur',  'Termin- und Optionsmärkte',\n",
    "                  'Galerie des Stils', 'Galerie', 'Neue Buecher', 'Online', 'Forschung und Technik', \n",
    "                  'Business-Service', 'Computer und Kommunikation', 'Ökonomie & Bildung', 'Auto-Mobil', \n",
    "                  'Die Handelsblatt-Woche', 'Wirtschaftsbuch']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data[~data['rubrics'].isin(non_economic_sections)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "621317"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the number of articles after removing articles from the non-economic sections\n",
    "len(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Additionally, we remove articles from the related sections and subsections. \n",
    "\n",
    "* This means that we not only remove articles from a section 'Kunstmarkt', but also from the subsections 'Kunstmarkt      Anleihen', 'Kunstmarkt      Aus den Galerien' etc. \n",
    "\n",
    "* We consider 'Kunstmarkt' and 'Themen und Trends      Kunstmarkt' as two close sections. \n",
    "\n",
    "* The full list of sections/subsections we remove can be found in 'subsections_hb.txt'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this function loads the dictionary with the related sections and subsections we want to exclude\n",
    "from os import path\n",
    "import codecs\n",
    "\n",
    "def dictionary_open(name):\n",
    "    with codecs.open(path.join(os.getcwd(), name),\n",
    "               'r',  'utf-8') as f:\n",
    "          dictionary = set(f.read().splitlines()[1:-1])\n",
    "    return dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "subsections_clean = dictionary_open('subsections_hb.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data[~data['rubrics'].isin(subsections_clean)]\n",
    "data = data.reset_index() # reset the index of the DataFrame\n",
    "del data['index'] # delete a column with an old index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "588837"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the number of articles after removing articles from the related subsections\n",
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Unternehmen und Märkte', 87617), ('Wirtschaft und Politik', 33223), ('Deutschland', 29652), ('Finanzzeitung', 28197), ('Unternehmen & Märkte', 19289), ('Meinung und Analyse', 17238), ('Titelseite', 16957), ('Beilage oder Sonderseite', 16278), ('International', 16252), ('Europa', 15709)]\n"
     ]
    }
   ],
   "source": [
    "# Sections and the number of articles per section\n",
    "counter_sections=collections.Counter(data['rubrics'])\n",
    "print(counter_sections.most_common(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section + text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove articles for which the following two conditions are met: a section is '' (unclassified), and a text contains a string 'KARRIERE'. As discussed, this type of articles is removed due to organizational changes (see above)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data[~((data['rubrics']=='') & (data['texts'].str.contains('KARRIERE')))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove articles that are characterized by the following conditions: a section contains a string \"Recht und Steuern\" (Law and taxes), and a text contains one of the legal terms from the list `legal_terms`.\n",
    "\n",
    "* Az./AZ/Aktenzeichen/AKTENZEICHEN - docket number\n",
    "* LAG/Landesarbeitsgericht - Regional Labour Court\n",
    "* BAG/Bundesarbeitsgericht - Federal Labour Court\n",
    "* VI R/IV A - case/law reference\n",
    "* BFH - Bundesfinanzhof, German Federal Fiscal court\n",
    "* Abs. - section\n",
    "* BGH/Bundesgerichtshof - Federal Supreme Court\n",
    "* BGB - Bürgerliches Gesetzbuch, Civil Code\n",
    "* BSG - Bundessozialgericht, Federal Social Court\n",
    "* EStG/Einkommensteuergesetz - Income Tax Act\n",
    "* OLG/Oberlandesgericht - Higher Regional Court\n",
    "* Anwaltgerichtshof - Lawyers' Court\n",
    "* Finanzgericht - Tax Court\n",
    "\n",
    "We remove these articles because they contain very detailed explanations of laws/tax deduction rules/court decisions which have a low chance to be relevant for forecasting economic activity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "legal_terms = ['Az\\\\.', 'AZ', 'Az\\\\:', 'Aktenzeichen', 'AKTENZEICHEN', 'LAG', 'Landesarbeitsgericht', 'Bundesarbeitsgericht', 'BAG', 'Arbeitsgericht', 'VI R','IV A','BFH','Abs\\\\.','BGH', 'Bundesgerichtshof', 'BGB','BSG','EStG','Einkommensteuergesetz','OLG', 'Oberlandesgericht', 'Anwaltgerichtshof', 'Finanzgericht']\n",
    "data = data[~((data.rubrics.str.contains('Recht und Steuern')) & (data.texts.str.contains('|'.join(legal_terms))))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "583767"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the number of articles after filtering out articles based on the section and text\n",
    "len(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section + title"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove articles satisfying two conditions: section is 'Inhalt' (content), and the title is not 'Termine des Tages.' Section 'Inhalt' mainly includes news reports about the Handelsblatt and its organization. The aritlces with the title 'Termine des Tages.' may include short news on the Economy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data[~(((data['rubrics']=='Inhalt') | (data['rubrics']=='Inhalt ;') | (data['rubrics']=='Inhalt      Der Werber-Rat'))\n",
    "              & (data['title']!='Termine des Tages.'))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "582998"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the number of articles after removing articles from the section 'Inhalt'\n",
    "len(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Title"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exclude articles with the following title patterns:\n",
    "\n",
    "* book suggestions (Buchtip)\n",
    "* some very short news on a few people (BUSINESS LOUNGE.)\n",
    "* articles about Handelsblatt and its organization (Liebe Leser)\n",
    "* articles with comments from the readers (DIE MEINUNG UNSERER LESER.)\n",
    "* announcement of events taking place this week (DIE HANDELSBLATT-WOCHE.)\n",
    "* non-economic news related to numbers (e.g., how many people watched the movie this week); Bilanz des Wochenendes.\n",
    "* sport news (SPORT TELEGRAMM.) \n",
    "* news related to the project 'Handelsblatt macht Schule' promoting economic education"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "title_patterns = ['BUCHTIP', 'Buchtip', 'BUSINESS LOUNGE\\\\.', 'Liebe Leser', 'DIE MEINUNG UNSERER LESER\\\\.', \n",
    "                 'DIE HANDELSBLATT\\\\-WOCHE\\\\.', 'Die Handelsblatt\\\\-Woche\\\\.', 'Die Handelsblattwoche\\\\.',\n",
    "                 'DIE HANDELSBLATTWOCHE\\\\.', 'Bilanz des Wochenendes\\\\.', 'BILANZ DES WOCHENENDES\\\\.',\n",
    "                 'SPORT TELEGRAMM\\\\.']\n",
    "data = data[~data['title_only'].str.contains('|'.join(title_patterns))]\n",
    "\n",
    "kicker_title_patterns = ['Handelsblatt macht Schule', 'HANDELSBLATT MACHT SCHULE\\\\.', \n",
    "                         'Aktuelles Wirtschaftswissen für den Unterricht', \n",
    "                         'Wirtschaftswissen für Schüler leicht gemacht',\n",
    "                        'Ökonomie ganz leicht gemacht für Jugendliche',\n",
    "                        'Aktuelles Wirtschaftswissen für Jugendliche',\n",
    "                        'Neuer Stoff für den Unterricht im Fach Wirtschaft',\n",
    "                        'Handelsblatt bringt Praxis in die Schulen',\n",
    "                        'Experten aus der Praxis in den Schulen',\n",
    "                        'Der neue Newcomer ist da',\n",
    "                        'Die neue Zeitung für Schüler ist da',\n",
    "                        'Wettbewerb\\\\: Eine Welt ohne Geld\\\\?',\n",
    "                        'Wettbewerb zum Thema Geld',\n",
    "                        'Wettbewerb für Schüler',\n",
    "                        'Schlauere Azubis dank Zeitunglesen',\n",
    "                        'Förderung für Auszubildende'\n",
    "                        ]\n",
    "data = data[~data.title.str.contains('|'.join(kicker_title_patterns))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exclude articles with the following titles:\n",
    "\n",
    "* Kontakte. (contacts of organizations: addresses and phone numbers)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data[~(data['title_only']=='Kontakte.')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "580596"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the number of articles after excluding articles based on the title patterns\n",
    "len(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Series"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exclude articles from the following non-economic series:\n",
    "\n",
    "* Neue Wirtschaftsliteratur (Handelsblatt-Beilage): economic books;\n",
    "* Golf (Handelsblatt-Beilage): golf;\n",
    "* Literatur (Handelsblatt-Beilage): literature;\n",
    "* Macher des Handelsblatts (Handelsblatt-Serie): about journalists working for Handelsblatt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "series_exclude = ['Neue Wirtschaftsliteratur (Handelsblatt-Beilage)', \n",
    "                  'Golf (Handelsblatt-Beilage)', 'Literatur (Handelsblatt-Beilage)', \n",
    "                  'Literatur (Handelsblatt - Beilage)', 'Macher des Handelsblatts (Handelsblatt-Serie)']\n",
    "data = data[~data['series_title'].isin(series_exclude)]\n",
    "data = data.reset_index() # reset the index of the DataFrame\n",
    "del data['index'] # delete a column with an old index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "579948"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the number of articles after excluding articles from the non-economic series\n",
    "len(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exclude articles that contain the following strings:\n",
    "\n",
    "* TERMINMÄRKTE / Tagesbericht: options trading, mainly quantiative information;\n",
    "* Deutsche Börsen Düsseldorf: stock market, mainly quantitative information;\n",
    "* DIGIX. Der E-Business-Index. or DIGIX. DIGIX Der E-Business-Index.: text corresponding to a graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_strings = ['TERMINMÄRKTE \\\\/ Tagesbericht', 'TERMINMAERKTE \\\\/ Tagesbericht', \n",
    "                'Deutsche Börsen Düsseldorf', 'Deutsche Boersen Duesseldorf', 'DIGIX\\\\. Der E\\\\-Business\\\\-Index.',\n",
    "               'DIGIX\\\\. DIGIX Der E\\\\-Business\\\\-Index\\\\.']\n",
    "data = data[~(data.texts.str.contains('|'.join(text_strings)))]\n",
    "data = data.reset_index() # reset the index of the DataFrame\n",
    "del data['index'] # delete a column with an old index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "579822"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the number of articles after excluding articles based on the text patterns\n",
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({2000: 33949, 1999: 32861, 2001: 30730, 1996: 27326, 1997: 27084, 1998: 26911, 2004: 26432, 1995: 26169, 2002: 25083, 1994: 25072, 2005: 24726, 2003: 24350, 2006: 24167, 2007: 23816, 2010: 23811, 2008: 23520, 2009: 23329, 2011: 22196, 2012: 20667, 2013: 17506, 2014: 17397, 2015: 16297, 2016: 13103, 2017: 12152, 2018: 11168})\n"
     ]
    }
   ],
   "source": [
    "counter=collections.Counter(data['year'])\n",
    "print(counter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Umlauts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To fix the issue with umlauts, we use the notebook Umlauts_fix written in Python 2. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "umlauts = ['ä', 'ö', 'ü', 'ß', 'Ä', 'Ö', 'Ü']\n",
    "umlauts_replace = ['ae', 'oe', 'ue', 'ss', 'AE', 'OE', 'UE']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "hb_umlauts_fix = data[(data.texts.str.contains('|'.join(umlauts_replace))) & (~data.texts.str.contains('|'.join(umlauts))) & (data.year<1999)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1994 wird ein Jahr des Wandels. Auf ein Neues!. Steuern und Beitraege steigen, staatliche Leistungen werden gesenkt - wahrlich kein berauschender Auftakt fuer das neue Jahr. Ohnehin sind die Erwartungen nicht sehr rosig. Zwar wird sich die Wirtschaft erholen. Ob es zu einem kraeftigen Aufschwung reicht, ist ungewiss. Sicher ist dagegen der Anstieg der Arbeitslosigkeit. Fuer Spannung sorgt der Wahlmarathon, der erst mit den Bundestagswahlen im Oktober entschieden wird. Kein Zweifel: 1994 wird ein Jahr der Unsicherheit und des Wandels. Ob es auch ein gutes Jahr wird, haengt davon ab, wie dieser Wandel bewaeltigt wird. 1993 war das Jahr der Krisenerkenntnis. Die Einsicht in die Kluft zwischen wirtschaftlich Machbarem und Anspruchsdenken hat erste Ansaetze zur Krisenbewaeltigung ermoeglicht. In der Tarifpolitik und bei den Sozialleistungen hat es - noch vor kurzem undenkbare - Einschnitte gegeben. Auch 1994 wird es Kuerzungen geben. Nur: Mit dem Rotstift allein kann kein Ausweg aus der Krise gezeichnet werden. In den Unternehmen sind innovative Strategien gefragt. Anstatt den Guertel immer wieder ein wenig enger zu schnallen, sollte der Staat seine Rolle grundsaetzlich ueberdenken. Ein Rueckzug auf die Kernaufgaben schafft verlaessliche Rahmendaten. Und die sind in unsicheren Zeiten besonders wichtig. Die Krise muss als Herausforderung, Wandel als Chance zum Fortschritt aufgefasst werden. Dann kann 1994 ein gutes Jahr werden - selbst wenn es zu Sylvester im Portemonnaie noch nicht messbar ist.'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# example of the text that we try to fix with a spellchecker\n",
    "hb_umlauts_fix.texts[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "hb_umlauts_fix.to_csv('hb_umlauts_fix.csv', encoding='utf-8-sig', sep = ';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "X:\\conda\\envs\\py3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:2785: DtypeWarning: Columns (3) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "hb_umlauts_fixed = pd.read_csv('hb_umlauts_fixed.csv', encoding = 'utf-8', sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.loc[hb_umlauts_fixed['Unnamed: 0.1'], 'texts'] = hb_umlauts_fixed.texts.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1994 wird ein Jahr des Wandels. Auf ein Neues!. Steuern und Beiträge steigen, staatliche Leistungen werden gesenkt - wahrlich kein berauschender Auftakt für das neue Jahr. Ohnehin sind die Erwartungen nicht sehr rosig. Zwar wird sich die Wirtschaft erholen. Ob es zu einem kräftigen Aufschwung reicht, ist ungewiss. Sicher ist dagegen der Anstieg der Arbeitslosigkeit. Für Spannung sorgt der Wahlmarathon, der erst mit den Bundestagswahlen im Oktober entschieden wird. Kein Zweifel: 1994 wird ein Jahr der Unsicherheit und des Wandels. Ob es auch ein gutes Jahr wird, hängt davon ab, wie dieser Wandel bewältigt wird. 1993 war das Jahr der Krisenerkenntnis. Die Einsicht in die Kluft zwischen wirtschaftlich Machbarem und Anspruchsdenken hat erste Ansätze zur Krisenbewältigung ermöglicht. In der Tarifpolitik und bei den Sozialleistungen hat es - noch vor kurzem undenkbare - Einschnitte gegeben. Auch 1994 wird es Kürzungen geben. Nur: Mit dem Rotstift allein kann kein Ausweg aus der Krise gezeichnet werden. In den Unternehmen sind innovative Strategien gefragt. Anstatt den Gürtel immer wieder ein wenig enger zu schnallen, sollte der Staat seine Rolle grundsätzlich überdenken. Ein Rückzug auf die Kernaufgaben schafft verlässliche Rahmendaten. Und die sind in unsicheren Zeiten besonders wichtig. Die Krise muss als Herausforderung, Wandel als Chance zum Fortschritt aufgefasst werden. Dann kann 1994 ein gutes Jahr werden - selbst wenn es zu Sylvester im Portemonnaie noch nicht messbar ist.'"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fixed version\n",
    "data.texts[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove English articles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the next pre-processing step, we filter out news articles written in English. To do that, we use a **langdetect** library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:32:41.138686\n"
     ]
    }
   ],
   "source": [
    "startTime = datetime.now()\n",
    "\n",
    "import identify_eng_2\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    pool = mp.Pool(NUM_CORE)\n",
    "    eng_results = pool.map(identify_eng_2.identify_eng_2, [text for text in data['texts']]) \n",
    "    pool.close()\n",
    "    pool.join()\n",
    "\n",
    "print(datetime.now()-startTime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['language'] = eng_results\n",
    "data = data[data.language==0]\n",
    "data = data.reset_index() # reset the index of the DataFrame\n",
    "del data['index'] # delete a column with an old index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "579765"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the number of articles after excluding English articles\n",
    "len(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove URLs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove URLs and html file references.\n",
    "E.g.: 'de/studie99.html', 'Amazon.com', etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:02:41.383332\n"
     ]
    }
   ],
   "source": [
    "startTime = datetime.now()\n",
    "\n",
    "import correct_url\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    pool = mp.Pool(NUM_CORE)\n",
    "    url_corrected = pool.map(correct_url.correct_url, [text for text in data['texts']]) \n",
    "    pool.close()\n",
    "    pool.join()\n",
    "\n",
    "print(datetime.now()-startTime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['texts'] = url_corrected"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## O instead of 0 problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In some cases, Optical Character Recognition (OCR) can not distinguish between '0' and 'O' ('o'). As a result, there are tokens like '1OO'. Using regular expressions, we identify problematic tokens and replace 'O' ('o') with '0'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:00:15.676602\n"
     ]
    }
   ],
   "source": [
    "startTime = datetime.now()\n",
    "\n",
    "import ocr_replace\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    pool = mp.Pool(NUM_CORE)\n",
    "    ocr_corrected = pool.map(ocr_replace.ocr_replace, [text for text in data['texts']]) \n",
    "    pool.close()\n",
    "    pool.join()\n",
    "\n",
    "print(datetime.now()-startTime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['texts'] = ocr_corrected"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fixing tokens containing a number and a word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In quite a few cases, a number and a word are erroneously merged together into a single token. Splitting these tokens into two tokens helps us to deal with the following problems:\n",
    "\n",
    "1. Hyphen-separated words will have the same spelling across the whole corpus. Examples: 20Jährige/20-Jährige, 100prozentig/100-prozentig, 30minütigen/30-minütigen etc.\n",
    "\n",
    "2. Numbers and the names of the currencies will represent two different tokens. Examples: 100DM/100 DM, 30Euro/30 Euro.\n",
    "\n",
    "3. Numbers and the measures of time/weight/distance etc. will be split into two tokens. Examples: 100km/100 km, 1Mill/1 Mill, 16Uhr/16 Uhr.\n",
    "\n",
    "4. Simple mistakes will be corrected: 30bis 40/30 bis 40, 10Fahrzeuge/10 Fahrzeuge, 10und/10 und.\n",
    "\n",
    "5. Mistakes in the beginnning of the sentences will be corrected: 10Welche/10 Welche, 8Wie/8 Wie, etc.\n",
    "\n",
    "The most frequently met exceptions from this rule are taken into account:\n",
    "\n",
    "1. Names of the companies/organizations: 1822direkt, 3Sat, 4MBO etc.\n",
    "2. Names of smartphone/airplane/satellite models: 4S, 1B, 1k, 328Jet, 5C, 777X etc.\n",
    "3. German nouns like 90er and English adjectives like 21st, 3rd."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:00:24.513277\n"
     ]
    }
   ],
   "source": [
    "startTime = datetime.now()\n",
    "\n",
    "import split_number_word\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    pool = mp.Pool(NUM_CORE)\n",
    "    split_corrected = pool.map(split_number_word.split_number_word, [text for text in data['texts']]) \n",
    "    pool.close()\n",
    "    pool.join()\n",
    "\n",
    "print(datetime.now()-startTime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['texts'] = split_corrected"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove fuzzy duplicates\n",
    "\n",
    "By fuzzy duplicates we understand nearly duplicated articles. These are:\n",
    "\n",
    "* drafts/minor revisions of the articles saved in the database;\n",
    "* slightly changed advertisements which are published several times during a month;\n",
    "* 'NACHT - REPORT' news reports about stock market which are published a few hours earlier than almost identical 'WALL STREET' articles.\n",
    "\n",
    "We identify 'fuzzy' duplicates using cosine similarity and choose a threshold of 93% based on some visual exploration. Here is the article by Ryan Basques we used as a reference: [Link](https://towardsdatascience.com/a-laymans-guide-to-fuzzy-document-deduplication-a3b3cf9a05a7). \n",
    "\n",
    "There are also a few exceptions that we take into account:\n",
    "\n",
    "* 'DEUTSCHE AKTIEN' articles are often erroneously classified as duplicates because they contain very similar information;\n",
    "* 'Fortsetzung von Seite' (continuation of an article from another page): not surprisingly, these documents are also very close to each other. Later we concatenate them into one article;\n",
    "* cosine similarity approach does not work perfectly for extremely long articles. A few of them (e.g., 'Stuttgart 21: Ist das Projekt noch zu stoppen?') we list as exceptions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Required input for the function 'fuzzy_duplicates': a dataframe for each month-year combination.\n",
    "# List with a year\n",
    "inputs_year = []\n",
    "# List with a month\n",
    "inputs_month = []\n",
    "# List with the dataframes containing 'year', 'month', and 'texts' columns\n",
    "inputs_month_year = []\n",
    "for year in list(set(data['year'])):\n",
    "    for month in list(set(data['month'])):\n",
    "        inputs_year.append(year)\n",
    "        inputs_month.append(month)\n",
    "        inputs_month_year.append(data[(data['year'] == year) & (data['month'] == month)][[\"month\", \"year\", \"texts\"]])\n",
    "        \n",
    "inputs = list(zip(inputs_year, inputs_month, inputs_month_year))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "X:\\conda\\envs\\py3\\lib\\site-packages\\gensim\\utils.py:1209: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:08:08.459927\n"
     ]
    }
   ],
   "source": [
    "startTime = datetime.now() # track time\n",
    "\n",
    "import fuzzy_duplicates # import a function that outputs the indices of fuzzy duplicates \n",
    "delete_indices = []\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    pool = mp.Pool(NUM_CORE)\n",
    "    # apply function to all combinations of month-year in parallel\n",
    "    delete_intermediate = pool.map(fuzzy_duplicates.fuzzy_duplicates, inputs)\n",
    "    delete_indices = delete_indices + delete_intermediate # create one list of indices\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "    \n",
    "print(datetime.now()-startTime)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Duplicates exploration\n",
    "\n",
    "In case you want to have a look at the identified duplicates, use the function 'fuzzy_duplicates_test' from the file 'fuzzy_duplicates_test_all'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:08:10.531109\n"
     ]
    }
   ],
   "source": [
    "startTime = datetime.now() # track time\n",
    "\n",
    "import fuzzy_duplicates_test_all \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    pool = mp.Pool(NUM_CORE)\n",
    "    dup_intermediate = pool.map(fuzzy_duplicates_test_all.fuzzy_duplicates_test, inputs) \n",
    "    duplicates = pd.concat(dup_intermediate) \n",
    "    pool.close()\n",
    "    pool.join()\n",
    "    \n",
    "print(datetime.now()-startTime)\n",
    "\n",
    "duplicates.to_csv('duplicates.csv', encoding = 'utf-8-sig', sep = ';')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drop the duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Free memory\n",
    "inputs = None\n",
    "# List of indices corresponding to the duplicated articles\n",
    "delete_indices = [item for sublist in delete_indices for item in sublist]\n",
    "# List of unique indices\n",
    "delete_indices = list(set(delete_indices))\n",
    "# Drop the fuzzy duplicates\n",
    "data.drop(data.index[delete_indices], inplace = True)\n",
    "data = data.reset_index() # reset the index of the DataFrame\n",
    "del data['index'] # delete a column with an old index"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py3",
   "language": "python",
   "name": "py3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
