{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Handelsblatt data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Handelsblatt* is a popular German daily newspaper. According to the IVW, Informationsgemeinscahft zur Feststellung der Verbreitung von Werbetr√§gern (Information Community for the Assessment of the Circulation of Media), it had a circulation of 140612 daily copies in the first quarter of 2021. An appealing feature of Handelsblatt for forecasting economic activity is its focus on the economy.\n",
    "\n",
    "We purchased Handelsblatt data from Genios, a German provider of business information. The corpus includes **980516** articles from January 1994 to November 2018. The data was purchased in February 2019. \n",
    "\n",
    "The data set includes 25 subfolders corresponding to a particular year (e.g., HB_1994). Each subfolder contains a few XML files which we parse to extract information relevant to our research project. Unfortunately, the copyright prevents us from publishing the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we need to read in the data. We create the list including the names of the 25 subfolders (`folder_list`) and apply the function `hb_load` to them in parallel by exploiting Python's `multiprocessing` library."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We extract the following XML elements:\n",
    "\n",
    "* datum - publication date\n",
    "* worte - the word count\n",
    "* ressort - section/subsection of the newspaper\n",
    "* titel-liste/serientitel - the name of the series\n",
    "* quelle/name - the name of the newspaper\n",
    "* quelle/seite-start - the page number in the newspaper\n",
    "* titel-liste/titel - article's title\n",
    "* titel-liste/dachzeile - article's kicker\n",
    "* titel-liste/untertitel - article's subheading\n",
    "* inhalt/vorspann - annotation\n",
    "* inhalt/text - text of the article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Handelsblatt is the main folder with 25 subfolders in it\n",
    "#path = 'E:\\\\Userhome\\\\mokuneva\\\\Handelsblatt' # your path here\n",
    "path = os.getcwd().replace('\\\\newspaper_data_processing\\\\Handelsblatt', '') + '\\\\Handelsblatt'\n",
    "\n",
    "# Create the list of all subfolders within Hb main folder.\n",
    "folder_list=[]\n",
    "\n",
    "for f in [f for f in os.listdir(path) ]: \n",
    "    # os.listdir(path) - names of directories                                         \n",
    "    folder_list.append(path + '\\\\' + f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of cores that will be used: 56\n"
     ]
    }
   ],
   "source": [
    "import multiprocessing as mp # use multiprocessing module for parallel computing\n",
    "\n",
    "NUM_CORE = mp.cpu_count()-8 # set the number of cores to use\n",
    "\n",
    "print(\"The number of cores that will be used: {}\".format(NUM_CORE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:00:39.740173\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "startTime = datetime.now() # track time\n",
    "\n",
    "import pandas as pd # load pandas: python data analysis library\n",
    "import hb_load # import a function that loads the data from one folder (see hb_load.py file for details)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    pool = mp.Pool(NUM_CORE)\n",
    "    data_intermediate = pool.map(hb_load.hb_load, folder_list) # load data from each folder in parallel\n",
    "    data = pd.concat(data_intermediate) # concatenate DataFrames corresponding to different folders\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "    \n",
    "print(datetime.now()-startTime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.sort_values(['year', 'month', 'day'], ascending=[True, True, True]) # sort the data in chronological order\n",
    "data.reset_index(inplace=True, drop=True) # reset the index of the DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>day</th>\n",
       "      <th>newspaper</th>\n",
       "      <th>texts</th>\n",
       "      <th>rubrics</th>\n",
       "      <th>title</th>\n",
       "      <th>series_title</th>\n",
       "      <th>title_only</th>\n",
       "      <th>kicker</th>\n",
       "      <th>word_c</th>\n",
       "      <th>page</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1994</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Handelsblatt</td>\n",
       "      <td>Nachrichten. Yuan abgewertet dpa PEKING. China...</td>\n",
       "      <td>Nachrichten</td>\n",
       "      <td>Nachrichten.</td>\n",
       "      <td></td>\n",
       "      <td>Nachrichten.</td>\n",
       "      <td></td>\n",
       "      <td>68</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1994</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Handelsblatt</td>\n",
       "      <td>Nachrichten. Ruecktritt abgelehnt afp NEU DELH...</td>\n",
       "      <td>Nachrichten</td>\n",
       "      <td>Nachrichten.</td>\n",
       "      <td></td>\n",
       "      <td>Nachrichten.</td>\n",
       "      <td></td>\n",
       "      <td>58</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1994</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Handelsblatt</td>\n",
       "      <td>Nachrichten. Ungarn wertet Forint ab rtr BUDAP...</td>\n",
       "      <td>Nachrichten</td>\n",
       "      <td>Nachrichten.</td>\n",
       "      <td></td>\n",
       "      <td>Nachrichten.</td>\n",
       "      <td></td>\n",
       "      <td>33</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1994</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Handelsblatt</td>\n",
       "      <td>Nachrichten. FDP zum Beamtenrecht dpa BONN. Di...</td>\n",
       "      <td>Nachrichten</td>\n",
       "      <td>Nachrichten.</td>\n",
       "      <td></td>\n",
       "      <td>Nachrichten.</td>\n",
       "      <td></td>\n",
       "      <td>55</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1994</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Handelsblatt</td>\n",
       "      <td>Nachrichten. Neue Drohungen dpa HAMBURG. Der F...</td>\n",
       "      <td>Nachrichten</td>\n",
       "      <td>Nachrichten.</td>\n",
       "      <td></td>\n",
       "      <td>Nachrichten.</td>\n",
       "      <td></td>\n",
       "      <td>86</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    year  month  day     newspaper  \\\n",
       "10  1994      1    3  Handelsblatt   \n",
       "11  1994      1    3  Handelsblatt   \n",
       "12  1994      1    3  Handelsblatt   \n",
       "13  1994      1    3  Handelsblatt   \n",
       "14  1994      1    3  Handelsblatt   \n",
       "\n",
       "                                                texts      rubrics  \\\n",
       "10  Nachrichten. Yuan abgewertet dpa PEKING. China...  Nachrichten   \n",
       "11  Nachrichten. Ruecktritt abgelehnt afp NEU DELH...  Nachrichten   \n",
       "12  Nachrichten. Ungarn wertet Forint ab rtr BUDAP...  Nachrichten   \n",
       "13  Nachrichten. FDP zum Beamtenrecht dpa BONN. Di...  Nachrichten   \n",
       "14  Nachrichten. Neue Drohungen dpa HAMBURG. Der F...  Nachrichten   \n",
       "\n",
       "           title series_title    title_only kicker  word_c page  \n",
       "10  Nachrichten.               Nachrichten.             68       \n",
       "11  Nachrichten.               Nachrichten.             58       \n",
       "12  Nachrichten.               Nachrichten.             33       \n",
       "13  Nachrichten.               Nachrichten.             55       \n",
       "14  Nachrichten.               Nachrichten.             86       "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[10:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "980516"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the number of article before pre-processing\n",
    "len(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Light pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first part of the article 327123 contains different symbols (e.g. zw|lf) instead of umlauts. The second part (which I keep) has the same content with the right format. The second part starts with 'NEU DELHI.'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.at[327123, 'texts'] = data['texts'][327123][data['texts'][327123].find('NEU DELHI.'):]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove short articles (<100 words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An important feature of the text is its length. There are a few ways to count the number of words in a text:\n",
    "\n",
    "1. Use the metadata ('worte') of an XML file. Beware that numbers are counted as words. This is the column 'word_c' in the DataFrame. \n",
    "2. Quick and dirty approach: split the tokens by space and calculate the length of the list. Numbers and other non-alphabetic characters are counted as words. If a space is used as a delimiter in large numbers (100 000), this number will be counted as two tokens.\n",
    "3. Use `count_words_mp` function, which counts only words. This is our preferred method because we exclude numbers from the analysis in both sentiment analysis and topic modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the second approach\n",
    "# data['w_count'] = data['texts'].str.split(' ').str.len()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:00:20.254305\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "startTime = datetime.now() # track time\n",
    "\n",
    "import count_words_mp # import the function calculating the number of words in a text\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    pool = mp.Pool(NUM_CORE)\n",
    "    count_results = pool.map(count_words_mp.count_words_mp, [text for text in data['texts']]) \n",
    "    pool.close()\n",
    "    pool.join()\n",
    "    \n",
    "print(datetime.now()-startTime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the result as a new column \"word_count\"\n",
    "data['word_count'] = count_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The difference between the first ('word_c' column) and third ('word_count' column) approaches might be significant. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff = (data['word_c']-data['word_count'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[695, 691, 591, 430, 429, 429, 404, 390, 387, 387]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(diff, reverse=True)[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This happens because the third approach does not count a number as a word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Fu√üball-Bundesliga. \\\\\\\\\\\\\\\\\\\\\\\\\\\\ zu Hause\\\\ ausw√§rts\\\\ Sp.\\\\ g.\\\\ u.\\\\ v.\\\\ Tore\\\\ Diff.\\\\ Pkt.\\\\ Sp.\\\\ g.\\\\ u.\\\\ v.\\\\ Tore\\\\ Pkt.\\\\ Sp.\\\\ g.\\\\ u.\\\\ v.\\\\ Tore\\\\ Pkt.\\\\ 1. ( 1)\\\\ Bayern M√ºnchen 34\\\\ 20\\\\ 11\\\\ 3\\\\ 68:34\\\\ +34\\\\ 71\\\\ 17\\\\ 13\\\\ 4\\\\ 0\\\\ 37:12\\\\ 43\\\\ 17\\\\ 7\\\\ 7\\\\ 3\\\\ 31:22\\\\ 28\\\\ 2. ( 2)\\\\ Bayer Leverkusen 34\\\\ 21\\\\ 6\\\\ 7\\\\ 69:41\\\\ +28\\\\ 69'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['texts'][list(diff).index(387)][:300]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because short texts have sparse semantic features, topic models and BOW-based sentiment tools perform better on longer texts. This is why we keep the texts longer than 100 words.\n",
    "\n",
    "However, at this point, we also keep articles shorter than 100 words if they can potentially be continued on another page and therefore represent part of the chained articles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "continued_strings_short = ['Fortsetzung von Seite', 'FORTSETZUNG VON SEITE', 'Fortsetzung Seite', 'FORTSETZUNG SEITE',\n",
    "                           'Fortsetzung auf Seite', 'FORTSETZUNG AUF SEITE', 'Fortsetzung n√§chste Seite', \n",
    "                           'FORTSETZUNG N√ÑCHSTE SEITE']\n",
    "# keep articles that contain at least one of the strings from the list 'continued_strings_short' and contain \n",
    "# less than 100 words\n",
    "short_cont_ind = list(data[(data['word_count']<100) & \n",
    "    (data.texts.str.contains('|'.join(continued_strings_short)))].index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An example of the articles we keep."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'New York: Anfangs besser. An der Wall Street eroeffnete der Dow-Jones-Index am Montag mit steigenden Notierungen, rutschte aber bis 12.45 Uhr Ortszeit auf 3752,69, 1,40 Punkte unter den Vortragsschluss. Fortsetzung Seite 17'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.iloc[short_cont_ind[0]]['texts']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also keep short articles published on the same day as articles with the `short_cont_ind` indices and with the same beginning of the title. These news reports may be part of longer chained articles as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:00:01.728834\n"
     ]
    }
   ],
   "source": [
    "startTime = datetime.now() # track time\n",
    "# articles with this beginning of the title are not chained articles\n",
    "exceptions = ['Nachrichten', 'Geldticker', 'Wirtschaft', 'Fortsetzun']\n",
    "cont_index = []\n",
    "for ind in short_cont_ind:\n",
    "    # short articles published on the same day as articles with short_cont_ind indices\n",
    "    cont_art = data[(data.day == data.day[ind]) & (data.month == data.month[ind]) & (data.year == data.year[ind]) &\n",
    "                   (data['word_count']<100)]\n",
    "    for i in cont_art.index:\n",
    "        # articles with the same beginning of the title -> potentially chained articles\n",
    "        if (data['texts'][i][:10] == data['texts'][ind][:10]) and (i!=ind) and \\\n",
    "        (data['texts'][ind][:10] not in exceptions):\n",
    "            cont_index.append(i)\n",
    "print(datetime.now()-startTime)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the first part of the chained article published on the page 15 that does not contain any string suggesting that it will be continued on another page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'AUSLAENDISCHE BOERSEN / Alle Indizes verbuchen Gewinne - London geschlossen. Aktienmaerkte in Europa begruessen das neue Jahr mit steigenden Notierungen. cbu DUESSELDORF. Die europaeischen Aktienmaerkte gingen sehr freundlich in das neue Jahr. Alle fuehrenden Indizes lagen im Plus, die wichtige Boerse in London blieb aber geschlossen. In Fernost erreichten Singapur und Hongkong neue Rekordstaende.'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.iloc[cont_index[0]]['texts']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, we can also find the second part of this article including a string 'Continued from page 15' and having the same title as the first part."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'AUSLAENDISCHE BOERSEN / Fortsetzung von Seite 15. Weitere Rekorde in Fernost Tokio: Geschlossen. Wegen eines Feiertages blieben Behoerden, Banken und Maerkte geschlossen.'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.iloc[short_cont_ind[1]]['texts']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "short_keep = list(sorted(set(short_cont_ind + cont_index)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mokuneva\\AppData\\Local\\Temp\\2\\ipykernel_31288\\2052437139.py:2: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  data = data.loc[short_keep].append(data[data['word_count']>=100])\n"
     ]
    }
   ],
   "source": [
    "# remove articles with the number of words<100 unless these are chained articles candidates\n",
    "data = data.loc[short_keep].append(data[data['word_count']>=100])\n",
    "data = data.sort_values(['year', 'month', 'day'], ascending=[True, True, True]) # sort the data in chronological order\n",
    "data.reset_index(inplace=True, drop=True) # reset the index of the DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "673712"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the number of articles after removing short articles\n",
    "len(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove exact duplicates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A few examples of duplicates in our corpus:\n",
    "* The same article enters the corpus twice with different publication dates (e.g., 6.4.1994 and 27.4.1994). In this case, a natural solution is to keep the first entry.\n",
    "* The same article appears twice with a slight variation in the metadata (e.g., one of the duplicates includes the title of the series, or the word count is a little different even though the articles are identical).\n",
    "* The same article enters the corpus twice with the same publication date and metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All the duplicated articles are saved as 'hb_duplicates' for further exploration.\n",
    "hb_duplicates = data[data['texts'].duplicated(keep = False)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>day</th>\n",
       "      <th>newspaper</th>\n",
       "      <th>texts</th>\n",
       "      <th>rubrics</th>\n",
       "      <th>title</th>\n",
       "      <th>series_title</th>\n",
       "      <th>title_only</th>\n",
       "      <th>kicker</th>\n",
       "      <th>word_c</th>\n",
       "      <th>page</th>\n",
       "      <th>word_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9710</th>\n",
       "      <td>1994</td>\n",
       "      <td>4</td>\n",
       "      <td>29</td>\n",
       "      <td>Handelsblatt</td>\n",
       "      <td>Importlizenzen im Vergleichsverfahren. HB DUES...</td>\n",
       "      <td>Weltwirtschaft</td>\n",
       "      <td>Importlizenzen im Vergleichsverfahren.</td>\n",
       "      <td></td>\n",
       "      <td>Importlizenzen im Vergleichsverfahren.</td>\n",
       "      <td></td>\n",
       "      <td>121</td>\n",
       "      <td></td>\n",
       "      <td>112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9830</th>\n",
       "      <td>1994</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>Handelsblatt</td>\n",
       "      <td>Importlizenzen im Vergleichsverfahren. HB DUES...</td>\n",
       "      <td>Weltwirtschaft</td>\n",
       "      <td>Importlizenzen im Vergleichsverfahren.</td>\n",
       "      <td></td>\n",
       "      <td>Importlizenzen im Vergleichsverfahren.</td>\n",
       "      <td></td>\n",
       "      <td>122</td>\n",
       "      <td></td>\n",
       "      <td>112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15338</th>\n",
       "      <td>1994</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>Handelsblatt</td>\n",
       "      <td>NACHTREPORT / Dow-Jones-Index steigt um 22 Pun...</td>\n",
       "      <td>Finanzzeitung; Geld und Kredit</td>\n",
       "      <td>NACHTREPORT / Dow-Jones-Index steigt um 22 Pun...</td>\n",
       "      <td></td>\n",
       "      <td>NACHTREPORT / Dow-Jones-Index steigt um 22 Pun...</td>\n",
       "      <td></td>\n",
       "      <td>638</td>\n",
       "      <td></td>\n",
       "      <td>604</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15598</th>\n",
       "      <td>1994</td>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>Handelsblatt</td>\n",
       "      <td>NACHTREPORT / Dow-Jones-Index steigt um 22 Pun...</td>\n",
       "      <td>Finanzzeitung; Geld und Kredit</td>\n",
       "      <td>NACHTREPORT / Dow-Jones-Index steigt um 22 Pun...</td>\n",
       "      <td></td>\n",
       "      <td>NACHTREPORT / Dow-Jones-Index steigt um 22 Pun...</td>\n",
       "      <td></td>\n",
       "      <td>636</td>\n",
       "      <td></td>\n",
       "      <td>604</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16977</th>\n",
       "      <td>1994</td>\n",
       "      <td>7</td>\n",
       "      <td>28</td>\n",
       "      <td>Handelsblatt</td>\n",
       "      <td>Glossar. Das Verarbeitende Gewerbe ist in West...</td>\n",
       "      <td>Wirtschaft und Politik</td>\n",
       "      <td>Glossar.</td>\n",
       "      <td></td>\n",
       "      <td>Glossar.</td>\n",
       "      <td></td>\n",
       "      <td>139</td>\n",
       "      <td></td>\n",
       "      <td>130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18493</th>\n",
       "      <td>1994</td>\n",
       "      <td>8</td>\n",
       "      <td>19</td>\n",
       "      <td>Handelsblatt</td>\n",
       "      <td>Solide Basisausbildung zu DDR-Zeiten. Ost-Inge...</td>\n",
       "      <td>Karriere</td>\n",
       "      <td>Solide Basisausbildung zu DDR-Zeiten. Ost-Inge...</td>\n",
       "      <td></td>\n",
       "      <td>Solide Basisausbildung zu DDR-Zeiten. Ost-Inge...</td>\n",
       "      <td></td>\n",
       "      <td>176</td>\n",
       "      <td></td>\n",
       "      <td>169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18627</th>\n",
       "      <td>1994</td>\n",
       "      <td>8</td>\n",
       "      <td>19</td>\n",
       "      <td>Handelsblatt</td>\n",
       "      <td>Solide Basisausbildung zu DDR-Zeiten. Ost-Inge...</td>\n",
       "      <td>Karriere</td>\n",
       "      <td>Solide Basisausbildung zu DDR-Zeiten. Ost-Inge...</td>\n",
       "      <td></td>\n",
       "      <td>Solide Basisausbildung zu DDR-Zeiten. Ost-Inge...</td>\n",
       "      <td></td>\n",
       "      <td>176</td>\n",
       "      <td></td>\n",
       "      <td>169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20943</th>\n",
       "      <td>1994</td>\n",
       "      <td>9</td>\n",
       "      <td>20</td>\n",
       "      <td>Handelsblatt</td>\n",
       "      <td>Hochtief will seine Beteiligung bei Holzmann e...</td>\n",
       "      <td>Unternehmen und Maerkte</td>\n",
       "      <td>Hochtief will seine Beteiligung bei Holzmann e...</td>\n",
       "      <td></td>\n",
       "      <td>Hochtief will seine Beteiligung bei Holzmann e...</td>\n",
       "      <td></td>\n",
       "      <td>738</td>\n",
       "      <td></td>\n",
       "      <td>715</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20947</th>\n",
       "      <td>1994</td>\n",
       "      <td>9</td>\n",
       "      <td>20</td>\n",
       "      <td>Handelsblatt</td>\n",
       "      <td>Hochtief will seine Beteiligung bei Holzmann e...</td>\n",
       "      <td>Unternehmen und Maerkte</td>\n",
       "      <td>Hochtief will seine Beteiligung bei Holzmann e...</td>\n",
       "      <td></td>\n",
       "      <td>Hochtief will seine Beteiligung bei Holzmann e...</td>\n",
       "      <td></td>\n",
       "      <td>738</td>\n",
       "      <td></td>\n",
       "      <td>715</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21266</th>\n",
       "      <td>1994</td>\n",
       "      <td>9</td>\n",
       "      <td>22</td>\n",
       "      <td>Handelsblatt</td>\n",
       "      <td>Glossar. Das Verarbeitende Gewerbe ist in West...</td>\n",
       "      <td>Wirtschaft und Politik; Konjunkturbarometer</td>\n",
       "      <td>Glossar.</td>\n",
       "      <td></td>\n",
       "      <td>Glossar.</td>\n",
       "      <td></td>\n",
       "      <td>133</td>\n",
       "      <td></td>\n",
       "      <td>126</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       year  month  day     newspaper  \\\n",
       "9710   1994      4   29  Handelsblatt   \n",
       "9830   1994      5    2  Handelsblatt   \n",
       "15338  1994      7    7  Handelsblatt   \n",
       "15598  1994      7    8  Handelsblatt   \n",
       "16977  1994      7   28  Handelsblatt   \n",
       "18493  1994      8   19  Handelsblatt   \n",
       "18627  1994      8   19  Handelsblatt   \n",
       "20943  1994      9   20  Handelsblatt   \n",
       "20947  1994      9   20  Handelsblatt   \n",
       "21266  1994      9   22  Handelsblatt   \n",
       "\n",
       "                                                   texts  \\\n",
       "9710   Importlizenzen im Vergleichsverfahren. HB DUES...   \n",
       "9830   Importlizenzen im Vergleichsverfahren. HB DUES...   \n",
       "15338  NACHTREPORT / Dow-Jones-Index steigt um 22 Pun...   \n",
       "15598  NACHTREPORT / Dow-Jones-Index steigt um 22 Pun...   \n",
       "16977  Glossar. Das Verarbeitende Gewerbe ist in West...   \n",
       "18493  Solide Basisausbildung zu DDR-Zeiten. Ost-Inge...   \n",
       "18627  Solide Basisausbildung zu DDR-Zeiten. Ost-Inge...   \n",
       "20943  Hochtief will seine Beteiligung bei Holzmann e...   \n",
       "20947  Hochtief will seine Beteiligung bei Holzmann e...   \n",
       "21266  Glossar. Das Verarbeitende Gewerbe ist in West...   \n",
       "\n",
       "                                           rubrics  \\\n",
       "9710                                Weltwirtschaft   \n",
       "9830                                Weltwirtschaft   \n",
       "15338               Finanzzeitung; Geld und Kredit   \n",
       "15598               Finanzzeitung; Geld und Kredit   \n",
       "16977                       Wirtschaft und Politik   \n",
       "18493                                     Karriere   \n",
       "18627                                     Karriere   \n",
       "20943                      Unternehmen und Maerkte   \n",
       "20947                      Unternehmen und Maerkte   \n",
       "21266  Wirtschaft und Politik; Konjunkturbarometer   \n",
       "\n",
       "                                                   title series_title  \\\n",
       "9710              Importlizenzen im Vergleichsverfahren.                \n",
       "9830              Importlizenzen im Vergleichsverfahren.                \n",
       "15338  NACHTREPORT / Dow-Jones-Index steigt um 22 Pun...                \n",
       "15598  NACHTREPORT / Dow-Jones-Index steigt um 22 Pun...                \n",
       "16977                                           Glossar.                \n",
       "18493  Solide Basisausbildung zu DDR-Zeiten. Ost-Inge...                \n",
       "18627  Solide Basisausbildung zu DDR-Zeiten. Ost-Inge...                \n",
       "20943  Hochtief will seine Beteiligung bei Holzmann e...                \n",
       "20947  Hochtief will seine Beteiligung bei Holzmann e...                \n",
       "21266                                           Glossar.                \n",
       "\n",
       "                                              title_only kicker  word_c page  \\\n",
       "9710              Importlizenzen im Vergleichsverfahren.            121        \n",
       "9830              Importlizenzen im Vergleichsverfahren.            122        \n",
       "15338  NACHTREPORT / Dow-Jones-Index steigt um 22 Pun...            638        \n",
       "15598  NACHTREPORT / Dow-Jones-Index steigt um 22 Pun...            636        \n",
       "16977                                           Glossar.            139        \n",
       "18493  Solide Basisausbildung zu DDR-Zeiten. Ost-Inge...            176        \n",
       "18627  Solide Basisausbildung zu DDR-Zeiten. Ost-Inge...            176        \n",
       "20943  Hochtief will seine Beteiligung bei Holzmann e...            738        \n",
       "20947  Hochtief will seine Beteiligung bei Holzmann e...            738        \n",
       "21266                                           Glossar.            133        \n",
       "\n",
       "       word_count  \n",
       "9710          112  \n",
       "9830          112  \n",
       "15338         604  \n",
       "15598         604  \n",
       "16977         130  \n",
       "18493         169  \n",
       "18627         169  \n",
       "20943         715  \n",
       "20947         715  \n",
       "21266         126  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hb_duplicates[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop the exact duplicates, keep the article with the earlier publication date ('first')\n",
    "data.drop_duplicates(['texts'], keep = 'first', inplace=True)\n",
    "data.reset_index(inplace=True, drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "670961"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the number of articles after removing exact duplicates\n",
    "len(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filtering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Handelsblatt's articles are organized into 2073 unique sections/subsections. We investigate the most frequently met ones and remove articles published within a few of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Unternehmen und M√§rkte', 87617), ('Wirtschaft und Politik', 33223), ('Deutschland', 29652), ('Finanzzeitung', 28435), ('Unternehmen & M√§rkte', 19289), ('Meinung und Analyse', 17238), ('Titelseite', 16957), ('Beilage oder Sonderseite', 16278), ('International', 16252), ('Europa', 15709)]\n"
     ]
    }
   ],
   "source": [
    "# Sections and the number of articles per section\n",
    "import collections\n",
    "counter_sections = collections.Counter(data.rubrics)\n",
    "print(counter_sections.most_common(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To ensure consistency in topical content over time, we remove articles from a few sections that were covered only within a limited time period:\n",
    "\n",
    "* 1) 'Karriere': news related to work, career; due to the organizational changes, starting from 2014, this section is only present in a weekend edition of the newspaper.\n",
    "* 2) 'Weekend Journal': the section was introduced in 2002 and did not receive any coverage after 2008. Moreover, the news discussed within this section is on average longer (663 words) than the news from the main section of interest \"Wirtschaft und Politik\" (436 words).\n",
    "* 3) 'Panorama': news from around the world, issued in 1997, existed until 2001.\n",
    "* 4) 'Business-Travel': travel-related news, e.g. weather, 2001-2006, only 366 articles in total.\n",
    "* 5) 'Fortschritt': scientific findings, new devices, 2001-2003.\n",
    "* 6) 'Wochenende': weekend news, 2012-2018, average article length is 1579.\n",
    "* 7) 'perspektiven': perspectives, non-economic topics like a prize for women in science, working abroad, or new beauty standards popularized by Dove's advertising campaign; 2007-2009."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "abandoned_sections = ['Karriere', 'Weekend Journal', 'Panorama', 'Business-Travel', \n",
    "                    'Fortschritt', 'Wochenende', 'perspektiven']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data[~data['rubrics'].isin(abandoned_sections)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "658760"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the number of articles after removing articles from the sections covered only within a certain time period\n",
    "len(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we want to exclude the articles from non-economic sections. The list of irrelevant sections follows.\n",
    "\n",
    "* 1) 'Sportreport': sport news.\n",
    "* 2) 'Kunstmarkt': art market.\n",
    "* 3) 'Sportreport      Nachrichten': sport news.\n",
    "* 4) 'Neue B√ºcher': an advertisement for new books.\n",
    "* 5) 'Wissenschaft & Debatte': science and debate.\n",
    "* 6) 'Literatur': literature.\n",
    "* 7) 'Leserbriefe': letters to the editor; starting from 2013, there are almost no letters/comments from the readers, because all the comments became digital.\n",
    "* 8) 'Reisen und Tagen': travel.\n",
    "* 9) 'Kultur': culture.\n",
    "* 10) 'Termin- und Optionsm√§rkte': futures and options markets, mostly quantitative info.\n",
    "* 11) 'Galerie des Stils': style and fashion.\n",
    "* 12) 'Galerie': non-economic news about a pop band, Nutella restaurant etc.\n",
    "* 13) 'Neue Buecher': new books.\n",
    "* 14) 'Online': news about the websites, digitalization, online services.\n",
    "* 15) 'Forschung und Technik': research and technology.\n",
    "* 16) 'Business-Service': news unrelated to the economy, e.g. telephone service for weather forecast, travel information etc.\n",
    "* 17) 'Computer und Kommunikation': computers and communication.\n",
    "* 18) '√ñkonomie & Bildung': economics and education, Handelsblatt's materials for school lessons, MBA studies, educating articles about how the economy works.\n",
    "* 19) 'Auto-Mobil': new car models.\n",
    "* 20) 'Die Handelsblatt-Woche': announcement of events taking place this week.\n",
    "* 21) 'Wirtschaftsbuch': economics books."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_economic_sections = ['Kunstmarkt', 'Sportreport', 'Sportreport      Nachrichten', \n",
    "                  'Neue B√ºcher', 'Wissenschaft & Debatte',  \n",
    "                  'Literatur', 'Leserbriefe', 'Reisen und Tagen', \n",
    "                  'Kultur',  'Termin- und Optionsm√§rkte',\n",
    "                  'Galerie des Stils', 'Galerie', 'Neue Buecher', 'Online', 'Forschung und Technik', \n",
    "                  'Business-Service', 'Computer und Kommunikation', '√ñkonomie & Bildung', 'Auto-Mobil', \n",
    "                  'Die Handelsblatt-Woche', 'Wirtschaftsbuch']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data[~data['rubrics'].isin(non_economic_sections)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "621597"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the number of articles after removing articles from the non-economic sections\n",
    "len(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Additionally, we remove articles from the related sections and subsections. \n",
    "\n",
    "* This means that we not only remove articles from a section 'Kunstmarkt', but also from the subsections 'Kunstmarkt      Anleihen', 'Kunstmarkt      Aus den Galerien' etc. \n",
    "\n",
    "* We consider 'Kunstmarkt' and 'Themen und Trends      Kunstmarkt' as two close sections. \n",
    "\n",
    "* The full list of sections/subsections we remove can be found in 'subsections_hb.txt'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this function loads the dictionary with the related sections and subsections we want to exclude\n",
    "from os import path\n",
    "import codecs\n",
    "\n",
    "def dictionary_open(name):\n",
    "    with codecs.open(path.join(os.getcwd(), name),\n",
    "               'r',  'utf-8') as f:\n",
    "          dictionary = set(f.read().splitlines()[1:-1])\n",
    "    return dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "subsections_clean = dictionary_open('subsections_hb.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data[~data['rubrics'].isin(subsections_clean)]\n",
    "data.reset_index(inplace=True, drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "589117"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the number of articles after removing articles from the related subsections\n",
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Unternehmen und M√§rkte', 87617), ('Wirtschaft und Politik', 33223), ('Deutschland', 29652), ('Finanzzeitung', 28435), ('Unternehmen & M√§rkte', 19289), ('Meinung und Analyse', 17238), ('Titelseite', 16957), ('Beilage oder Sonderseite', 16278), ('International', 16252), ('Europa', 15709)]\n"
     ]
    }
   ],
   "source": [
    "# Sections and the number of articles per section\n",
    "counter_sections=collections.Counter(data['rubrics'])\n",
    "print(counter_sections.most_common(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section + text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove articles for which the following two conditions are met: a section is '' (unclassified), and a text contains a string 'KARRIERE'. As discussed, this type of articles is removed due to organizational changes (see above)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data[~((data['rubrics']=='') & (data['texts'].str.contains('KARRIERE')))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove articles that are characterized by the following conditions: a section contains a string \"Recht und Steuern\" (Law and taxes), and a text contains one of the legal terms from the list `legal_terms`.\n",
    "\n",
    "* Az./AZ/Aktenzeichen/AKTENZEICHEN - docket number\n",
    "* LAG/Landesarbeitsgericht - Regional Labour Court\n",
    "* BAG/Bundesarbeitsgericht - Federal Labour Court\n",
    "* VI R/IV A - case/law reference\n",
    "* BFH - Bundesfinanzhof, German Federal Fiscal court\n",
    "* Abs. - section\n",
    "* BGH/Bundesgerichtshof - Federal Supreme Court\n",
    "* BGB - B√ºrgerliches Gesetzbuch, Civil Code\n",
    "* BSG - Bundessozialgericht, Federal Social Court\n",
    "* EStG/Einkommensteuergesetz - Income Tax Act\n",
    "* OLG/Oberlandesgericht - Higher Regional Court\n",
    "* Anwaltgerichtshof - Lawyers' Court\n",
    "* Finanzgericht - Tax Court\n",
    "\n",
    "We remove these articles because they contain very detailed explanations of laws/tax deduction rules/court decisions which have a low chance to be relevant for forecasting economic activity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "legal_terms = ['Az\\\\.', 'AZ', 'Az\\\\:', 'Aktenzeichen', 'AKTENZEICHEN', 'LAG', 'Landesarbeitsgericht', 'Bundesarbeitsgericht', 'BAG', 'Arbeitsgericht', 'VI R','IV A','BFH','Abs\\\\.','BGH', 'Bundesgerichtshof', 'BGB','BSG','EStG','Einkommensteuergesetz','OLG', 'Oberlandesgericht', 'Anwaltgerichtshof', 'Finanzgericht']\n",
    "data = data[~((data.rubrics.str.contains('Recht und Steuern')) & (data.texts.str.contains('|'.join(legal_terms))))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "584047"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the number of articles after filtering out articles based on the section and text\n",
    "len(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section + title"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove articles satisfying two conditions: section is 'Inhalt' (content), and the title is not 'Termine des Tages.' Section 'Inhalt' mainly includes news reports about the Handelsblatt and its organization. The aritlces with the title 'Termine des Tages.' may include short news on the Economy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data[~(((data['rubrics']=='Inhalt') | (data['rubrics']=='Inhalt ;') | (data['rubrics']=='Inhalt      Der Werber-Rat'))\n",
    "              & (data['title']!='Termine des Tages.'))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "583278"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the number of articles after removing articles from the section 'Inhalt'\n",
    "len(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Title"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exclude articles with the following title patterns:\n",
    "\n",
    "* book suggestions (Buchtip)\n",
    "* some very short news on a few people (BUSINESS LOUNGE.)\n",
    "* articles about Handelsblatt and its organization (Liebe Leser)\n",
    "* articles with comments from the readers (DIE MEINUNG UNSERER LESER.)\n",
    "* announcement of events taking place this week (DIE HANDELSBLATT-WOCHE.)\n",
    "* non-economic news related to numbers (e.g., how many people watched the movie this week); Bilanz des Wochenendes.\n",
    "* sport news (SPORT TELEGRAMM.) \n",
    "* news related to the project 'Handelsblatt macht Schule' promoting economic education"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "title_patterns = ['BUCHTIP', 'Buchtip', 'BUSINESS LOUNGE\\\\.', 'Liebe Leser', 'DIE MEINUNG UNSERER LESER\\\\.', \n",
    "                 'DIE HANDELSBLATT\\\\-WOCHE\\\\.', 'Die Handelsblatt\\\\-Woche\\\\.', 'Die Handelsblattwoche\\\\.',\n",
    "                 'DIE HANDELSBLATTWOCHE\\\\.', 'Bilanz des Wochenendes\\\\.', 'BILANZ DES WOCHENENDES\\\\.',\n",
    "                 'SPORT TELEGRAMM\\\\.']\n",
    "data = data[~data['title_only'].str.contains('|'.join(title_patterns))]\n",
    "\n",
    "kicker_title_patterns = ['Handelsblatt macht Schule', 'HANDELSBLATT MACHT SCHULE\\\\.', \n",
    "                         'Aktuelles Wirtschaftswissen f√ºr den Unterricht', \n",
    "                         'Wirtschaftswissen f√ºr Sch√ºler leicht gemacht',\n",
    "                        '√ñkonomie ganz leicht gemacht f√ºr Jugendliche',\n",
    "                        'Aktuelles Wirtschaftswissen f√ºr Jugendliche',\n",
    "                        'Neuer Stoff f√ºr den Unterricht im Fach Wirtschaft',\n",
    "                        'Handelsblatt bringt Praxis in die Schulen',\n",
    "                        'Experten aus der Praxis in den Schulen',\n",
    "                        'Der neue Newcomer ist da',\n",
    "                        'Die neue Zeitung f√ºr Sch√ºler ist da',\n",
    "                        'Wettbewerb\\\\: Eine Welt ohne Geld\\\\?',\n",
    "                        'Wettbewerb zum Thema Geld',\n",
    "                        'Wettbewerb f√ºr Sch√ºler',\n",
    "                        'Schlauere Azubis dank Zeitunglesen',\n",
    "                        'F√∂rderung f√ºr Auszubildende'\n",
    "                        ]\n",
    "data = data[~data.title.str.contains('|'.join(kicker_title_patterns))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exclude articles with the following titles:\n",
    "\n",
    "* Kontakte. (contacts of organizations: addresses and phone numbers)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data[~(data['title_only']=='Kontakte.')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "580876"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the number of articles after excluding articles based on the title patterns\n",
    "len(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Series"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exclude articles from the following non-economic series:\n",
    "\n",
    "* Neue Wirtschaftsliteratur (Handelsblatt-Beilage): economic books;\n",
    "* Golf (Handelsblatt-Beilage): golf;\n",
    "* Literatur (Handelsblatt-Beilage): literature;\n",
    "* Macher des Handelsblatts (Handelsblatt-Serie): about journalists working for Handelsblatt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "series_exclude = ['Neue Wirtschaftsliteratur (Handelsblatt-Beilage)', \n",
    "                  'Golf (Handelsblatt-Beilage)', 'Literatur (Handelsblatt-Beilage)', \n",
    "                  'Literatur (Handelsblatt - Beilage)', 'Macher des Handelsblatts (Handelsblatt-Serie)']\n",
    "data = data[~data['series_title'].isin(series_exclude)]\n",
    "data.reset_index(inplace=True, drop=True) # reset the index of the DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "580228"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the number of articles after excluding articles from the non-economic series\n",
    "len(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exclude articles that contain the following strings:\n",
    "\n",
    "* TERMINM√ÑRKTE / Tagesbericht: options trading, mainly quantitative information;\n",
    "* Deutsche B√∂rsen D√ºsseldorf: stock market, mainly quantitative information;\n",
    "* DIGIX. Der E-Business-Index. or DIGIX. DIGIX Der E-Business-Index.: text corresponding to a graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_strings = ['TERMINM√ÑRKTE \\\\/ Tagesbericht', 'TERMINMAERKTE \\\\/ Tagesbericht', \n",
    "                'Deutsche B√∂rsen D√ºsseldorf', 'Deutsche Boersen Duesseldorf', 'DIGIX\\\\. Der E\\\\-Business\\\\-Index.',\n",
    "               'DIGIX\\\\. DIGIX Der E\\\\-Business\\\\-Index\\\\.']\n",
    "data = data[~(data.texts.str.contains('|'.join(text_strings)))]\n",
    "data.reset_index(inplace=True, drop=True) # reset the index of the DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "580102"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the number of articles after excluding articles based on the text patterns\n",
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({2000: 33949, 1999: 32861, 2001: 30730, 1996: 27326, 1997: 27086, 1998: 26912, 2004: 26432, 1995: 26170, 1994: 25349, 2002: 25083, 2005: 24726, 2003: 24350, 2006: 24167, 2007: 23816, 2010: 23811, 2008: 23520, 2009: 23329, 2011: 22196, 2012: 20667, 2013: 17506, 2014: 17397, 2015: 16297, 2016: 13102, 2017: 12152, 2018: 11168})\n"
     ]
    }
   ],
   "source": [
    "counter=collections.Counter(data['year'])\n",
    "print(counter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Umlauts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To fix the issue with umlauts, we use the notebook `Umlauts_fix` written in Python 2. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "umlauts = ['√§', '√∂', '√º', '√ü', '√Ñ', '√ñ', '√ú']\n",
    "umlauts_replace = ['ae', 'oe', 'ue', 'ss', 'AE', 'OE', 'UE']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "hb_umlauts_fix = data[(data.texts.str.contains('|'.join(umlauts_replace))) & (~data.texts.str.contains('|'.join(umlauts))) & (data.year<1999)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1994 wird ein Jahr des Wandels. Auf ein Neues!. Steuern und Beitraege steigen, staatliche Leistungen werden gesenkt - wahrlich kein berauschender Auftakt fuer das neue Jahr. Ohnehin sind die Erwartungen nicht sehr rosig. Zwar wird sich die Wirtschaft erholen. Ob es zu einem kraeftigen Aufschwung reicht, ist ungewiss. Sicher ist dagegen der Anstieg der Arbeitslosigkeit. Fuer Spannung sorgt der Wahlmarathon, der erst mit den Bundestagswahlen im Oktober entschieden wird. Kein Zweifel: 1994 wird ein Jahr der Unsicherheit und des Wandels. Ob es auch ein gutes Jahr wird, haengt davon ab, wie dieser Wandel bewaeltigt wird. 1993 war das Jahr der Krisenerkenntnis. Die Einsicht in die Kluft zwischen wirtschaftlich Machbarem und Anspruchsdenken hat erste Ansaetze zur Krisenbewaeltigung ermoeglicht. In der Tarifpolitik und bei den Sozialleistungen hat es - noch vor kurzem undenkbare - Einschnitte gegeben. Auch 1994 wird es Kuerzungen geben. Nur: Mit dem Rotstift allein kann kein Ausweg aus der Krise gezeichnet werden. In den Unternehmen sind innovative Strategien gefragt. Anstatt den Guertel immer wieder ein wenig enger zu schnallen, sollte der Staat seine Rolle grundsaetzlich ueberdenken. Ein Rueckzug auf die Kernaufgaben schafft verlaessliche Rahmendaten. Und die sind in unsicheren Zeiten besonders wichtig. Die Krise muss als Herausforderung, Wandel als Chance zum Fortschritt aufgefasst werden. Dann kann 1994 ein gutes Jahr werden - selbst wenn es zu Sylvester im Portemonnaie noch nicht messbar ist.'"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# example of the text that we try to fix with a spellchecker\n",
    "hb_umlauts_fix.texts[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "hb_umlauts_fix.to_csv('hb_umlauts_fix.csv', encoding='utf-8-sig', sep = ';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mokuneva\\AppData\\Local\\Temp\\2\\ipykernel_31288\\1128006412.py:2: DtypeWarning: Columns (3) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  hb_umlauts_fixed = pd.read_csv('hb_umlauts_fixed.csv', encoding = 'utf-8', sep=';')\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "hb_umlauts_fixed = pd.read_csv('hb_umlauts_fixed.csv', encoding = 'utf-8', sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.loc[hb_umlauts_fixed['Unnamed: 0.1'], 'texts'] = hb_umlauts_fixed.texts.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1994 wird ein Jahr des Wandels. Auf ein Neues!. Steuern und Beitr√§ge steigen, staatliche Leistungen werden gesenkt - wahrlich kein berauschender Auftakt f√ºr das neue Jahr. Ohnehin sind die Erwartungen nicht sehr rosig. Zwar wird sich die Wirtschaft erholen. Ob es zu einem kr√§ftigen Aufschwung reicht, ist ungewiss. Sicher ist dagegen der Anstieg der Arbeitslosigkeit. F√ºr Spannung sorgt der Wahlmarathon, der erst mit den Bundestagswahlen im Oktober entschieden wird. Kein Zweifel: 1994 wird ein Jahr der Unsicherheit und des Wandels. Ob es auch ein gutes Jahr wird, h√§ngt davon ab, wie dieser Wandel bew√§ltigt wird. 1993 war das Jahr der Krisenerkenntnis. Die Einsicht in die Kluft zwischen wirtschaftlich Machbarem und Anspruchsdenken hat erste Ans√§tze zur Krisenbew√§ltigung erm√∂glicht. In der Tarifpolitik und bei den Sozialleistungen hat es - noch vor kurzem undenkbare - Einschnitte gegeben. Auch 1994 wird es K√ºrzungen geben. Nur: Mit dem Rotstift allein kann kein Ausweg aus der Krise gezeichnet werden. In den Unternehmen sind innovative Strategien gefragt. Anstatt den G√ºrtel immer wieder ein wenig enger zu schnallen, sollte der Staat seine Rolle grunds√§tzlich √ºberdenken. Ein R√ºckzug auf die Kernaufgaben schafft verl√§ssliche Rahmendaten. Und die sind in unsicheren Zeiten besonders wichtig. Die Krise muss als Herausforderung, Wandel als Chance zum Fortschritt aufgefasst werden. Dann kann 1994 ein gutes Jahr werden - selbst wenn es zu Sylvester im Portemonnaie noch nicht messbar ist.'"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fixed version\n",
    "data.texts[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove English articles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the next pre-processing step, we filter out news articles written in English. To do that, we use a **langdetect** library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:12:51.574581\n"
     ]
    }
   ],
   "source": [
    "startTime = datetime.now()\n",
    "\n",
    "import identify_eng\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    pool = mp.Pool(NUM_CORE)\n",
    "    eng_results = pool.map(identify_eng.identify_eng, [text for text in data['texts']]) \n",
    "    pool.close()\n",
    "    pool.join()\n",
    "\n",
    "print(datetime.now()-startTime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['language'] = eng_results\n",
    "data = data[data.language==0]\n",
    "data.reset_index(inplace=True, drop=True) # reset the index of the DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "580045"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the number of articles after excluding English articles\n",
    "len(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove URLs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove URLs and html file references.\n",
    "E.g.: 'de/studie99.html', 'Amazon.com', etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:00:41.858401\n"
     ]
    }
   ],
   "source": [
    "startTime = datetime.now()\n",
    "\n",
    "import correct_url\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    pool = mp.Pool(NUM_CORE)\n",
    "    url_corrected = pool.map(correct_url.correct_url, [text for text in data['texts']]) \n",
    "    pool.close()\n",
    "    pool.join()\n",
    "\n",
    "print(datetime.now()-startTime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['texts'] = url_corrected"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## O instead of 0 problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In some cases, Optical Character Recognition (OCR) can not distinguish between '0' and 'O' ('o'). As a result, there are tokens like '1OO'. Using regular expressions, we identify problematic tokens and replace 'O' ('o') with '0'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:00:19.013766\n"
     ]
    }
   ],
   "source": [
    "startTime = datetime.now()\n",
    "\n",
    "import ocr_replace\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    pool = mp.Pool(NUM_CORE)\n",
    "    ocr_corrected = pool.map(ocr_replace.ocr_replace, [text for text in data['texts']]) \n",
    "    pool.close()\n",
    "    pool.join()\n",
    "\n",
    "print(datetime.now()-startTime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['texts'] = ocr_corrected"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fixing tokens containing a number and a word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In quite a few cases, a number and a word are erroneously merged together into a single token. Splitting these tokens into two tokens helps us to deal with the following problems:\n",
    "\n",
    "1. Hyphen-separated words will have the same spelling across the whole corpus. Examples: 20J√§hrige/20-J√§hrige, 100prozentig/100-prozentig, 30min√ºtigen/30-min√ºtigen etc.\n",
    "\n",
    "2. Numbers and the names of the currencies will represent two different tokens. Examples: 100DM/100 DM, 30Euro/30 Euro.\n",
    "\n",
    "3. Numbers and the measures of time/weight/distance etc. will be split into two tokens. Examples: 100km/100 km, 1Mill/1 Mill, 16Uhr/16 Uhr.\n",
    "\n",
    "4. Simple mistakes will be corrected: 30bis 40/30 bis 40, 10Fahrzeuge/10 Fahrzeuge, 10und/10 und.\n",
    "\n",
    "5. Mistakes in the beginnning of the sentences will be corrected: 10Welche/10 Welche, 8Wie/8 Wie, etc.\n",
    "\n",
    "The most frequently met exceptions from this rule are taken into account:\n",
    "\n",
    "1. Names of the companies/organizations: 1822direkt, 3Sat, 4MBO etc.\n",
    "2. Names of smartphone/airplane/satellite models: 4S, 1B, 1k, 328Jet, 5C, 777X etc.\n",
    "3. German nouns like 90er and English adjectives like 21st, 3rd."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:00:19.568016\n"
     ]
    }
   ],
   "source": [
    "startTime = datetime.now()\n",
    "\n",
    "import split_number_word\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    pool = mp.Pool(NUM_CORE)\n",
    "    split_corrected = pool.map(split_number_word.split_number_word, [text for text in data['texts']]) \n",
    "    pool.close()\n",
    "    pool.join()\n",
    "\n",
    "print(datetime.now()-startTime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['texts'] = split_corrected"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove fuzzy duplicates\n",
    "\n",
    "By fuzzy duplicates we understand nearly duplicated articles. These are:\n",
    "\n",
    "* drafts/minor revisions of the articles saved in the database;\n",
    "* slightly changed advertisements which are published several times during a month;\n",
    "* 'NACHT - REPORT' news reports about stock market which are published a few hours earlier than almost identical 'WALL STREET' articles.\n",
    "\n",
    "We identify 'fuzzy' duplicates using cosine similarity and choose a threshold of 93% based on some visual exploration. Here is the article by Ryan Basques we used as a reference: [Link](https://towardsdatascience.com/a-laymans-guide-to-fuzzy-document-deduplication-a3b3cf9a05a7). \n",
    "\n",
    "There are also a few exceptions that we take into account:\n",
    "\n",
    "* 'DEUTSCHE AKTIEN' articles are often erroneously classified as duplicates because they contain very similar information;\n",
    "* 'Fortsetzung von Seite' (continuation of an article from another page): not surprisingly, these documents are also very close to each other. Later we concatenate them into one article;\n",
    "* cosine similarity approach does not work perfectly for extremely long articles. A few of them (e.g., 'Stuttgart 21: Ist das Projekt noch zu stoppen?') we list as exceptions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Required input for the function 'fuzzy_duplicates': a dataframe for each month-year combination.\n",
    "# List with a year\n",
    "inputs_year = []\n",
    "# List with a month\n",
    "inputs_month = []\n",
    "# List with the dataframes containing 'year', 'month', and 'texts' columns\n",
    "inputs_month_year = []\n",
    "for year in list(set(data['year'])):\n",
    "    for month in list(set(data['month'])):\n",
    "        inputs_year.append(year)\n",
    "        inputs_month.append(month)\n",
    "        inputs_month_year.append(data[(data['year'] == year) & (data['month'] == month)][[\"month\", \"year\", \"texts\"]])\n",
    "        \n",
    "inputs = list(zip(inputs_year, inputs_month, inputs_month_year))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:02:55.219889\n"
     ]
    }
   ],
   "source": [
    "startTime = datetime.now() # track time\n",
    "\n",
    "import fuzzy_duplicates # import a function that outputs the indices of fuzzy duplicates \n",
    "delete_indices = []\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    pool = mp.Pool(NUM_CORE)\n",
    "    # apply function to all combinations of month-year in parallel\n",
    "    delete_intermediate = pool.map(fuzzy_duplicates.fuzzy_duplicates, inputs)\n",
    "    delete_indices = delete_indices + delete_intermediate # create one list of indices\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "    \n",
    "print(datetime.now()-startTime)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Duplicates exploration\n",
    "\n",
    "In case you want to have a look at the identified duplicates, use the function 'fuzzy_duplicates_test' from the file 'fuzzy_duplicates_test_all'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:02:53.796095\n"
     ]
    }
   ],
   "source": [
    "startTime = datetime.now() # track time\n",
    "\n",
    "import fuzzy_duplicates_test_all \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    pool = mp.Pool(NUM_CORE)\n",
    "    dup_intermediate = pool.map(fuzzy_duplicates_test_all.fuzzy_duplicates_test, inputs) \n",
    "    duplicates = pd.concat(dup_intermediate) \n",
    "    pool.close()\n",
    "    pool.join()\n",
    "    \n",
    "print(datetime.now()-startTime)\n",
    "\n",
    "duplicates.to_csv('duplicates.csv', encoding = 'utf-8-sig', sep = ';')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drop the duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Free memory\n",
    "inputs = None\n",
    "# List of indices corresponding to the duplicated articles\n",
    "delete_indices = [item for sublist in delete_indices for item in sublist]\n",
    "# List of unique indices\n",
    "delete_indices = list(set(delete_indices))\n",
    "# Drop the fuzzy duplicates\n",
    "data.drop(data.index[delete_indices], inplace = True)\n",
    "data.reset_index(inplace=True, drop=True) # reset the index of the DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "579052"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the number of articles after removing fuzzy duplicates\n",
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_csv('before_chained.csv', encoding = 'utf-8-sig', sep = ';')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Concatenate chained articles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By the chained articles we mean the news reports that start on one page and continue on another page of the newspaper. Since the news database is prepared using OCR technology, each part of a chained news article represents a separate entry in the database, i.e. a separate news report.\n",
    "\n",
    "Ideally, all the parts of a chained article must be combined into one news report. However, as nothing in the metadata helps us identify chained articles, the merging process is not straightforward. We do our best to merge at least some chained articles based on the text alone."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data = pd.read_csv('before_chained.csv', encoding = 'utf-8-sig', sep=';', index_col = 0, dtype = {'kicker': 'str',\n",
    "                                                                                                 'page': 'str',\n",
    "                                                                                                 'series_title': 'str',\n",
    "                                                                                                 'rubrics': 'str'})\n",
    "data.page = data.page.fillna('')\n",
    "data.series_title = data.series_title.fillna('')\n",
    "data.kicker = data.kicker.fillna('')\n",
    "data.rubrics = data.rubrics.fillna('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second part of a chained article normally contains one of the following strings: 'Fortsetzung von Seite' (continuantion from page) or 'FORTSEZUNG VON SEITE'. We select the artilces that meet this critetion and try to merge them with their beginnings.\n",
    "\n",
    "However, we also exclude the articles that contain strings from both lists, `continued_strings_1` and `continued_strings_2`. This is because these chained articles have already been merged by the Handelsblatt team."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "continued_strings_1 = ['Fortsetzung S\\\\.', 'Fortsetzung auf Seite', 'Fortsetzung Seite', 'Fortsetzung n√§chste Seite',\n",
    "                       'Fortsetzung Seiten', 'FORTSETZUNG S\\\\.', 'FORTSETZUNG AUF SEITE', 'FORTSETZUNG SEITE', \n",
    "                       'FORTSETZUNG N√ÑCHSTE SEITE', 'FORTSETZUNG SEITEN']\n",
    "continued_strings_2 = ['Fortsetzung von Seite', 'FORTSETZUNG VON SEITE']\n",
    "\n",
    "# Articles that contain one of the strings from 'continued_strings_1' and one of the strings from \n",
    "# 'continued_strings_2' are examples of chained articles that have already been merged. \n",
    "continued_articles = data[(data.texts.str.contains('|'.join(continued_strings_2))) & \n",
    "                         (~data.texts.str.contains('|'.join(continued_strings_1)))]\n",
    "continued_articles = continued_articles.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "844"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# There are 844 articles that could potentially be part of the chained articles.\n",
    "len(continued_articles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An input of the function `chained_articles` is a tuple, where the first element is a row of the dataframe corresponding to the second part of the chained article. The second element of the tuple is a dataframe containing all articles published on the same day as the second part of the chained article, i.e. all articles that could potentially be the first part of the chained article."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Required input for the function 'chained_articles': a row of the 'continued_articles' df, and a subset of the\n",
    "# 'data' df corresponding to the articles published on the same day as the considered chained article.\n",
    "\n",
    "# List with the potentially chained articles\n",
    "cont_input = []\n",
    "# List with the dataframes containing articles published on the same day as the considered chained article\n",
    "data_input = []\n",
    "\n",
    "# To merge the articles from 'chain_exceptions' with the corresponding first parts, more complicated rules \n",
    "# are required than those we use in the funciton 'chained_articles'. Therefore, we list them as exceptions.\n",
    "chain_exceptions = [139, 140, 185]\n",
    "# An input for the function 'chained_articles'.\n",
    "for ind in continued_articles.index:\n",
    "    if ind not in chain_exceptions:\n",
    "        cont_input.append(continued_articles.iloc[[ind]])\n",
    "        data_input.append(data[(data.day == continued_articles['day'][ind]) & \n",
    "                      (data.month == continued_articles['month'][ind]) &\n",
    "                     (data.year == continued_articles['year'][ind])])\n",
    "    \n",
    "inputs_cont = list(zip(cont_input, data_input))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function `chained_articles` returns a list with two tuples. The first tuple contains two indices corresponging to the first and second part of a chained article. These two parts must be merged into one article.\n",
    "    \n",
    "The second tuple contains two indices as well. The first index corresponds to a part of the chained article that is a duplicate because the merged version of this chained article already exists in the database. The article with this index must be deleted. The second index corresponds to the merged version of the chained article. This article must be kept in the database.\n",
    "\n",
    "All the rules we use to find the first part of a chained article are described in detail in the function `chained_articles`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of cores that will be used: 56\n"
     ]
    }
   ],
   "source": [
    "import multiprocessing as mp # use multiprocessing module for parallel computing\n",
    "\n",
    "NUM_CORE = mp.cpu_count()-8 # set the number of cores to use\n",
    "\n",
    "print(\"The number of cores that will be used: {}\".format(NUM_CORE))\n",
    "\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:00:07.383422\n"
     ]
    }
   ],
   "source": [
    "startTime = datetime.now() # track time\n",
    "\n",
    "import chained_articles # import a function that outputs two parts of a chained article and duplicated articles\n",
    "chained_pair = []\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    pool = mp.Pool(NUM_CORE)\n",
    "    chained_intermediate = pool.map(chained_articles.chained_articles, inputs_cont)\n",
    "    chained_pair = chained_pair + chained_intermediate \n",
    "    pool.close()\n",
    "    pool.join()\n",
    "    \n",
    "print(datetime.now()-startTime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "206"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The list of unique tuples, where the first element is a duplicate and \n",
    "# the second element is a merged chained article.\n",
    "duplicates = list(set([sublist[1] for sublist in chained_pair if sublist[1] != (-1, -1)]))\n",
    "# The list with indices of all the duplicated/merged articles.\n",
    "dup_merged = [tup[0] for tup in duplicates]+[tup[1] for tup in duplicates]\n",
    "# The list with all the chained articles that we would like to merge.\n",
    "chained_pair_list = [sublist[0] for sublist in chained_pair if sublist[0] != ()]\n",
    "# Exclude duplicated/merged articles from the list of the chained articles.\n",
    "chained_pair_list = [pair for pair in chained_pair_list if (pair[0] not in dup_merged) and \\\n",
    "                     (pair[1] not in dup_merged)]\n",
    "# The rules we use allow us to merge 206 chained articles\n",
    "len(chained_pair_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'AUSL√ÑNDISCHE B√ñRSEN / Alle Indizes verbuchen Gewinne - London geschlossen. Aktienm√§rkte in Europa begr√ºssen das neue Jahr mit steigenden Notierungen. cbu D√úSSELDORF. Die europ√§ischen Aktienm√§rkte gingen sehr freundlich in das neue Jahr. Alle f√ºhrenden Indizes lagen im Plus, die wichtige B√∂rse in London blieb aber geschlossen. In Fernost erreichten Singapur und Hongkong neue Rekordst√§nde.'"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# An example of the first part\n",
    "data['texts'][chained_pair_list[0][0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'AUSL√ÑNDISCHE B√ñRSEN / Fortsetzung von Seite 15. Weitere Rekorde in Fernost Tokio: Geschlossen. Wegen eines Feiertages blieben Beh√∂rden, Banken und M√§rkte geschlossen.'"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# An example of the second part\n",
    "data['texts'][chained_pair_list[0][1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "203"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The indices of the first part of the chained articles.\n",
    "first_part_ind = [pair[0] for pair in chained_pair_list]\n",
    "# The indices of the second part of the chained articles.\n",
    "second_part_ind = [pair[1] for pair in chained_pair_list]\n",
    "# Consider the case where the same article was paired with two different articles.\n",
    "common_ind = [ind for ind in first_part_ind if ind in second_part_ind]\n",
    "# In this case remove one of the pairs.\n",
    "chained_pair_list = [pair for pair in chained_pair_list if pair[0] not in common_ind]\n",
    "# The number of chained articles we are able to concatenate.\n",
    "len(chained_pair_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The indices of the first part of the chained articles.\n",
    "first_part_ind = [pair[0] for pair in chained_pair_list]\n",
    "# The indices of the second part of the chained articles.\n",
    "second_part_ind = [pair[1] for pair in chained_pair_list]\n",
    "# Merge the first and the second parts.\n",
    "data.loc[first_part_ind, 'texts'] = data.loc[first_part_ind, 'texts'].values + \\\n",
    "\" \" + data.loc[second_part_ind, 'texts'].values\n",
    "# Drop the second part.\n",
    "data.drop(data.index[second_part_ind], inplace = True)\n",
    "# The indices of the duplicates.\n",
    "dup_indices = [tup[0] for tup in duplicates]\n",
    "# Drop the duplicates.\n",
    "data.drop(data.index[dup_indices], inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:00:01.022448\n"
     ]
    }
   ],
   "source": [
    "import count_words_mp # import the function calculating the number of words in a text\n",
    "# Calculate the word count of the merged articles.\n",
    "\n",
    "startTime = datetime.now() # track time\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    pool = mp.Pool(NUM_CORE)\n",
    "    count_results = pool.map(count_words_mp.count_words_mp, [text for text in data.loc[first_part_ind, 'texts']]) \n",
    "    pool.close()\n",
    "    pool.join()\n",
    "    \n",
    "print(datetime.now()-startTime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace the \"word_count\" of the merged articles.\n",
    "data.loc[first_part_ind, 'word_count'] = count_results\n",
    "# Drop short articles.\n",
    "data = data[data['word_count']>=100]\n",
    "data.reset_index(inplace=True, drop=True) # reset the index of the DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "578620"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the number of articles after concatenating chained articles\n",
    "len(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handelsblatt-specific problems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a few small problems that are specific to Handelsblatt and that we have noticed while fixing other issues. \n",
    "\n",
    "&nbsp; &nbsp; 1. The end of the text with the index 205950 contains unusual symbols. We have removed this part from the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'l'"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This is the part we delete.\n",
    "data['texts'][205950][data['texts'][205950].find(' k{'):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.at[205950, 'texts'] = data['texts'][205950][:data['texts'][205950].find(' k{')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&nbsp; &nbsp; 2. Some texts contain unicode errors. For example, the name Jaros≈Çaw is written as Jaros√Ö‚Äöaw. We fix these errors using the function `unicode_correct`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:00:02.025037\n"
     ]
    }
   ],
   "source": [
    "import unicode_correct # import the function correcting the unicode errors present in the database\n",
    "\n",
    "startTime = datetime.now() # track time\n",
    "\n",
    "to_correct = list(data[data.texts.str.contains('√Ö')].index)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    pool = mp.Pool(NUM_CORE)\n",
    "    unicode_corrected = pool.map(unicode_correct.unicode_correct, [text for text in data.loc[to_correct, 'texts']]) \n",
    "    pool.close()\n",
    "    pool.join()\n",
    "    \n",
    "print(datetime.now()-startTime)\n",
    "\n",
    "# Replace the texts with the corrected version of the articles.\n",
    "data.loc[to_correct, 'texts'] = unicode_corrected"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&nbsp; &nbsp; 3. We drop two texts that contain a string 'MMHANDELSBLATT' because they include a lot of errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['texts'][71133][data['texts'][71133].find('MMHANDELSBLATT'):(data['texts'][71133].find('MMHANDELSBLATT')+1000)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices_drop = list(data[data.texts.str.contains('MMHANDELSBLATT')].index)\n",
    "data.drop(data.index[indices_drop], inplace = True)\n",
    "data.reset_index(inplace=True, drop=True) # reset the index of the DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "578618"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the number of articles after excluding articles that contain a string 'MMHANDELSBLATT'\n",
    "len(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&nbsp; &nbsp; 4. We drop 12 texts with broken umlauts and too many non-systematic mistakes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:00:18.919588\n"
     ]
    }
   ],
   "source": [
    "startTime = datetime.now()\n",
    "\n",
    "# Find the words like '}ber' and 'gel|st' in the articles' texts.\n",
    "import find_umlaut\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    pool = mp.Pool(NUM_CORE)\n",
    "    problem_umlaut = pool.map(find_umlaut.find_umlaut, [text for text in data['texts']]) \n",
    "    pool.close()\n",
    "    pool.join()\n",
    "\n",
    "print(datetime.now()-startTime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Indices of potentially problematic articles.\n",
    "problem_umlaut_list = [tup[0] for tup in enumerate(problem_umlaut) if tup[1] != []]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The last five articles contain unusual words with '|' inside. However, in these cases, '|' does not correspond\n",
    "# to the umlaut '√∂'. Example: 'netzwerk|nordbayern'.\n",
    "data.drop(data.index[problem_umlaut_list[:-5]], inplace = True)\n",
    "data.reset_index(inplace=True, drop=True) # reset the index of the DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "578606"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the number of articles after excluding articles with broken umlauts and too many non-systematic mistakes\n",
    "len(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&nbsp; &nbsp; 5. Drop the text with an index 43024 because it is written in French (we have tested that all the other articles are in German)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.drop(data.index[43024], inplace = True)\n",
    "data.reset_index(inplace=True, drop=True) # reset the index of the DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "578605"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the number of articles after excluding the article in French\n",
    "len(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean articles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Handelsblatt articles include some text passages that are unlikely to be relevant for either topic modeling or sentiment analysis. We decided to clean the affected articles from these text passages to make the analysis easier for our models.\n",
    "\n",
    "We remove the following information from the texts:\n",
    "\n",
    "   * 1) additional information in the text meant for the author\n",
    "   * 2) the list of articles on a particular topic\n",
    "   * 3) reference to additional information\n",
    "   * 4) the calendar of events\n",
    "   * 5) the composition of an index\n",
    "   * 6) the box with additional information on the topic\n",
    "   * 7) the addresses/Internet addresses\n",
    "   * 8) the links\n",
    "   * 9) the word game for investors\n",
    "   * 10) information on how special pages on the topic can be ordered\n",
    "   * 11) educational articles on how to understand financial news\n",
    "   * 12) announcement of premieres\n",
    "   * 13) announcement of trade fairs\n",
    "   * 14) announcement of events\n",
    "   * 15) place and time/organizer of the event"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:01:58.302403\n"
     ]
    }
   ],
   "source": [
    "startTime = datetime.now()\n",
    "\n",
    "import clean_hb_articles\n",
    "if __name__ == \"__main__\":\n",
    "    pool = mp.Pool(NUM_CORE)\n",
    "    cleaned_articles = pool.map(clean_hb_articles.clean_hb_articles, [text for text in data['texts']]) \n",
    "    pool.close()\n",
    "    pool.join()\n",
    "    \n",
    "print(datetime.now()-startTime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['texts'] = cleaned_articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:00:22.909384\n"
     ]
    }
   ],
   "source": [
    "import count_words_mp # import the function calculating the number of words in a text\n",
    "# Calculate an updated word count of the cleaned articles.\n",
    "\n",
    "startTime = datetime.now() # track time\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    pool = mp.Pool(NUM_CORE)\n",
    "    count_results = pool.map(count_words_mp.count_words_mp, [text for text in data['texts']]) \n",
    "    pool.close()\n",
    "    pool.join()\n",
    "    \n",
    "print(datetime.now()-startTime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update the word count in the data frame.\n",
    "data['word_count'] = count_results\n",
    "# Drop short articles.\n",
    "data = data[data['word_count']>=100]\n",
    "data.reset_index(inplace=True, drop=True) # reset the index of the DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "578392"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the number of articles after excluding articles shorter than 100 words\n",
    "len(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exclude tables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we preprocess the news articles by removing tables to minimize noise and emphasize relevant content. We begin by calculating a numerical density metric for each text, which is computed as the ratio of the count of numbers to the total word count (excluding numbers). Texts with a numerical density of at least 20% are considered as candidates for containing tables. We manually examine these texts to identify recurring strings that typically precede tables. Using regular expressions, we exclude the tables based on these strings. Moreover, we delete some text segments predominantly comprising numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:00:13.328351\n"
     ]
    }
   ],
   "source": [
    "# use the 'numeric_articles' function to identify articles with a high share of numbers in them\n",
    "import sys\n",
    "import os\n",
    "import itertools\n",
    "sys.path.insert(1, os.getcwd().replace('Handelsblatt', 'dpa'))\n",
    "import numeric_articles\n",
    "\n",
    "inputs = zip(data['texts'], data['word_count'], itertools.repeat(0.20))\n",
    "\n",
    "startTime = datetime.now()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    pool = mp.Pool(NUM_CORE)\n",
    "    tables = pool.starmap(numeric_articles.numeric_articles, inputs)\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "\n",
    "print(datetime.now()-startTime)\n",
    "data['tables'] = tables\n",
    "tables = data[data.tables == True]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is an example of an article containing a table with details on new bond issuances in the Euromarket by various financial institutions. To remove this table, we apply the following regular expression:  `r'Londoner Banken berichteten von folgenden neuen Euromarkt.{0,}'`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ANLEIHEN / Die Kapitalnachfrage ist weiter sehr hoch - Anleger sind zur√ºckhaltend. Bayernhypo und DSL am DM - Markt. ret FRANKFURT / M. Auch zur Wochenmitte sahen sich die Bondm√§rkte mit zahlreichen Kapitalw√ºnschen konfrontiert. Die anhaltend hohe Kapitalnachfrage war eine Ursache f√ºr die Zur√ºckhaltung der Anleger und f√ºr die in diesem Kontext festzustellende anhaltende Korrekturphase an den Anleihem√§rkten rund um den Globus. Auch der deutsche Rentenmarkt neigte leicht zur Schw√§che. B√∂rsennotierte √∂ffentliche DM - Anleihen wiesen Kursver√§nderungen zwischen plus 0,10 DM und minus 0,35 DM auf. Die Umlaufrendite erh√∂hte sich um einen Basispunkt auf 5,44%. Die den Markt regulierenden Stellen gaben Material im Nennwert von 38,9 Mill. DM in Frankfurt an den Markt ab; davon entfielen 20 Mill. DM auf die 61 / 4% ige Silvesteranleihe des Bundes, die bei einem Emissionskurs von 100,40% zu 99,68% in den amtlichen Handel eingef√ºhrt wurde. An der D√ºsseldorfer B√∂rse belief sich der Abgabesaldo auf 111,5 Mill. DM. Der Rex wurde mit 109,25 (109,31), der RexP mit 156,26 (156,31) ermittelt. Am Sekund√§raktion f√ºr DM - Auslandsanleihen ergaben sich nur geringe Kursver√§nderungen. Der zuletzt sehr aktive Prim√§rseite sah sich mit zwei weiteren gr√∂√üeren Emissionen konfrontiert. Die Bayerische Hypothekenbank AG kam mit einem j√§hrigen Bond √ºber 500 Mill. DM f√ºr ihre Finanztochter Bayer Hypo Finance N.V. Die 6% ige Emission wird zu 101,10% angeboten. Die DSL Finance B.V. brachte √ºber die Dresdner Bank einen Bond √ºber 1 Mrd. DM. Genaue Konditionen der j√§hrigen Emission standen bei Redaktionsschluss noch nicht fest. Nach dem markanten Kursanstieg des Vortags zeigten sich Dollarbonds am Mittwoch etwas leichter. Nach Angaben des Handels warten die Anleger auf die f√ºr den morgigen Freitag anstehenden Arbeitsmarktdaten. Der f√ºr den Monat November gemeldete Auftragseingang langlebiger Wirtschaftsg√ºter best√§tigte den Aufw√§rtstrend der US - Konjunktur. Mit plus 1,4% lag dieser Konjunkturindikator √ºber der bei durchschnittlich 1,2% angesiedelten Sch√§tzung von Wall - Street - Analysten. Die j√§hrigen 61 / 4% igen \" Treasuries \" wurden gegen 10.30 Uhr New Yorker Ortszeit auf einem Kursniveau von 9814 / 32% zu 9815 / 32 um 6 / 32 schw√§cher bei einer Rendite von 6,37% gehandelt. Ebenso hoch wie am Vortag schlugen zur Wochenmitte die Neu - Emissionswellen am Eurobondmarkt. Londoner Banken berichteten von folgenden neuen Euromarkt - Emissionen: Abbey National Treasury, 1 Mrd. $ , 3 Jahre, 5% zu 101,30%, Goldman Sachs; La Caisse Centrale des Jardins du Quebec, 100 Mill. can $ , 6 Jahre, 61 / 4% zu 1011 / 4%, Hambros Bank; Primary Industry Bank of Australia, 50 Mill. a $ , 5 Jahre, 63 / 4% zu 101,15%, Hambros Bank; LB Schleswig Holstein, 100 Mill. lb, 10 Jahre, 61 / 4% zu 100,05%, Daiwa; Cooperative Wholesale Society Ltd, 50 Mill. lb, 25 Jahre, S.G. Warburg; Credit Foncier, 3 Mrd. FF, f√§llig am 28.11.2002, 53 / 4% zu 99,20%, BNP; Bayernhypo, 200 Mrd. Lire, 7 Jahre, 7,85% zu 101,685%, Euromobiliare; Credit Local de France, 200 Mrd. Lire, 5 Jahre, 71 / 2% zu 1017 / 8%, J.P. Morgan; GECC, 250 Mrd. Lire, 5 Jahre, 73 / 8% zu 101,40%, Paribas; K√∂nigreich Schweden, 250 Mrd. Lire, 5 Jahre, 7,375%, zu 1011 / 2%, Deutsche Bank; Rabobank, 200 Millionen hfl, 8 Jahre, 57 / 8% zu 101,95%, Rabobank; Stadt Amsterdam, 500 Mill. hfl, 10 Jahre, 51 / 2% zu 99,65%, ING Bank; De Nationale Investeringsbank, 200 Mill. hfl, 15 Jahre, 6% zu 991 / 2%, ING; OEKB, 30 Mrd. Yen - Floater, 7 Jahre, Sechsmonatslibor plus 70 Basispunkte zu 107,816%, Nikko Europe.'"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tables.loc[215,'texts']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clean tables using regular expressions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:01:01.384302\n"
     ]
    }
   ],
   "source": [
    "import clean_tables_hb\n",
    "startTime = datetime.now()\n",
    "if __name__ == \"__main__\":\n",
    "    pool = mp.Pool(NUM_CORE)\n",
    "    remove_tables = pool.map(clean_tables_hb.clean_tables_hb, [text for text in data['texts']]) \n",
    "    pool.close()\n",
    "    pool.join()\n",
    "    \n",
    "print(datetime.now()-startTime)\n",
    "\n",
    "data['texts'] = remove_tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:00:21.745026\n"
     ]
    }
   ],
   "source": [
    "startTime = datetime.now()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    pool = mp.Pool(NUM_CORE)\n",
    "    count_results = pool.map(count_words_mp.count_words_mp, [text for text in data['texts']]) \n",
    "    pool.close()\n",
    "    pool.join()\n",
    "    \n",
    "print(datetime.now()-startTime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update the content of the column \"word_count\"\n",
    "data['word_count'] = count_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "578168\n"
     ]
    }
   ],
   "source": [
    "# remove articles with less than 100 words\n",
    "data = data[data['word_count']>=100]\n",
    "data = data.sort_values(['year', 'month', 'day'], ascending=[True, True, True]) # sort the data in chronological order\n",
    "data.reset_index(inplace=True, drop=True)\n",
    "# the number of articles after excluding articles shorter than 100 words\n",
    "print(len(data))\n",
    "del data['tables']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Identify articles that predominantly consist of numbers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Articles that predominantly consist of numerical data present challenges for sentiment or topic analysis as, once numbers are removed, they tend to contain limited information. Therefore, we remove articles with a numerical density of 50% or greater."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:00:14.238645\n"
     ]
    }
   ],
   "source": [
    "# use the 'numeric_articles' function to identify economic articles with a high share of numbers in them\n",
    "inputs = zip(data['texts'], data['word_count'], itertools.repeat(0.50))\n",
    "\n",
    "startTime = datetime.now()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    pool = mp.Pool(NUM_CORE)\n",
    "    numeric_list = pool.starmap(numeric_articles.numeric_articles, inputs)\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "\n",
    "print(datetime.now()-startTime)\n",
    "data['numeric'] = numeric_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Liste der NBL - Beauftragten. Manfred Baatsch Generalbevollm√§chtigter Kl√∂ckner - Humboldt - Deutz AG Deutz - M√ºlheimer - Stra√üe 111 51063 K√∂ln Telefon: 0221 / 8222090 Telefax: 0221 / 8222095 Dr. Pol Bamelis Mitglied des Vorstandes Bayer AG 51368 Leverkusen Telefon: 0214 / 3081841 Telefax: 0214 / 3050720 Dr. Karl Braunhofer Mitglied des Vorstandes BayWa AG Arabellastrasse4, 81925 M√ºnchen Telefon: 089 / 9222 - 3250 Telefax: 089 / 9222 - 3703 Bruno √úberredender Leiter der Materialwirtschaft Siemag Transplan GmbH Obere Industriestrasse8 57250 Netphen Telefon: 02738 / 21 - 0 Telefax: 02738 / 21 - 297 Manfred Ciesielski Mitglied des Vorstandes Karstadt AG Theodor - Althoff - Strasse2 45133 Essen Telefon: 0201 / 7271 Telefax: 0201 / 7274958 Carl T√∂nen Leiter des Einkaufs Deutsche Shell AG Postfach 600520, √úberseering 35 22297 Hamburg Telefon: 040 / 6324 - 5800 Telefax: 040 / 6324 - 5822 Rolf Dill Ostbeauftragter Tengelmann Warenhandels sellschaft Wissollstrasse5, 45478 M√ºhlheim Telefon: 0208 / 5806 - 596 Telefax: 0208 / 5806 - 742 Ulrich Diller Leiter Zentrale Materialwirtschaft Zahnradfabrik Friedrichshafen AG Postfach 88038 88046 Friedrichshafen Telefon: 07541 / 777470 Telefax: 07541 / 777180 Arnold Dohmen Gesch√§ftsbereichsleiter Einkauf Deutsche Bundespost Telekom Postfach 2000, 53105 Bonn Telefon: 0228 / 181 - 0 Telefax: 0228 / 181 - 8872 Dr. Michael Dunkelberg Generalbevollm√§chtigter L.Possehl & Co. mbH Beckergrube 38 - 52, 23552 L√ºbeck Telefon: 0451 / 148210 Telefax: 0451 / 148255 Gerhard Full Mitglied des Vorstandes Linde AG Abraham - Lincoln - Stra√üe 21 65189 Wiesbaden Telefon: 0611 / 7700 Telefax: 0611 / 770269 Dr. Heinz Gentz Mitglied des Vorstandes Veba AG Bennigsen - Platz1 40474 D√ºsseldorf Telefon: 0211 / 4579270 / 1 Telefax: 0211 / 4579535 Rainer Grohe Mitglied des Vorstandes Viag AG Georg - von - B√∂selager - Stra√üe 35 53117 Bonn Telefon: 0228 / 5522022 Telefax: 0228 / 5521959 Dr. Hans - G√ºnter Gr√ºnewald Mitglied der Gesch√§ftsf√ºhrung Henkel KGaA, Henkelstrasse 67 40589 D√ºsseldorf Telefon: 0211 / 7974455 Telefax: 0211 / 7982325 Roland G√ºnther Leiter des Einkaufs Esso AG Kapstadtring2 22297 Hamburg Telefon: 040 / 6393 - 2291 Telefax: 040 / 6393 - 2111 Ernst - Eggert Gumrich Direktor Degussa AG Weissfra√ºnstrasse9 60311 Frankfurt Telefon: 069 / 2185144 Telefax: 069 / 2182955 Willi Harrer Mitglied des Vorstandes Quelle Schickedanz AG & Co. N√ºrnberger Stra√üe 91 - 95 90762 F√ºrth Telefon: 0911 / 1422840 Telefax: 0911 / 1424123 Petra Hartjes Coca - Cola GmbH Max - Keith - Stra√üe 66 45136 Essen Telefon: 0201 / 8211486 Telefax: 0201 / 8211176 Werner Haverkamp Gesch√§ftsf√ºhrender Union Deutsche Lebensmittelwerke GmbH Dammtorwall 15, 20355 Hamburg Telefon: 040 / 3493103 Telefax: 040 / 354742 Dr. occ. publ. J√ºrgen Ger√§usch Vorsitzender der Gesch√§ftsf√ºhrung Ger√§usch Holding GmbH Her√§usstrasse 12 - 14, 63450 Hanau Telefon: 06181 / 35 - 417 Telefax: 06181 / 35 - 677 Dr. Holger Hildebrand Hauptbereichsleiter Ruhrgas AG Huttropstrasse 60, 45138 Essen Telefon: 0201 / 1843086 Telefax: 0201 / 1843189 Dr. Claus Dieter Hoffmann Direktor Robert Bosch GmbH Robert - Bosch - Platz1 70839 Gerlingen Telefon: 0711 / 8116715 Telefax: 0711 / 8116053 Dr. Hans - Jochen H√ºchting Leiter Personal und Sozialwesen Firma Carl Freudenberg Postfach 100363, 69465 Weinheim Telefon: 06201 / 80 - 1 Telefax: 06201 / 69300 Dr. Ulf H√ºlbert Mitglied des Vorstandes MAN AG Ungererstrasse 69 80805 M√ºnchen Telefon: 089 / 15802233 Telefax: 089 / 15803366 Dr. Werner Inderfurth Direktor Deutsche Babcock AG Duisburger Stra√üe 375 46049 Oberhausen Telefon: 02871 / 922463 Telefax: 02871 / 922784 Dr. Gerhard Jooss Mitglied des Vorstandes Friedrich Krupp AG Altendorfer Stra√üe 103 45143 Essen Telefon: 0201 / 1882110 Telefax: 0201 / 1882373 G√ºnter Jordan Leiter des Einkaufs Deutsche Bundespost Postdienst Heinrich - von - Stephan - Strasse1 53175 Bonn Telefon: 0228 / 1824300 Telefax: 0228 / 1827024 Bernd Juskowiak Leiter des Einkaufs Mobil Oil AG Steinstrasse5, 20095 Hamburg Telefon: 040 / 30022517 Telefax: 040 / 30022830 Helmut Kittler Leiter des Einkaufs Adam Opel AG, Postfach 1710 65407 R√ºsselsheim Telefon: 06142 / 666612 Telefax: 06142 / 12227 Horst Kramp Mitglied des Vorstandes Schering AG M√ºllerstrasse 170 - 178 13353 Berlin Telefon: 030 / 4681973 Telefax: 030 / 4682438 Ulrich Krause Gesch√§ftsf√ºhrender Handelsges. Sparrenberg mbH Robert - Bunsen - Strasse3 33617 Bielefeld Telefon: 0521 / 155 - 2713 Telefax: 0521 / 155 - 2864 Kurt Krieg Mitglied des Vorstandes Strabag Stra√üen - und Tiefbau AG Siegburger Stra√üe 241 50679 K√∂ln Telefon: 0221 / 82401 Telefax: 0221 / 8242936 Dr. Gerhard K√ºhne Mitglied des Vorstandes Siemens AG Wittelsbacherplatz2 80333 M√ºnchen Telefon: 089 / 2343700 / 39 Telefax: 089 / 2344001 Dr. Gerd K√ºnne Gesch√§ftsf√ºhrender Hermes Schleifmittel GmbH & Co. Luruper Hauptstra√üe 106 - 122 22547 Hamburg Telefon: 040 / 8330 - 210 Telefax: 040 / 8330 - 230 Dr. Gerhard Liener Mitglied des Vorstandes Daimler - Benz AG Postfach 800230 70502 Stuttgart Telefon: 0711 / 1794320 Telefax: 0711 / 1794325 Dr. Gerhard L√∂gters Mitglied des Vorstandes Philipp Holzmann AG Taunusanlage1, 60329 Frankfurt Telefon: 069 / 262400 Telefax: 069 / 262406 J√ºrgen. Maas Mitglied des Vorstandes Kaufhof Holding AG und Mitglied der Gesch√§ftsf√ºhrung des Metro - Gruppen - Einkaufs Postfach 101008, 50450 K√∂ln Claus M√§rz - Siebje Gesch√§ftsf√ºhrender Diehl GmbH & Co. Stephanstrasse 49, 90478 N√ºrnberg Telefon: 0911 / 9472744 Telefax: 0911 / 9473691 Gerhard Meise NG / EK Fordwerke AG Henry - Ford - Strasse1 50735 K√∂ln Telefon: 0221 / 9014286 Telefax: 0221 / 9012650 Axel Minuth Leiter des Zentraleinkaufs Alcatel SEL AG Lorenzstrasse 10, 70435 Stuttgart Telefon: 0711 / 821 - 5352 Telefax: 0711 / 821 - 3230 Manfred M√ºller Vorsitzender des Vorstandes Nordwest Handel AG Berliner Stra√üe 26 - 36 58135 Hagen Telefon: 02331 / 461389 Telefax: 02331 / 461104 Karl - Ludwig Natho Direktor Firma E.Merck Frankfurter Stra√üe 250 64293 Darmstadt Telefon: 06151 / 722332 Telefax: 06151 / 781669 Peter Nietzold Gesch√§ftsleitung f√ºr Marketing / Kommunikation Computer Service Partner GmbH (CSP) Wolfener Stra√üe 22, 12681 Berlin Telefon: 030 / 93897 - 100 Telefax: 030 / 93897 - 109 Hans - Werner Nolting Stellv. Vorstandsmitglied Metallgesellschaft AG Reuterweg 14, 60323 Frankfurt Telefon: 069 / 1593636 Telefax: 069 / 1592109 Rainer Nowak Leiter Zentraleinkauf Bosch - Siemens Hausger√§te GmbH Hochstra√üe 17, 81669 M√ºnchen Telefon 089 / 45902273 Telefax 089 / 45902985 Hans - J√∂rg Petersen Bereichsleiter Edeka Zentrale AG New - York - Ring6, 22297 Hamburg Telefon: 040 / 63770 Telefax: 040 / 63772231 Josef Pink Leiter des Einkaufs B√∂hringer Ingelheim KG 55218 Ingelheim Telefon: 06132 / 772526 Telefax: 06132 / 774509 Werner Rau Gesch√§ftsf√ºhrender Einkauf / Export Adolf W√ºrth GmbH & Co. KG Postfach 1261, 74653 K√ºnzelsau Telefon: 07940 / 150 Telefax: 07940 / 154115 Manfred Reimer Regionaldirektor Berlin und Deutschland Ost Deutsche Lufthansa AG Kleiststrasse 23 - 26, 10787 Berlin Telefon: 030 / 8875 - 3001 Telefax: 030 / 8875 - 3600 Eckart Riefenstahl Leiter der Beschaffung Porsche AG Porschestrasse 42, 70435 Stuttgart Telefon: 0711 / 8270 Telefax: 0711 / 8276241 Prof. Dr. Karlheinz Gener√∂ser Mitglied des Vorstandes Thyssen AG August - Thyssen - Strasse1 40211 D√ºsseldorf Telefon: 0211 / 82436273 Telefax: 0211 / 82438885 Klaus Sattler Mitglied des Vorstandes Nestle Deutschland AG Lyoner Stra√üe 23 Postfach 710707, 60497 Frankfurt Telefon: 069 / 66711 Telefax: 069 / 66713190 Dr. Ernst Schadow Mitglied des Vorstandes Hoechst AG Postfach 800320 65903 Frankfurt Telefon: 069 / 3055242 Telefax: 069 / 359501 Dr. Dieter Schadt Vorsitzender des Vorstandes Franz Haniel & Cie. GmbH Postfach 130420 47104 Duisburg Telefon: 0203 / 806251 Telefax: 0203 / 806673 Dr. Peter Sch√§fer Mitglied des Vorstandes Beierdorf AG Tr√§nennasse 48, 20253 Hamburg Telefon: 040 / 5692310 Telefax: 040 / 5692002 Dr. Erhard Schipporeit Vorsitzender des Vorstandes Varta AG Am Leineufer 51, 30419 Hannover Telefon: 0511 / 7903602 Telefax: 0511 / 7903766 Werner Schlotmann Mitglied des Vorstandes Bilfinger & Berger Bau AG Carl - Rei√ü - Platz 1 - 5 68165 Mannheim Telefon: 0621 / 4592215 Telefax: 0621 / 4592221 Erich Schmitt Mitglied des Vorstandes Audi AG Ressort Einkauf, Finanz und Organisation 85045 Ingolstadt Telefon: 0841 / 896300 Telefax: 0841 / 894711 Peter Schmitt SKF GmbH Gunnar - Wester - Stra√üe. 12 97421 Schweinfurt Telefon: 09721 / 56 - 0 Telefax: 09721 / 566000 Klaus Schweickart Vorsitzender des Vorstandes Altana Industrie - Aktien und Anlagen AG Seedammweg 55, 61352 Bad Homburg Telefon: 06172 / 404 - 0 Telefax: 06172 / 404401 Wolfgang Seuthe Direktor K√ºhne & Nagel AG & Co. Herrengraben1, 20459 Hamburg Telefon: 040 / 37606118 Telefax: 040 / 37606200 Hans - Peter Siler Abteilungsdirektor Bayernwerk AG Nymphenburger Stra√üe 39 80335 M√ºnchen Telefon: 089 / 12543525 Telefax: 089 / 12543906 Kurt St√§hlern Mitglied des Vorstandes Preussag AG Postfach 4827 30048 Hannover Telefon: 05341 / 213120 Telefax: 05341 / 212281 Wolfram Staiger Gesch√§ftsf√ºhrender Marketing IBM Deutschland GmbH Pascalstrasse 100, 70569 Stuttgart Telefon: 0711 / 785 - 4201 Telefax: 0711 / 785 - 4119 Dr. Wolfgang Stra√üburg Direktor RWE AG Kruppstrasse5, 45128 Essen Telefon: 0201 / 1853140 Telefax: 0201 / 1852782 Martin Swoboda Leiter Unternehmensplanung Continental AG Vahrenwalder Strasse9 30165 Hannover Telefon: 0511 / 9381125 Telefax: 0511 / 9382766 Dr. Horst Teltschik Mitglied des Vorstandes BMW AG Postfach 400240 80702 M√ºnchen Telefon: 089 / 38956800 / 1 Telefax: 089 / 38956008 Hans Tempel Mitglied des Vorstandes Schichau Seebeckwerft AG Riedemannstrasse1 27572 Bremerhaven Telefon: 0471 / 392578 Telefax: 0471 / 392214 Peter Uhlig Bereichsleiter Materialwirtschaft Reemtsma Cigarettenfabrik GmbH Parkstra√üe 51, 22605 Hamburg Telefon: 040 / 8220 - 1595 Telefax: 040 / 8220 - 1333 Klaus Vetter Gesch√§ftsf√ºhrender Vetter F√∂rdertechnik GmbH Postfach 310343 57046 Siegen Telefon: 0271 / 3502 - 0 Telefax: 0271 / 350286 Eugen Vichof Vorsitzender des Beirates Allkauf SB - Warenhaus GmbH & Co. KG Reyersh√ºtte 51 41065 M√∂nchengladbach Telefon: 02161 / 4030 Telefax: 02161 / 403406 Dr. Udo Wagner Mitglied des Vorstandes ABB Asea Brown Boveri AG Gottlieb - Daimler - Strasse8 68165 Mannheim Telefon: 0621 / 4381302 Telefax: 0621 / 4381281 Ulrich Weber Leiter des Einkaufs N√ºrnberger Bund Gro√üeinkauf e.G. Sch√ºrmannstrasse 30 45136 Essen Telefon: 0201 / 18930 Telefax: 0201 / 1893297 Prof. Dr. Dietmar Werner Mitglied des Vorstandes BASF AG Carl - Bosch - Stra√üe 38 67063 Ludwigshafen Telefon: 0621 / 6041919 Telefax: 0621 / 6071259 Peter Prinz Wittgenstein Mitglied des Vorstandes Mannesmann AG Postfach 103641 40027 D√ºsseldorf Telefon: 0211 / 8202684 Telefax: 0211 / 8202116 Dr. - Ing. Hans - Joachim Wolff Sprecher des Vorstandes Dyckerhoff & Widmann AG Erdinger Strasse1, 81829 M√ºnchen Telefon: 089 / 9255 - 3512 Telefax: 089 / 9255 - 3443 Martin -√§pfel Mitglied des Vorstandes Otto Versand Wandsbeker Stra√üe 3 - 7 22179 Hamburg Telefon: 040 / 64618775 Telefax: 040 / 64611188 Michael Ziesler Mitglied des Vorstandes Saarbergwerke AG Trierer Str. 1, 66111 Saarbr√ºcken Telefon: 0681 / 4051 Telefax: 0681 / 4054205 Ulrich Zimmermann Gesch√§ftsf√ºhrender Luwa GmbH Hana√ºr Landstra√üe 200 60314 Frankfurt Telefon: 069 / 40350 Telefax: 069 / 4035307'"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# inspect example article with high share of numbers\n",
    "data[data.numeric == True]['texts'].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'WAHLERGEBNISSE / In einigen L√§ndern drastische Denkzettel f√ºr die regierenden Kr√§fte. Verschiebungen in der Parteienlandschaft. HB D√úSSELDORF. Die Europawahlen brachten deutliche Verschiebungen bei den Stimmenanteilen gegen√ºber 1989. Die Wahlbeteiligung lag bei 57% (1989: 58,5%). Deutschland: Wahlbeteiligung 60,1%. 99 Sitze. CDU / CSU 38,8% (47 Sitze, 1989: 31 Sitze), SPD 32,2% (40 Sitze, 1989: 30 Sitze), B√ºndnis 90 / Die Gr√ºnen 10,1% (12 Sitze, 1989: sieben), FDP 4,1% (keinen Sitz), Republikaner 3,9% (keinen Sitz). Vorl√§ufiges amtliches Endergebnis. Frankreich: Wahlbeteiligung: 53,5%. 87 Sitze. Neogaullisten und liberale UdF 25,5% (29 Sitze, 1989: 33 Sitze), Sozialisten 14,5% (16 Sitze, 1989: 22), Maastricht - Gegner 12,4% (13 Sitze, 1989: keinen), Nationale Front 10,6% (zehn Sitze, 1989: zehn), Kommunisten 6,9% (sechs Sitze, 1989: sieben). Vorl√§ufiges amtliches Endergebnis. Gro√übritannien: Wahlbeteiligung 36%. 87 Sitze. Konservative 28% (18 Sitze, 1989: 32), Labour - Partei 45% (62 Sitze, 1989: 45) Liberaldemokraten (zwei Sitze, 1989: keine). Vorl√§ufiges Endergebnis. Italien: Wahlbeteiligung 74,8%. 87 Sitze. Forza Italia 30,6% (27 Sitze, 1989: keine), Nationale Allianz 12,5% (elf Sitze, 1989: vier), Liga Nord 6,6% (sechs Sitze, 1989: zwei), PDS 19,1% (16 Sitze, 1989: 22), Volkspartei 10% (neun Sitze, 1989: 27), Kommunisten 6,1% (f√ºnf Sitze, 1989: keinen), Gr√ºne 3,2% (drei Sitze, 1989: f√ºnf). Vorl√§ufiges Endergebnis. Spanien: Wahlbeteiligung 59,6%. 64 Sitze. Sozialisten 30,7% (22 Sitze, 1989: 27), Volkspartei 40,2% (28 Sitze, 1989: 15), Vereinigte Linke 13,5% (neun Sitze, 1989: vier). Vorl√§ufiges Endergebnis. D√§nemark: 16 Sitze. Wahlbeteiligung 52,5%. Sozialdemokraten 15,8% (drei Sitze, 1989: zwei), Konservative 17,7% (drei Sitze, 1989: vier), Rechtsliberale 18,9% (vier Sitze, 1989: f√ºnf). Endergebnis. Niederlande: 31 Sitze. Wahlbeteiligung 35,6%. Christdemokraten 30,8% (10 Sitze), Sozialdemokraten 22,9% (acht Sitze), Rechtsliberale 17,9% (sechs Sitze), Linksliberale 5,8% (vier Sitze), Kalvinisten 7,8% (zwei Sitze), Gr√ºne 3,7% (ein Sitz). Vorl√§ufiges Ergebnis. Belgien: 25 Sitze. Wahlbeteiligung 83,3%. Sozialisten 22,4% (sechs Sitze, 1989: acht), Christdemokraten 24,3% (sieben Sitze wie 1989), Liberale 20,4% (sechs Sitze, 1989: vier), Gr√ºne 16% (drei Sitze, 1989: vier), Rechte 11,4% (drei Sitze, 1989: einer). Vorl√§ufiges Endergebnis. Portugal: 25 Sitze. Wahlbeteiligung 35,7%. Sozialistische Partei (PS) 34,8% (neun Sitze, 1989: acht), Sozialdemokraten (PSD) 34,4% (neun Sitze, 1989: acht), Christdemokraten 12,4% (drei Sitze, 1989: drei), Kommunisten 11,2%, (drei Sitze, 1989: vier). Vorl√§ufiges Endergebnis. Griechenland: 25 Sitze. Sozialisten / Pasok 32,7% (10 Sitze), Nationalisten 8,7% (zwei Sitze), Kommunisten 6,3% (zwei Sitze), NEA / Konservative 32,7% (acht Sitze). Hochrechnung. Irland: 15 Sitze. Fianna Fail 30,7% (sieben Sitze, 1989: sechs), Labour 11,5% (ein Sitz, unver√§ndert), Fine G√§b 26,5% (zwei Sitze, 1989: vier), Gr√ºne 9,6% (ein Sitz, 1989: keiner). Hochrechnung.'"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[data.numeric == True]['texts'].iloc[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data[data.numeric == True])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "578154\n"
     ]
    }
   ],
   "source": [
    "# drop articles predominantly consisting of numbers\n",
    "data = data[data.numeric == False]\n",
    "del data['numeric']\n",
    "data.reset_index(inplace=True, drop=True)\n",
    "# the number of articles after removing articles that predominantly consist of numbers\n",
    "print(len(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Identify articles that predominantly consist of names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We eliminate texts with a name density of at least 15% (relative to the total word count, excluding numbers) as part of our pre-processing pipeline. This exclusion is important to guarantee that the remaining articles contain sufficient content for effective topic analysis, as the removal of common German names is a standard pre-processing step in LDA model estimation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = os.getcwd().replace('\\\\Handelsblatt', '') + '\\\\SZ'\n",
    "\n",
    "# Load the dictionary containing common German first and last names\n",
    "with open(path + \"\\\\names.txt\", \"r\", encoding=\"utf-8-sig\") as f:\n",
    "    names_list = f.read().splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:01:15.355966\n"
     ]
    }
   ],
   "source": [
    "sys.path.insert(1, os.getcwd().replace('Handelsblatt', 'SZ'))\n",
    "import count_names\n",
    "\n",
    "inputs = zip(data['texts'], data['word_count'], itertools.repeat(names_list))\n",
    "\n",
    "startTime = datetime.now()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    pool = mp.Pool(NUM_CORE)\n",
    "    names_result = pool.starmap(count_names.count_names, inputs)\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "\n",
    "print(datetime.now()-startTime)\n",
    "\n",
    "data['names'] = names_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\" wir. \" . steht als Berufsverband Unternehmen der deutschen Wirtschaft unabh√§ngig von der Unternehmensgr√∂√üe und der Rechtsform f√ºr eine Mitgliedschaft offen. wir. - Vereinsrechtl. Vorstand: Dr. - Ing. E.h. Eberhard v. K√ºnheim (Vors.) Vorsitzender des AR der BMW AG Dr. Wolfgang S√∂ller Vorsitzender des AR Dresdner Bank AG Dr. Ludolf v. Wartenberg Hauptgesch√§ftsf√ºhrer und Mitglied des Pr√§sidiums des BDI e.V. Mitglieder Dr. Henning Schulte - H√∂lle, Allianz Holding AG Dr. Eberhard v. K√∂rner, Asea Brown Boveri AG Dr. J√ºrgen Strube, BASF AG Dr. Manfred Schneider, Bayer AG Dr. Eberhard Martini, Bayerische Hypotheken - u. Wechsel - Bank AG Dr. Albrecht Schmidt, Bayerische Vereinsbank AG W. Steinriede, Berliner Bank AG Dr. Mark W√∂ssner, Unternehmensgruppe Bertelsmann AG Dr. Christian Roth, Bilfinger & Berger Bau AG Dr. Eberh. v. K√ºnheim, BMW AG Dr. Ludolf v. Wartenberg, Bundesverband d. Deutschen Industrie e.V. Dr. Manfred Weber, Bundesverband deutscher Banken e.V. Dr. Hubert Burda, Burda GmbH Martin K√ºhlhaussen, Commerzbank AG Edzard Reuter, Daimler Benz AG Hilmar Kopper, Deutsche Bank AG Dr. Franz Schoser, Deutscher Industrie - und Handelstag Dr. Wolfgang S√∂ller, Dresdner Bank AG Adolf Kracht, Gerling - Konzern Versicherungs - Beteiligungs - AG Dr. Wolfgang Peiner, Goth√§r Versicherungen Dr. Tyll Necker, Hako - Werke GmbH & Co. Dr. Hans Dietrich Winkhaus, Henkel KGaA Wolf - Dieter Baumgartl, HDI Haftpflichtverband der Deutschen Industrie Dr. J. Odewald, Kaufhof Holding AG Dr. H. - Christoph v. Rohr, Kl√∂ckner - Werke AG Dr. G. Cromme, Fried. Krupp AG Dr. Hans Meinhardt, Linde AG Dr. Kajo Neukirchen, Metallgesellschaft AG Dr. Frank Niethammer, Niethammer Unternehmens - Beteiligungs GmbH Dr. Arend Oetker, Dr. Arend Oetker Holding GmbH & Co. Dr. Michael Otto, Otto - Versand Dr. Klaus Liesen, Ruhrgas AG Dr. Klaus Murmann, Sauer Getriebe AG Dr. Dieter Murmann, J.P. Sauer & Sohn Maschinenbau GmbH & Co. Dr. Guiseppe Vita, Schering AG Prof. G. Zeidler, Alcatel SEL AG Dr. Heinrich v. Pierer, Siemens AG Dr. Volker Hauff, Axel Springer Verlag AG Dr. Martin Willich, Studio Hamburg Atelier GmbH Helmut Ricke, Deutsche Bundespost Telekom Erivan Karl Haub, Tengelmann Warenhandelsgesellschaft Ulrich Hartmann, Veba AG Dr. Ferd. Piech, Volkswagen AG Alphons Horten, Horten AG wir. - Gesch√§ftsf√ºhrung: wir. Wirtschafts - Initiativen f√ºr Deutschland e.V. Dr. J√ºrgen Bauer, Gesch√§ftsf√ºhrender, Uhlandstr. 29, 10719 Berlin, Tel.: 030 / 8823025, Fax: 030 / 8838970'"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example of an article with a high proportion of names\n",
    "data[data.names>=0.15].iloc[0]['texts']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "578127\n"
     ]
    }
   ],
   "source": [
    "# Exclude texts with a name density of at least 15%\n",
    "data = data[data.names<0.15]\n",
    "del data['names']\n",
    "data.reset_index(inplace=True, drop=True)\n",
    "# the number of articles after removing articles that predominantly consist of names\n",
    "print(len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({2000: 33869, 1999: 32788, 2001: 30645, 1996: 27152, 1997: 26929, 1998: 26855, 2004: 26417, 1995: 25944, 2002: 24961, 1994: 24737, 2005: 24702, 2003: 24311, 2006: 24128, 2007: 23795, 2010: 23771, 2008: 23498, 2009: 23309, 2011: 22175, 2012: 20626, 2013: 17475, 2014: 17390, 2015: 16280, 2016: 13094, 2017: 12144, 2018: 11132})\n"
     ]
    }
   ],
   "source": [
    "counter=collections.Counter(data['year'])\n",
    "print(counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_csv('hb_prepro_final.csv', encoding = 'utf-8-sig', sep = ';')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
