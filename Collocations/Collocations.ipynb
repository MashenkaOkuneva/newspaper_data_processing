{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we identify collocations by using the part-of-speech tagger. As in [Hansen (2018)](https://academic.oup.com/qje/article/133/2/801/4582916), we later use the collocations in the topic model. By collocations we understand two-word and three-word sequences that have a specific meaning (e.g., labour market).\n",
    "\n",
    "We follow the article by [Markus Konrad](https://datascience.blog.wzb.eu/2016/07/13/accurate-part-of-speech-tagging-of-german-texts-with-nltk/) and use supervised classification for POS (part-of-speech) tagger. This means that a tagger is trained with a large [text corpus](https://www.ims.uni-stuttgart.de/forschung/ressourcen/korpora/tiger/).\n",
    "\n",
    "Here are the steps we take to train the tagger (please see the article by Markus Konrad for a detailed explanation):\n",
    "\n",
    "Step 1: donwload the TIGER corpus in CONLL09 format 'tigercorpus-2.2.conll09.tar.gz' [here](https://www.ims.uni-stuttgart.de/documents/ressourcen/korpora/tiger-corpus/download/start.html), unzip it and save it to your working directory.\n",
    "\n",
    "Step 2: read the corpus with NLTK library:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "corp = nltk.corpus.ConllCorpusReader('.', 'tiger_release_aug07.corrected.16012013.conll09',\n",
    "                                    ['ignore', 'words', 'ignore', 'ignore', 'pos'],\n",
    "                                    encoding = 'utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 3: Use the Python class ClassifierBasedGermanTagger to determine POS. This tagger inspects words for prefixes, suffixes, and other attributes and also takes the sequence of words into account. Download the folder 'ClassifierBasedGermanTagger' [here](https://github.com/ptnplanet/NLTK-Contributions) and save it to your working directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Das', u'ART'),\n",
       " ('ist', u'VAFIN'),\n",
       " ('ein', u'ART'),\n",
       " ('einfacher', u'ADJA'),\n",
       " ('Test', u'NN'),\n",
       " ('dritte', u'ADJA')]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import the tagger:\n",
    "from ClassifierBasedGermanTagger.ClassifierBasedGermanTagger import ClassifierBasedGermanTagger\n",
    "\n",
    "# load the sentences from the corpus:\n",
    "tagged_sents = [sentence for sentence in corp.tagged_sents()]\n",
    "\n",
    "# train the tagger with the complete corpus (the accuracy is around 96%)\n",
    "tagger = ClassifierBasedGermanTagger(train=tagged_sents)\n",
    "\n",
    "# determine POS of a word in a sentence:\n",
    "tagger.tag(['Das', 'ist', 'ein', 'einfacher', 'Test', 'dritte'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The part of speech tagset for the Tiger corpus can be found on the page 12 [here](https://www.google.com/url?sa=t&rct=j&q=&esrc=s&source=web&cd=&cad=rja&uact=8&ved=2ahUKEwj_1-L0q6j5AhUFOHoKHZQlBRoQFnoECAYQAQ&url=https%3A%2F%2Fwww.ims.uni-stuttgart.de%2Fdocuments%2Fressourcen%2Fkorpora%2Ftiger-corpus%2Fannotation%2Ftiger_introduction.pdf&usg=AOvVaw2jtxYYF9U_nY7Lt-c-ZbzL).\n",
    "\n",
    "Step 4: Save the whole tagger object to disk using **pickle**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('nltk_pos.pickle', 'wb') as f:\n",
    "    pickle.dump(tagger, f, protocol=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we identify collocations based on the Handelsblatt data. We use only one newspaper for this task to reduce computational cost. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data = pd.read_csv('E:\\\\Userhome\\\\mokuneva\\\\newspaper_data_processing\\\\Handelsblatt\\\\hb_prepro_final.csv', encoding = 'utf-8-sig', sep=';', index_col = 0, dtype = {'kicker': 'str',\n",
    "                                                                                                 'page': 'str',\n",
    "                                                                                                 'series_title': 'str',\n",
    "                                                                                                 'rubrics': 'str'})\n",
    "data.page = data.page.fillna('')\n",
    "data.series_title = data.series_title.fillna('')\n",
    "data.kicker = data.kicker.fillna('')\n",
    "data.rubrics = data.rubrics.fillna('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We tag every word in the news articles using the tagger trained above. By collocations we understnad two-word (three-word) sequences that satisfy POS patterns proposed by [Lang, Schneider, and Suchowolec (2016)](https://www.google.com/url?sa=t&rct=j&q=&esrc=s&source=web&cd=&cad=rja&uact=8&ved=2ahUKEwjGi66hu6j5AhUBVPEDHRVuDf0QFnoECAcQAQ&url=https%3A%2F%2Fheiup.uni-heidelberg.de%2Freader%2Fdownload%2F361%2F361-69-81161-1-10-20180515.pdf&usg=AOvVaw2-EKkmc6ZA62Re2L3d0SwS) and whose frequency is above 100 (50).\n",
    "\n",
    "Considered patterns are AN, NN, N Prep N, N Det N, A A N, where A stands for adjectives, N for nouns, Prep for prepositions, and Det for determiners."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n",
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n",
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Download the nltk_data and save it to the folder Collocations.\n",
    "nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "startTime = datetime.now() # track time\n",
    "\n",
    "import multiprocessing as mp\n",
    "NUM_CORE = 4\n",
    "\n",
    "import bitridic_german\n",
    "import bigrams_trigrams_multi\n",
    "import most_freq\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    list_of_texts = data.texts\n",
    "    # Split each text into sentences.\n",
    "    list_of_objects = [bitridic_german.BiTriDic(i) for i in list_of_texts]\n",
    "    \n",
    "    pool = mp.Pool(NUM_CORE)\n",
    "    # Tag each word in the news article, create a list of bigrams and trigrams satisfying the POS patterns\n",
    "    # proposed by Lang, Schneider, and Suchowolec (2016).\n",
    "    list_of_bigrams_trigrams = pool.map(bigrams_trigrams_multi.worker_bigr_trigr, ((obj) for obj in list_of_objects))\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "\n",
    "# A list of bigrams based on all articles.\n",
    "list_of_bigrams = [item[0][0] for item in list_of_bigrams_trigrams]\n",
    "# A list of trigrams based on all articles.\n",
    "list_of_trigrams = [item[1][0] for item in list_of_bigrams_trigrams]\n",
    "\n",
    "# Two-word collocations whose frequency is above 100.\n",
    "most_freq_bigrams = most_freq.most_freq(list_of_bigrams, 'bigrams', 100)\n",
    "# Three-word collocations whose frequency is above 50.\n",
    "most_freq_trigrams = most_freq.most_freq(list_of_trigrams, 'trigrams', 50)\n",
    "\n",
    "print(datetime.now()-startTime)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
