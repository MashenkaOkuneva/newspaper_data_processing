{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we identify collocations by using the part-of-speech tagger. As in [Hansen (2018)](https://academic.oup.com/qje/article/133/2/801/4582916), we later use the collocations in the topic model. By collocations we understand two-word and three-word sequences that have a specific meaning (e.g., labour market).\n",
    "\n",
    "We follow the article by [Markus Konrad](https://datascience.blog.wzb.eu/2016/07/13/accurate-part-of-speech-tagging-of-german-texts-with-nltk/) and use supervised classification for POS (part-of-speech) tagger. This means that a tagger is trained with a large [text corpus](https://www.ims.uni-stuttgart.de/forschung/ressourcen/korpora/tiger/).\n",
    "\n",
    "Here are the steps we take to train the tagger (please see the article by Markus Konrad for a detailed explanation):\n",
    "\n",
    "Step 1: donwload the TIGER corpus in CONLL09 format 'tigercorpus-2.2.conll09.tar.gz' [here](https://www.ims.uni-stuttgart.de/documents/ressourcen/korpora/tiger-corpus/download/start.html), unzip it and save it to your working directory.\n",
    "\n",
    "Step 2: read the corpus with NLTK library:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "corp = nltk.corpus.ConllCorpusReader('.', 'tiger_release_aug07.corrected.16012013.conll09',\n",
    "                                    ['ignore', 'words', 'ignore', 'ignore', 'pos'],\n",
    "                                    encoding = 'utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 3: Use the Python class ClassifierBasedGermanTagger to determine POS. This tagger inspects words for prefixes, suffixes, and other attributes and also takes the sequence of words into account. Download the folder 'ClassifierBasedGermanTagger' [here](https://github.com/ptnplanet/NLTK-Contributions) and save it to your working directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Das', u'ART'),\n",
       " ('ist', u'VAFIN'),\n",
       " ('ein', u'ART'),\n",
       " ('einfacher', u'ADJA'),\n",
       " ('Test', u'NN'),\n",
       " ('dritte', u'ADJA')]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import the tagger:\n",
    "from ClassifierBasedGermanTagger.ClassifierBasedGermanTagger import ClassifierBasedGermanTagger\n",
    "\n",
    "# load the sentences from the corpus:\n",
    "tagged_sents = [sentence for sentence in corp.tagged_sents()]\n",
    "\n",
    "# train the tagger with the complete corpus (the accuracy is around 96%)\n",
    "tagger = ClassifierBasedGermanTagger(train=tagged_sents)\n",
    "\n",
    "# determine POS of a word in a sentence:\n",
    "tagger.tag(['Das', 'ist', 'ein', 'einfacher', 'Test', 'dritte'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The part of speech tagset for the Tiger corpus can be found on the page 12 [here](https://www.google.com/url?sa=t&rct=j&q=&esrc=s&source=web&cd=&cad=rja&uact=8&ved=2ahUKEwj_1-L0q6j5AhUFOHoKHZQlBRoQFnoECAYQAQ&url=https%3A%2F%2Fwww.ims.uni-stuttgart.de%2Fdocuments%2Fressourcen%2Fkorpora%2Ftiger-corpus%2Fannotation%2Ftiger_introduction.pdf&usg=AOvVaw2jtxYYF9U_nY7Lt-c-ZbzL).\n",
    "\n",
    "Step 4: Save the whole tagger object to disk using **pickle**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('nltk_pos.pickle', 'wb') as f:\n",
    "    pickle.dump(tagger, f, protocol=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we identify collocations based on our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3336299\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>texts</th>\n",
       "      <th>day</th>\n",
       "      <th>month</th>\n",
       "      <th>year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Schalck: Milliardenkredit sicherte Zahlungsfäh...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1991</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Welajati: Iran bleibt bei einem Krieg am Golf ...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1991</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Bush will offenbar seinen Außenminister erneut...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1991</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Sperrfrist 1. Januar 1000 HBV fordert umfassen...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1991</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Schamir weist Nahost-Äußerungen des neuen EG-P...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1991</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               texts  day  month  year\n",
       "0  Schalck: Milliardenkredit sicherte Zahlungsfäh...    1      1  1991\n",
       "1  Welajati: Iran bleibt bei einem Krieg am Golf ...    1      1  1991\n",
       "2  Bush will offenbar seinen Außenminister erneut...    1      1  1991\n",
       "3  Sperrfrist 1. Januar 1000 HBV fordert umfassen...    1      1  1991\n",
       "4  Schamir weist Nahost-Äußerungen des neuen EG-P...    1      1  1991"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from ast import literal_eval\n",
    "\n",
    "# Set the path variable to point to the 'newspaper_data_processing' directory.\n",
    "path = os.getcwd().replace('\\\\Collocations', '')\n",
    "\n",
    "# Load pre-processed 'dpa' dataset from a CSV file.\n",
    "dpa = pd.read_csv(path + '\\\\dpa\\\\' + 'dpa_prepro_final.csv', encoding = 'utf-8', sep=';', index_col = 0,  keep_default_na=False,\n",
    "                   dtype = {'rubrics': 'str', \n",
    "                            'source': 'str',\n",
    "                            'keywords': 'str',\n",
    "                            'title': 'str',\n",
    "                            'city': 'str',\n",
    "                            'genre': 'str',\n",
    "                            'wordcount': 'str'},\n",
    "                  converters = {'paragraphs': literal_eval})\n",
    "\n",
    "# Keep only the article texts and their respective publication dates.\n",
    "dpa = dpa[['texts', 'day', 'month', 'year']]\n",
    "\n",
    "# Load pre-processed 'SZ' dataset from a CSV file.\n",
    "sz = pd.read_csv(path + '\\\\SZ\\\\' + 'sz_prepro_final.csv', encoding = 'utf-8-sig', sep=';', index_col = 0, dtype = {'newspaper': 'str',\n",
    "                                                                                                 'newspaper_2': 'str',\n",
    "                                                                                                 'quelle_texts': 'str',\n",
    "                                                                                                 'page': 'str',\n",
    "                                                                                                 'rubrics': 'str'})\n",
    "sz.page = sz.page.fillna('')\n",
    "sz.newspaper = sz.newspaper.fillna('')\n",
    "sz.newspaper_2 = sz.newspaper_2.fillna('')\n",
    "sz.rubrics = sz.rubrics.fillna('')\n",
    "sz.quelle_texts = sz.quelle_texts.fillna('')\n",
    "\n",
    "# Keep only the article texts and their respective publication dates.\n",
    "sz = sz[['texts', 'day', 'month', 'year']]\n",
    "\n",
    "# Load pre-processed 'Handelsblatt' dataset from a CSV file.\n",
    "hb = pd.read_csv(path + '\\\\Handelsblatt\\\\' + 'hb_prepro_final.csv', encoding = 'utf-8-sig', sep=';', index_col = 0, dtype = {'kicker': 'str',\n",
    "                                                                                                 'page': 'str',\n",
    "                                                                                                 'series_title': 'str',\n",
    "                                                                                                 'rubrics': 'str'})\n",
    "hb.page = hb.page.fillna('')\n",
    "hb.series_title = hb.series_title.fillna('')\n",
    "hb.kicker = hb.kicker.fillna('')\n",
    "hb.rubrics = hb.rubrics.fillna('')\n",
    "\n",
    "# Keep only the article texts and their respective publication dates.\n",
    "hb = hb[['texts', 'day', 'month', 'year']]\n",
    "\n",
    "# Load pre-processed 'Welt' dataset from a CSV file.\n",
    "welt = pd.read_csv(path + '\\\\Welt\\\\' + 'welt_prepro_final.csv', encoding = 'utf-8-sig', sep=';', index_col = 0, dtype = {'newspaper': 'str',\n",
    "                                                                                                 'rubrics': 'str',\n",
    "                                                                                                 'title': 'str'})\n",
    "welt.title = welt.title.fillna('')\n",
    "welt.rubrics = welt.rubrics.fillna('')\n",
    "\n",
    "# Keep only the article texts and their respective publication dates.\n",
    "welt = welt[['texts', 'day', 'month', 'year']]\n",
    "\n",
    "# Concatenate the 'dpa', 'sz', 'hb', and 'welt' DataFrames into a single DataFrame 'data'.\n",
    "data = pd.concat([dpa, sz, hb, welt])\n",
    "\n",
    "# The number of articles in the final dataset.\n",
    "print(len(data))\n",
    "\n",
    "# Sort the data in chronological order.\n",
    "data = data.sort_values(['year', 'month', 'day'], ascending=[True, True, True])\n",
    "# Reset the index of the DataFrame\n",
    "data.reset_index(inplace=True, drop=True)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this project, we limit our training dataset to include only articles published from 1991 through 2009, inclusive. This decision is driven by our later goal of performing an out-of-sample forecast of GDP growth for the years 2010 to 2018. By excluding data from 2010 onward during the model training phase, we ensure that our forecast does not incorporate information that was not yet available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the dataset to include only articles published before 2010.\n",
    "data = data[data.year < 2010]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We tag every word in the news articles using the tagger trained above. We define collocations as two-word or three-word sequences that adhere to the part-of-speech (POS) patterns proposed by [Lang, Schneider, and Suchowolec (2016)](https://www.google.com/url?sa=t&source=web&rct=j&opi=89978449&url=https://heiup.uni-heidelberg.de/catalog/view/361/509/81161&ved=2ahUKEwi4itSg2IqGAxX2evEDHWMbCYcQFnoECBgQAQ&usg=AOvVaw3gvsMJQoJ5J8pNfZZ3FDje). Each collocation must appear in over 100 texts for two-word sequences and over 50 texts for three-word sequences. However, to maintain computational efficiency and focus on the most salient collocations, we only consider the top 10,000 most frequently occurring bigrams and trigrams. This means that even if a certain bigram or trigram appears in more than 100 or 50 texts respectively, it will be overlooked if it is not among the 10,000 most common collocations.\n",
    "\n",
    "Considered patterns are AN, NN, N Prep N, N Det N, A A N, where A stands for adjectives, N for nouns, Prep for prepositions, and Det for determiners."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Download the nltk_data and save it to the folder Collocations.\n",
    "nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 days, 7:55:04.370000\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "startTime = datetime.now() # track time\n",
    "\n",
    "import multiprocessing as mp\n",
    "NUM_CORE = 4\n",
    "\n",
    "import bitridic_german\n",
    "import bigrams_trigrams_multi\n",
    "import most_freq\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    list_of_texts = data.texts\n",
    "    # Split each text into sentences.\n",
    "    list_of_objects = [bitridic_german.BiTriDic(i) for i in list_of_texts]\n",
    "    \n",
    "    pool = mp.Pool(NUM_CORE)\n",
    "    # Tag each word in the news article, create a list of bigrams and trigrams satisfying the POS patterns\n",
    "    # proposed by Lang, Schneider, and Suchowolec (2016).\n",
    "    list_of_bigrams_trigrams = pool.map(bigrams_trigrams_multi.worker_bigr_trigr, ((obj) for obj in list_of_objects))\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "\n",
    "# A list of bigrams based on all articles.\n",
    "list_of_bigrams = [item[0][0] for item in list_of_bigrams_trigrams]\n",
    "# A list of trigrams based on all articles.\n",
    "list_of_trigrams = [item[1][0] for item in list_of_bigrams_trigrams]\n",
    "\n",
    "# Two-word collocations whose frequency is above 100, restricted to the top 10,000 most frequent bigrams.\n",
    "most_freq_bigrams = most_freq.most_freq(list_of_bigrams, 'bigrams', 100)\n",
    "# Three-word collocations whose frequency is above 50, restricted to the top 10,000 most frequent trigrams.\n",
    "most_freq_trigrams = most_freq.most_freq(list_of_trigrams, 'trigrams', 50)\n",
    "\n",
    "print(datetime.now()-startTime)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
