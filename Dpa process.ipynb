{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import os\n",
    "from datetime import datetime\n",
    "import multiprocessing as mp\n",
    "from langdetect import detect\n",
    "import re\n",
    "import numpy as np\n",
    "import worker_xml\n",
    "from itertools import repeat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the number of cores to use\n",
    "NUM_CORE = mp.cpu_count()-2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dpa Data (1991 - 2019)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Folder with unpacked articles\n",
    "path = 'D:\\\\Studium\\\\PhD\\Media Tenor\\\\dpa\\\\dpa_unpacked'\n",
    "folder_list = []\n",
    "\n",
    "# 2 folders\n",
    "for fol in [fol for fol in os.listdir(path)]:\n",
    "\n",
    "    # Within each folder: folders for different years\n",
    "    for f in [f for f in os.listdir(path + '\\\\' + fol)]:\n",
    "        folder_list.append(path + '\\\\' + fol + '\\\\' + f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select a path to the folder for storing results\n",
    "PATH = r'D:\\\\Studium\\\\PhD\\\\Media Tenor'\n",
    "os.chdir(PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the 'worker_xml' function to load articles\n",
    "startTime = datetime.now()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    pool = mp.Pool(NUM_CORE)\n",
    "    df_list = pool.map(worker_xml.worker_xml, folder_list)\n",
    "    data = pd.concat(df_list)\n",
    "    data.reset_index(inplace=True, drop=True)\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "\n",
    "print(datetime.now()-startTime)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out articles with less than 50 words\n",
    "data = data[pd.to_numeric(data['wordcount']) >= 50]\n",
    "data.reset_index(inplace=True, drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "delete_indices = []\n",
    "\n",
    "# Filter out articles that mainly consists of numbers\n",
    "for index, row in data.iterrows():\n",
    "\n",
    "    # Count number of integers and words\n",
    "    count_n = sum(s.isdigit() for s in data.iloc[index]['texts'])\n",
    "    count_w = data.iloc[index]['wordcount']\n",
    "\n",
    "    if count_n/count_w > 1/2:\n",
    "        delete_index.append(index)\n",
    "\n",
    "data.drop(delete_indices, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out insignificant articles based on keywords, genres etc. and reset the index \n",
    "# of the dataframe afterwards\n",
    "fil_titles = '''Edelmetallpreise|Tageskalender|Tabelle|SPORT|SPORT\\:|\n",
    "                Sport|Berichtigung|Sortenkurse|Devisenkurse|Impressum|Testmeldung|\n",
    "                WOCHENVORSCHAU|Kurse A|Kurse B|Kurse C'''\n",
    "data.drop(data[data['title'].str.contains(fil_titles)].index, inplace=True)\n",
    "data.reset_index(inplace=True, drop=True)\n",
    "\n",
    "genre_clean = '''Tabelle|Historisches|Achtung|Sport|SPORT'''\n",
    "data.drop(data[data['genre'].str.contains(genre_clean)].index, inplace=True)\n",
    "data.reset_index(inplace=True, drop=True)\n",
    "\n",
    "keyword_clean = '''Tagesvorschau|Sport|Redaktionshinweis|SUM|DGAP|Sport|SPORT|\n",
    "                     Wochenvorschau|SPO'''\n",
    "data.drop(data[data['keywords'].str.contains(keyword_clean)].index, inplace=True)\n",
    "data.reset_index(inplace=True, drop=True)\n",
    "\n",
    "data.drop(data[data['rubrics'].str.contains('iq')].index, inplace=True)\n",
    "data.reset_index(inplace=True, drop=True)\n",
    "\n",
    "text_clean = '''Schalterverkaufskurse:|dpa-news.de'''\n",
    "data.drop(data[data['texts'].str.contains(text_clean)].index, inplace=True)\n",
    "data.reset_index(inplace=True, drop=True)\n",
    "\n",
    "data.drop(data[data['source'] == 'dpa-frei\\dpa-wahl'].index, inplace=True)\n",
    "data.reset_index(inplace=True, drop=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Delete Duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare inputs\n",
    "inputs_year = []\n",
    "inputs_month = []\n",
    "for year in list(set(data['year'])):\n",
    "    for month in list(set(data['month'])):\n",
    "        inputs_year.append(year)\n",
    "        inputs_month.append(month)\n",
    "\n",
    "inputs = list(zip(inputs_year, inputs_month, repeat(data)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import a function that outputs the indices of duplicates\n",
    "import fuzzy_duplicates\n",
    "\n",
    "startTime = datetime.now()\n",
    "delete_indices = []\n",
    "\n",
    "# Apply function to all combinations of month-year in parallel\n",
    "if __name__ == \"__main__\":\n",
    "    pool = mp.Pool(NUM_CORE)\n",
    "    delete_indices =  pool.map(fuzzy_duplicates.fuzzy_duplicates, inputs)\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "\n",
    "print(datetime.now()-startTime)\n",
     "\n",
    "data.drop(delete_indices, inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split up articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store potential multiple articles separately and delete them from the\n",
    "# original data\n",
    "s_mult_art = '''dpa-Nachrichtenüberblick|Nachrichtenüberblick|\n",
    "                Vorschau|vorschau|VORSCHAU|Tagesvorschau'''\n",
    "mult_art = data[data['title'].str.contains(s_mult_art)]\n",
    "mult_art = mult_art.append(data[data['keywords'].str.contains(s_mult_art)])\n",
    "mult_art = mult_art.append(data[data['genre'].str.contains(s_mult_art)])\n",
    "\n",
    "data.drop(data[data['title'].str.contains(s_mult_art)].index, inplace=True)\n",
    "data.drop(data[data['keywords'].str.contains(s_mult_art)].index, inplace=True)\n",
    "data.drop(data[data['genre'].str.contains(s_mult_art)].index, inplace=True)\n",
    "\n",
    "# Define a function that splits up potential multiple articles into single articles\n",
    "def split_mult_art(multiple_articles):\n",
    "\n",
    "    split_art = []\n",
    "    weekdays = ['Montag', 'Dienstag', 'Mittwoch', 'Donnerstag',\n",
    "                'Freitag', 'Samstag', 'Sonntag']\n",
    "    multiple_articles.reset_index(inplace=True, drop=True)\n",
    "\n",
    "    for index, row in multiple_articles.iterrows():\n",
    "\n",
    "        # Split articles\n",
    "        mult_art = re.split(r'  +', row[\"texts\"])\n",
    "        keep_index = []\n",
    "\n",
    "        # Keep any text wich contains a weekday or is longer than 100 words\n",
    "        # but shorter than 50. The 50 words boundary stems from the filter\n",
    "        # before and should be adapted accordingly\n",
    "        for art in mult_art:\n",
    "            if (any([day in art for day in weekdays]) or\n",
    "                    len(art.split()) >= 100) and len(art.split()) >= 50:\n",
    "                keep_index.append(mult_art.index(art))\n",
    "\n",
    "        keep_art = [mult_art[i] for i in keep_index]\n",
    "\n",
    "        if keep_art != []:\n",
    "            for art in keep_art:\n",
    "                # Store the meta data for the article\n",
    "                seperated_articles = multiple_articles.iloc[index][multiple_articles.iloc[index] != 'texts']\n",
    "                # Assign the new text to the matching meta data\n",
    "                seperated_articles['texts'] = art\n",
    "                split_art.append(seperated_articles)\n",
    "\n",
    "    return(pd.concat(split_art, axis=1).transpose())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Append newly split articles to the orginal data\n",
    "data.append(split_mult_art(mul_art))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Delete English Articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import a function that outputs the indices of englisch articles\n",
    "import identify_eng\n",
    "\n",
    "import itertools\n",
    "\n",
    "# Delete all English articles from the data\n",
    "startTime = datetime.now()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    pool = mp.Pool(NUM_CORE)\n",
    "    split_dfs = np.array_split(data, NUM_CORE)\n",
    "    index_eng = pool.map(identify_eng.identify_eng, split_dfs)\n",
    "    index_eng = list(itertools.chain(*index_eng))\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "\n",
    "print(datetime.now()-startTime)", 
    "\n",
    "data.drop(index_eng, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtering Based on Topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into economic and finance articles\n",
    "afx = data[data['topic'] == 'afx']\n",
    "WiPo = data[data['topic'] == 'WiPo']\n",
    "\n",
    "# Filter out articles based on keywords which are specific to the economic news\n",
    "keyword_clean = '''Kurse|KURSE|kurse|Börse International|Terminbörse|\n",
    "                Finanzmärkte International'''\n",
    "WiPo = WiPo.drop(WiPo[WiPo['keywords'].str.contains(keyword_clean)].index, inplace=True)\n",
    "WiPo.reset_index(inplace=True, drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save final results\n",
    "WiPo.to_csv('dpa_WiPo_cleaned.csv')\n",
    "afx.to_csv('dpa_afx_cleaned.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
