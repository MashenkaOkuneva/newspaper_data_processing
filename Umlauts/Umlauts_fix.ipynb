{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To start working with our data, we had to take one extra pre-processing step. \n",
    "\n",
    "In the news articles from the 90s, the German umlauts (ö,ä,ü,ß) were often replaced with correspondingly 'oe','ae','ue','ss'. As a result, a word 'Nürnberg' is saved in the corpus as u'Nuernberg' or u'N\\xfcrnberg' depending on the publication date. We use a spellchecker PyHunSpell to ensure that all (or almost all) the words have a unique representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import PyHunSpell\n",
    "import hunspell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\envs\\py27\\lib\\site-packages\\IPython\\core\\interactiveshell.py:2714: DtypeWarning: Columns (2) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "# load the data\n",
    "import pandas as pd \n",
    "data_umlaut = pd.read_csv('hb_umlauts_fix.csv', encoding = 'utf-8', sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\envs\\py27\\lib\\site-packages\\sacremoses\\truecase.py:26: UserWarning: You should really be using Python3!!! Tick tock, tick tock, https://pythonclock.org/\n",
      "  \"You should really be using Python3!!! \"\n"
     ]
    }
   ],
   "source": [
    "# Correct text 57565: some words and numbers are merged into a single token\n",
    "import nltk \n",
    "from sacremoses import MosesTokenizer, MosesDetokenizer # detokenizing package\n",
    "md = MosesDetokenizer() \n",
    "import re\n",
    "tokens = nltk.word_tokenize(data_umlaut['texts'][57565])\n",
    "tokens_new = []\n",
    "# Split tokens containing text and numbers into two parts\n",
    "# E.g., \"OEsterreich1624,70\" should be split into \"OEsterreich\" and \"1624,70\"\n",
    "for t in tokens:\n",
    "    match = re.match(r\"([a-z]+)([0-9,]+)\", t, re.I)\n",
    "    if match:\n",
    "        items = match.groups()\n",
    "        tokens_new.append(items[0] + ' ' + items[1])\n",
    "    else:\n",
    "        tokens_new.append(t)\n",
    "data_umlaut.loc[57565,'texts'] = md.detokenize(tokens_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\mokuneva\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# make sure that you have downloaded nltk_data\n",
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1:00:41.123000\n"
     ]
    }
   ],
   "source": [
    "# use a spellchecker (umlautf.py function) to make umlaut replacements where necessary\n",
    "\n",
    "from datetime import datetime\n",
    "startTime = datetime.now()\n",
    "import umlautf\n",
    "\n",
    "import multiprocessing as mp\n",
    "NUM_CORE = mp.cpu_count() # set to the number of cores you want to use\n",
    "\n",
    "import worker_umlaut\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    list_of_texts = data_umlaut.texts\n",
    "    list_of_objects = [umlautf.spell(i) for i in list_of_texts]\n",
    "    \n",
    "    pool = mp.Pool(NUM_CORE)\n",
    "    corrected_umlauts = pool.map(worker_umlaut.worker_umlaut, ((obj) for obj in list_of_objects))\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "        \n",
    "print datetime.now() - startTime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_umlaut['texts'] = corrected_umlauts "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_umlaut.to_csv('hb_umlauts_fixed.csv', encoding='utf-8-sig', sep = ';')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
