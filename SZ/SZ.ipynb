{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import multiprocessing as mp # use multiprocessing module for parallel computing\n",
    "import pandas as pd # load pandas: python data analysis library\n",
    "import sz_load # import a function that loads data from one folder (see sz_load.py file for details)\n",
    "import codecs\n",
    "import numpy as np\n",
    "import collections\n",
    "# import the module replacing incorrectly encoded German umlaut characters with their correct versions\n",
    "import umlauts_correct_sz \n",
    "import correct_url_sz\n",
    "import clean_sz_articles\n",
    "import itertools\n",
    "import clean_tables_sz\n",
    "import count_names\n",
    "\n",
    "from itertools import repeat\n",
    "from datetime import datetime\n",
    "\n",
    "# We import some functions from the Handelsblatt folder\n",
    "import sys\n",
    "sys.path.insert(1, os.getcwd().replace('SZ', 'Handelsblatt'))\n",
    "import count_words_mp # import the function calculating the number of words in a text\n",
    "import split_number_word\n",
    "import chained_articles # import a function that outputs two parts of a chained article and duplicated articles\n",
    "import find_umlaut\n",
    "\n",
    "# We import some functions from the 'dpa' folder\n",
    "sys.path.insert(1, os.getcwd().replace('SZ', 'dpa'))\n",
    "import identify_ger \n",
    "import fuzzy_duplicates_test_all_dpa\n",
    "import fuzzy_duplicates_dpa # import a function that outputs the indices of fuzzy duplicates \n",
    "import numeric_articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of cores that will be used: 56\n"
     ]
    }
   ],
   "source": [
    "NUM_CORE = mp.cpu_count()-8 # set the number of cores to use\n",
    "\n",
    "print(\"The number of cores that will be used: {}\".format(NUM_CORE))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Süddeutsche Zeitung (SZ) data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Süddeutsche Zeitung* is a renowned German daily newspaper with a strong focus on both politics and economy in its national edition. In the first quarter of 2021, the newspaper had a circulation of 304,769 daily copies, as reported by the IVW (Informationsgemeinschaft zur Feststellung der Verbreitung von Werbeträgern), a German organization responsible for assessing media circulation. Due to its comprehensive coverage of economic topics, the Süddeutsche Zeitung is particularly valuable for economic forecasting.\n",
    "\n",
    "We acquired the SZ dataset from Genios, a reputable German provider of business information. The dataset comprises **2,034,968** articles published between January 1994 and November 2018. The data acquisition took place in February 2019.\n",
    "\n",
    "The dataset is organized into three primary folders: 'Sueddeutsche', 'Sueddetusche_historisch', and 'Sueddeutsche_regional'. For our research project, which aims to forecast economic variables using newspaper data, we decided to focus on the first two folders, as regional data is considered less relevant to our objectives. The 'Sueddeutsche_historisch' folder holds 11 subfolders for the years 1994 to 2004, such as SZH_1994, while the 'Sueddeutsche' folder contains 14 subfolders for each year between 2005 and 2018, like SZ_2005. In total, there are 25 subfolders, one for each year. Each subfolder contains several XML files, from which we extract relevant information for our project. Regrettably, due to copyright restrictions, we are unable to publish the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we need to read in the data. We create the list including the names of the 25 subfolders (`folder_list`) and apply the function `sz_load` to them in parallel by exploiting Python's `multiprocessing` library."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We extract the following XML elements:\n",
    "\n",
    "* datum - publication date\n",
    "* ressort - section/subsection of the newspaper\n",
    "* quelle/name - description of the source, useful for filtering out regional articles\n",
    "* titel-liste/titel - article's title\n",
    "* titel-liste/dachzeile - article's kicker\n",
    "* titel-liste/untertitel - article's subheading\n",
    "* inhalt/vorspann - annotation\n",
    "* inhalt/text - text of the article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'SZ' is the main folder with 3 subfolders ('Sueddeutsche', 'Sueddetusche_historisch', and 'Sueddeutsche_regional') in it\n",
    "#path = 'G:\\\\Userhome Mariia\\\\SZ' # your path here\n",
    "path = os.getcwd().replace('\\\\newspaper_data_processing\\\\SZ', '') + '\\\\SZ'\n",
    "\n",
    "# Create the list of all subfolders within SZ main folder.\n",
    "folder_list=[]\n",
    "\n",
    "for fol in [fol for fol in os.listdir(path)[:2] ]: # 3 folders: SZ, SZ_historisch, SZ_regional (we use the first two)\n",
    "    # os.listdir(path) - names of directories\n",
    "    for f in [f for f in os.listdir(path + '\\\\' + fol) ]:  # within each folder: folders for different years                                          \n",
    "        folder_list.append(path + '\\\\' + fol + '\\\\' + f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:02:34.441543\n"
     ]
    }
   ],
   "source": [
    "startTime = datetime.now() # track time\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    pool = mp.Pool(NUM_CORE)\n",
    "    data_intermediate = pool.map(sz_load.sz_load, folder_list) # load data from each folder in parallel\n",
    "    data = pd.concat(data_intermediate) # concatenate DataFrames from different folders\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "    \n",
    "print(datetime.now()-startTime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.sort_values(['year', 'month', 'day'], ascending=[True, True, True]) # sort the data in chronological order\n",
    "data = data.reset_index() # reset the index of the DataFrame\n",
    "del data['index'] # delete a column with an old index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>day</th>\n",
       "      <th>newspaper</th>\n",
       "      <th>newspaper_2</th>\n",
       "      <th>texts</th>\n",
       "      <th>rubrics</th>\n",
       "      <th>title</th>\n",
       "      <th>quelle_texts</th>\n",
       "      <th>page</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1994</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>- Süddeutsche Zeitung (1994 - 2004)</td>\n",
       "      <td>SZ</td>\n",
       "      <td>Kein Automatismus bei den Miet-Zusatzkosten Wa...</td>\n",
       "      <td>IMMOBILIEN</td>\n",
       "      <td>Kein Automatismus bei den Miet-Zusatzkosten Wa...</td>\n",
       "      <td>SZ NR. 001 VOM 03.01.1994 SEITE 271</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1994</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>- Süddeutsche Zeitung (1994 - 2004)</td>\n",
       "      <td>SZ</td>\n",
       "      <td>STUETZE DER KONJUNKTUR: Waehrend viele Wirtsch...</td>\n",
       "      <td>IMMOBILIEN</td>\n",
       "      <td>STUETZE DER KONJUNKTUR:</td>\n",
       "      <td>SZ NR. 001 VOM 03.01.1994 SEITE 271</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1994</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>- Süddeutsche Zeitung (1994 - 2004)</td>\n",
       "      <td>SZ</td>\n",
       "      <td>In Berlin/Brandenburg: Bau wartet auf den Boom...</td>\n",
       "      <td>IMMOBILIEN</td>\n",
       "      <td>In Berlin/Brandenburg: Bau wartet auf den Boom.</td>\n",
       "      <td>SZ NR. 001 VOM 03.01.1994 SEITE 271</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1994</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>- Süddeutsche Zeitung (1994 - 2004)</td>\n",
       "      <td>SZ</td>\n",
       "      <td>Bueromarkt Berlin Die Preise kommen den Mieter...</td>\n",
       "      <td>IMMOBILIEN</td>\n",
       "      <td>Bueromarkt Berlin Die Preise kommen den Mieter...</td>\n",
       "      <td>SZ NR. 001 VOM 03.01.1994 SEITE 271</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1994</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>- Süddeutsche Zeitung (1994 - 2004)</td>\n",
       "      <td>SZ</td>\n",
       "      <td>Maureen; irische Geschichten. 36 Pubbesitzer, ...</td>\n",
       "      <td>Roman</td>\n",
       "      <td>Maureen; irische Geschichten.</td>\n",
       "      <td>SZ NR. 001 VOM 03.01.1994 SEITE 400</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    year  month  day                             newspaper newspaper_2  \\\n",
       "10  1994      1    3   - Süddeutsche Zeitung (1994 - 2004)          SZ   \n",
       "11  1994      1    3   - Süddeutsche Zeitung (1994 - 2004)          SZ   \n",
       "12  1994      1    3   - Süddeutsche Zeitung (1994 - 2004)          SZ   \n",
       "13  1994      1    3   - Süddeutsche Zeitung (1994 - 2004)          SZ   \n",
       "14  1994      1    3   - Süddeutsche Zeitung (1994 - 2004)          SZ   \n",
       "\n",
       "                                                texts     rubrics  \\\n",
       "10  Kein Automatismus bei den Miet-Zusatzkosten Wa...  IMMOBILIEN   \n",
       "11  STUETZE DER KONJUNKTUR: Waehrend viele Wirtsch...  IMMOBILIEN   \n",
       "12  In Berlin/Brandenburg: Bau wartet auf den Boom...  IMMOBILIEN   \n",
       "13  Bueromarkt Berlin Die Preise kommen den Mieter...  IMMOBILIEN   \n",
       "14  Maureen; irische Geschichten. 36 Pubbesitzer, ...       Roman   \n",
       "\n",
       "                                                title  \\\n",
       "10  Kein Automatismus bei den Miet-Zusatzkosten Wa...   \n",
       "11                            STUETZE DER KONJUNKTUR:   \n",
       "12    In Berlin/Brandenburg: Bau wartet auf den Boom.   \n",
       "13  Bueromarkt Berlin Die Preise kommen den Mieter...   \n",
       "14                      Maureen; irische Geschichten.   \n",
       "\n",
       "                           quelle_texts page  \n",
       "10  SZ NR. 001 VOM 03.01.1994 SEITE 271       \n",
       "11  SZ NR. 001 VOM 03.01.1994 SEITE 271       \n",
       "12  SZ NR. 001 VOM 03.01.1994 SEITE 271       \n",
       "13  SZ NR. 001 VOM 03.01.1994 SEITE 271       \n",
       "14  SZ NR. 001 VOM 03.01.1994 SEITE 400       "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[10:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2034968"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the number of article before pre-processing\n",
    "len(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Light pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove short articles (<100 words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `count_words_mp` function is used to count the number of words in a text. This function only counts words and excludes numbers from the analysis. It takes into consideration Latin letters with any diacritics, such as umlauts. The function removes specific punctuation marks and non-alphabetic characters before counting the words, ensuring that the final count is accurate and relevant for sentiment analysis and topic modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:00:41.796058\n"
     ]
    }
   ],
   "source": [
    "startTime = datetime.now() # track time\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    pool = mp.Pool(NUM_CORE)\n",
    "    count_results = pool.map(count_words_mp.count_words_mp, [text for text in data['texts']]) \n",
    "    pool.close()\n",
    "    pool.join()\n",
    "    \n",
    "print(datetime.now()-startTime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the result as a new column \"word_count\"\n",
    "data['word_count'] = count_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Shorter texts often lack sufficient semantic information, making it challenging for topic models and bag-of-words-based sentiment analysis tools to perform effectively. As a result, we focus on texts with more than 100 words to ensure better performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove articles with the number of words<100\n",
    "data = data[data['word_count']>=100]\n",
    "data = data.sort_values(['year', 'month', 'day'], ascending=[True, True, True]) # sort the data in chronological order\n",
    "data.reset_index(inplace=True, drop=True) # reset the index of the DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1516777"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the number of articles after removing short articles\n",
    "len(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove exact duplicates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removing duplicates is crucial because they do not contribute any new information and only introduce noise into the analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All the duplicated articles are saved as 'sz_duplicates' for further exploration.\n",
    "sz_duplicates = data[data['texts'].duplicated(keep = False)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our corpus, there are two main types of duplicates:\n",
    "\n",
    "1. The same article appears twice, published on different pages because SZ is a regional newspaper, and an identical article can be published on various pages for different regions, such as page 27 in Munich and page 28 in Ebersberg."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>day</th>\n",
       "      <th>newspaper</th>\n",
       "      <th>newspaper_2</th>\n",
       "      <th>texts</th>\n",
       "      <th>rubrics</th>\n",
       "      <th>title</th>\n",
       "      <th>quelle_texts</th>\n",
       "      <th>page</th>\n",
       "      <th>word_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>1994</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>- Süddeutsche Zeitung (1994 - 2004)</td>\n",
       "      <td>SZ</td>\n",
       "      <td>Bittere Klage des OB an Finanzminister Waigel ...</td>\n",
       "      <td>Muenchen</td>\n",
       "      <td>Bittere Klage des OB an Finanzminister Waigel ...</td>\n",
       "      <td>SZ NR. 001 VOM 03.01.1994 SEITE f28</td>\n",
       "      <td></td>\n",
       "      <td>236</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>1994</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>- Süddeutsche Zeitung (1994 - 2004)</td>\n",
       "      <td>SZ</td>\n",
       "      <td>Bittere Klage des OB an Finanzminister Waigel ...</td>\n",
       "      <td>Muenchen</td>\n",
       "      <td>Bittere Klage des OB an Finanzminister Waigel ...</td>\n",
       "      <td>SZ NR. 001 VOM 03.01.1994 SEITE b33</td>\n",
       "      <td></td>\n",
       "      <td>236</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    year  month  day                             newspaper newspaper_2  \\\n",
       "51  1994      1    3   - Süddeutsche Zeitung (1994 - 2004)          SZ   \n",
       "96  1994      1    3   - Süddeutsche Zeitung (1994 - 2004)          SZ   \n",
       "\n",
       "                                                texts   rubrics  \\\n",
       "51  Bittere Klage des OB an Finanzminister Waigel ...  Muenchen   \n",
       "96  Bittere Klage des OB an Finanzminister Waigel ...  Muenchen   \n",
       "\n",
       "                                                title  \\\n",
       "51  Bittere Klage des OB an Finanzminister Waigel ...   \n",
       "96  Bittere Klage des OB an Finanzminister Waigel ...   \n",
       "\n",
       "                           quelle_texts page  word_count  \n",
       "51  SZ NR. 001 VOM 03.01.1994 SEITE f28              236  \n",
       "96  SZ NR. 001 VOM 03.01.1994 SEITE b33              236  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sz_duplicates.loc[[51,96]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. The same article is included in the corpus with different publication dates (e.g., 12.10.2018 and 13.10.2018). In such cases, a sensible approach is to retain the first entry."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>day</th>\n",
       "      <th>newspaper</th>\n",
       "      <th>newspaper_2</th>\n",
       "      <th>texts</th>\n",
       "      <th>rubrics</th>\n",
       "      <th>title</th>\n",
       "      <th>quelle_texts</th>\n",
       "      <th>page</th>\n",
       "      <th>word_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1510995</th>\n",
       "      <td>2018</td>\n",
       "      <td>10</td>\n",
       "      <td>12</td>\n",
       "      <td>Süddeutsche Zeitung</td>\n",
       "      <td>SZ</td>\n",
       "      <td>Impressum. HERAUSGEGEBEN VOM SÜDDEUTSCHEN VERL...</td>\n",
       "      <td>Ressort: Meinungsseite      Rubrik: Impressum</td>\n",
       "      <td>Impressum.</td>\n",
       "      <td>Süddeutsche Zeitung, 12.10.2018, Ausgabe Münch...</td>\n",
       "      <td>4</td>\n",
       "      <td>246</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1511135</th>\n",
       "      <td>2018</td>\n",
       "      <td>10</td>\n",
       "      <td>13</td>\n",
       "      <td>Süddeutsche Zeitung</td>\n",
       "      <td>SZ</td>\n",
       "      <td>Impressum. HERAUSGEGEBEN VOM SÜDDEUTSCHEN VERL...</td>\n",
       "      <td>Ressort: Meinungsseite      Rubrik: Impressum</td>\n",
       "      <td>Impressum.</td>\n",
       "      <td>Süddeutsche Zeitung, 13.10.2018, Ausgabe Münch...</td>\n",
       "      <td>4</td>\n",
       "      <td>246</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         year  month  day            newspaper newspaper_2  \\\n",
       "1510995  2018     10   12  Süddeutsche Zeitung          SZ   \n",
       "1511135  2018     10   13  Süddeutsche Zeitung          SZ   \n",
       "\n",
       "                                                     texts  \\\n",
       "1510995  Impressum. HERAUSGEGEBEN VOM SÜDDEUTSCHEN VERL...   \n",
       "1511135  Impressum. HERAUSGEGEBEN VOM SÜDDEUTSCHEN VERL...   \n",
       "\n",
       "                                               rubrics       title  \\\n",
       "1510995  Ressort: Meinungsseite      Rubrik: Impressum  Impressum.   \n",
       "1511135  Ressort: Meinungsseite      Rubrik: Impressum  Impressum.   \n",
       "\n",
       "                                              quelle_texts page  word_count  \n",
       "1510995  Süddeutsche Zeitung, 12.10.2018, Ausgabe Münch...    4         246  \n",
       "1511135  Süddeutsche Zeitung, 13.10.2018, Ausgabe Münch...    4         246  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sz_duplicates.loc[[1510995,1511135]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop the exact duplicates, keep the article with the earlier publication date ('first')\n",
    "data.drop_duplicates(['texts'], keep = 'first', inplace=True)\n",
    "data.reset_index(inplace=True, drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1499007"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the number of articles after removing exact duplicates\n",
    "len(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filtering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As SZ is a general newspaper, it contains numerous sections that may not be relevant for economic forecasting. To address this, we have manually examined each section of the newspaper by reviewing a sample of articles from every section. Using our professional expertise, we have identified and chosen the sections that are most likely to be relevant for economic forecasting, focusing on recent events rather than historical topics.\n",
    "\n",
    "The sections we retain, which we refer to as 'priority 1' due to their high relevance for economic forecasting, include 'Politik' (Politics), 'Börse und Finanzen' (Stock Market and Finance), 'Geld' (Money), 'Meinungsseite' (Opinion), 'Nachrichten' (News) - which served as a substitute for the 'Politik' section between 1999 and 2003, 'Seite vier' (Page Four), and 'Wirtschaft' (Economy). Among these sections, 'Politik', 'Meinungsseite', 'Nachrichten', and 'Seite vier' focus on politics, while 'Börse und Finanzen', 'Geld', and 'Wirtschaft' cover economic topics.\n",
    "\n",
    "Other sections which might be potentially interesting include 'Dokument', 'Themen', 'Themen des Tages', 'Themen aus Deutschland', 'Themen aus dem Ausland' (all related to politics), and 'Immobilien' (real estate news connected to economic development). However, we leave out these 'priority 2' articles and only concentrate on 'priority 1' articles mentioned earlier, as 'priority 2' articles often have a different format (they are frequently longer) and are less focused on current events.\n",
    "\n",
    "We exclude numerous other sections, such as local news ('München'), advertisements ('Anzeiger'), entertainment news ('Freizeit', 'Hobby', 'Reise', etc.), and letters from readers ('Briefe an die SZ'/'Leserbriefe').\n",
    "\n",
    "In our analysis, we further narrow down the selection by excluding specific subsections from the preserved sections. Examples of these excluded subsections include 'Inhaltsverzeichnis' (Contents), 'Akt. Lexikon' (explanation of important terms), 'Anzeige' (advertisements), and 'Impressum' (contact information, such as names of journalists, their telephone numbers, addresses of editorial offices in different cities, etc.)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this function loads dictionaries with subsections of the newspaper we are going to analyze\n",
    "def dictionary_open(name):\n",
    "    with codecs.open(os.path.join(os.getcwd(), name),\n",
    "               'r',  'utf-8') as f:\n",
    "          dictionary = set(f.read().splitlines()[1:-1])\n",
    "    return dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Each of the dictionaries contains subsections from the sections 'Politics', 'Stock market and Finance', 'Money', 'Opinion', \n",
    "# 'News', 'Page four', and 'Economy'.\n",
    "\n",
    "politics_s = dictionary_open('politics.txt')\n",
    "boerse_s = dictionary_open('boerse.txt')\n",
    "geld_s = dictionary_open('geld.txt')\n",
    "meinung_s = dictionary_open('meinung.txt')\n",
    "nachrichten_s = dictionary_open('nachrichten.txt')\n",
    "vier_s = dictionary_open('vier.txt')\n",
    "wirtschaft_s = dictionary_open('wirtschaft.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a column with the corresponding section based on subsections from the XML files.\n",
    "data['section'] = u'Other'\n",
    "data.loc[(np.isin(data['rubrics'].values, list(politics_s))),'section'] = u'Politik'\n",
    "data.loc[(np.isin(data['rubrics'].values, list(boerse_s))),'section'] = u'Börse und Finanzen'\n",
    "data.loc[(np.isin(data['rubrics'].values, list(geld_s))),'section'] = u'Geld'\n",
    "data.loc[(np.isin(data['rubrics'].values, list(meinung_s))),'section'] = u'Meinungsseite'\n",
    "data.loc[(np.isin(data['rubrics'].values, list(nachrichten_s))),'section'] = u'Nachrichten'\n",
    "data.loc[(np.isin(data['rubrics'].values, list(vier_s))),'section'] = u'Seite vier'\n",
    "data.loc[(np.isin(data['rubrics'].values, list(wirtschaft_s))),'section'] = u'Wirtschaft'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a column \"priority1\" (these are the most important sections) with two global sections \n",
    "# Economy and Politics.\n",
    "Economy = [u'Börse und Finanzen', u'Geld', u'Wirtschaft']\n",
    "Politics = [u'Politik', u'Nachrichten', u'Seite vier', u'Meinungsseite']\n",
    "\n",
    "data['priority1'] = u'Other'\n",
    "data.loc[(np.isin(data['section'].values, Politics)),'priority1'] = u'Politics'\n",
    "data.loc[(np.isin(data['section'].values, Economy)),'priority1'] = u'Economy'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only keep 'priority 1' articles.\n",
    "data = data[(data['priority1']==u'Politics') | (data['priority1']==u'Economy')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "561294"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the number of articles after removing irrelevant sections\n",
    "data.reset_index(inplace=True, drop=True)\n",
    "len(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exclude articles that contain the following strings:\n",
    "\n",
    "* Hultschiner Straße 8, 81677 München: contact details of SZ;\n",
    "* Bayerische Warenbörse/Bayerische Warenboerse: Bavarian commodity exchange (prices for different commodities, no narrative);\n",
    "* Südbayerischer Schweinemarkt/Suedbayerischer Schweinemarkt: pig market, prices;\n",
    "* Münchner Schlachtviehmarkt/Muenchner Schlachtviehmarkt: slaughter cattle market, prices;\n",
    "* Notierungen der Butter- und Käsebörse/Notierungen der Butter- und Kaeseboerse/Butter- und Kaeseboerse/Butter- und Käsebörse: market of butter and cheese, prices;\n",
    "* Abonnement: : details about the subscription cost;\n",
    "* MARKTDATEN VOM TAGE: quantitative information on stock market prices, interest rates;\n",
    "* bat Studiotheater/Theater Arena/Theater Akademie/Theater Deutsches Theater/Theater Ballhaus Naunynstrasse: a detailed list of various cultural events;\n",
    "* Herausgegeben vom Sueddeutschen Verlag/Herausgegeben vom Süddeutschen Verlag: a list of staff members;\n",
    "* Das Publikum im Kleinen Haus: theater news;\n",
    "* Konkurse\\, Vergleich und Gesamtvollstreckungsverfahren: list of companies that have filed for bankruptcy or insolvency;\n",
    "* AUS DEM INHALT/S Z AM WO C H E N E N D E/SZ AM WOCHENENDE/SIE LESEN HEUTE/Aus dem Inhalt Franjo Kiseljak: contents;\n",
    "* ^Was diese Woche bringt: lists of upcoming events, meetings, and announcements related to various topics. The articles starting with \"Was diese Woche bringt\" cover a wide range of subjects, making it difficult to categorize or analyze them effectively.;\n",
    "* Die größten Unternehmen. Rang/Die globalen Top 50: list of the largest German companies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_strings = ['Hultschiner Straße 8\\, 81677 München', 'Bayerische Warenbörse', \n",
    "                'Bayerische Warenboerse', 'Südbayerischer Schweinemarkt', 'Suedbayerischer Schweinemarkt',\n",
    "               'Münchner Schlachtviehmarkt', 'Muenchner Schlachtviehmarkt', 'Notierungen der Butter- und Käsebörse',\n",
    "               'Notierungen der Butter- und Kaeseboerse', 'Butter- und Kaeseboerse', 'Butter- und Käsebörse', \n",
    "                'Abonnement\\:', 'MARKTDATEN VOM TAGE', 'bat Studiotheater', 'Theater Arena', \n",
    "                'Herausgegeben vom Sueddeutschen Verlag', 'Herausgegeben vom Süddeutschen Verlag', 'Theater Akademie',\n",
    "               'Theater Deutsches Theater', 'Theater Ballhaus Naunynstrasse', 'Das Publikum im Kleinen Haus',\n",
    "               'Konkurse\\, Vergleich und Gesamtvollstreckungsverfahren', 'AUS DEM INHALT', 'S Z AM WO C H E N E N D E',\n",
    "               'SZ AM WOCHENENDE', 'SIE LESEN HEUTE', 'Aus dem Inhalt Franjo Kiseljak', '^Was diese Woche bringt',\n",
    "               'Die größten Unternehmen\\. Rang', 'Die globalen Top 50']\n",
    "data = data[~(data.texts.str.contains('|'.join(text_strings)))]\n",
    "data.reset_index(inplace=True, drop=True) # reset the index of the DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "559488"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the number of articles after excluding articles based on the text patterns\n",
    "len(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Title"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exclude articles with the following titles:\n",
    "\n",
    "* HEUTE IN DER SZ./HEUTE IN DER SZ /.: contents;\n",
    "* Konkurse, Vergleiche und Gesamtvollstreckungsverfahren./Konkurse, Vergleiche und Gesamtvollstreckungen./Konkurse, Vergleiche und Zwangsvollstreckungsverfahren./Konkurse, Vergleiche und Zwangsvollstreckungen.: list of companies that have filed for bankruptcy or insolvency;\n",
    "* INHALT.: contents;\n",
    "* Gewinnzahlen./Gewinnquoten./GEWINNZAHLEN./GEWINNQUOTEN./FINANZEN. Gewinnquoten./UNTERNEHMEN. Gewinnquoten./KURZ GEMELDET. Gewinnquoten./POLITIK UND MARKT. Gewinnquoten./POLITIK UND MARKT. Gewinnzahlen./KURZ GEMELDET. Gewinnzahlen./UNTERNEHMEN. Gewinnzahlen.: winning numbers and prizes for various German lotteries;\n",
    "* Außerdem in dieser Ausgabe.: contents;\n",
    "* Kinoprogramm.: movie schedule;\n",
    "* Terminkalender.: a schedule of various local events;\n",
    "* Notdienste.: a list of emergency and helpline contacts;\n",
    "* SIE LESEN HEUTE IN DER SZ./SI E LE S E N H EUTE.: contents;\n",
    "* 'Gipfelstürmer.': a description of a competition called \"Gipfelstürmer\", every article titled 'Gipfelstürmer.' contains identical text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "titles_to_exclude = [\n",
    "    'HEUTE IN DER SZ.', 'Konkurse, Vergleiche und Gesamtvollstreckungsverfahren.',\n",
    "    'INHALT.', 'Gewinnzahlen.', 'Gewinnquoten.', 'GEWINNZAHLEN.',\n",
    "    'Außerdem in dieser Ausgabe.', 'Kinoprogramm.', 'Terminkalender.', 'GEWINNQUOTEN.',\n",
    "    'Notdienste.', 'FINANZEN. Gewinnquoten.', 'Konkurse, Vergleiche und Gesamtvollstreckungen.',\n",
    "    'SIE LESEN HEUTE IN DER SZ.', 'UNTERNEHMEN. Gewinnquoten.', 'KURZ GEMELDET. Gewinnquoten.', \n",
    "    'POLITIK UND MARKT. Gewinnquoten.', 'SI E LE S E N H EUTE.', 'POLITIK UND MARKT. Gewinnzahlen.',\n",
    "    'KURZ GEMELDET. Gewinnzahlen.', 'UNTERNEHMEN. Gewinnzahlen.', 'HEUTE IN DER SZ /.', \n",
    "    'Konkurse, Vergleiche und Zwangsvollstreckungsverfahren.', 'Konkurse, Vergleiche und Zwangsvollstreckungen.',\n",
    "    'Gipfelstürmer.'\n",
    "]\n",
    "\n",
    "data = data[~data['title'].isin(titles_to_exclude)]\n",
    "data.reset_index(inplace=True, drop=True) # reset the index of the DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "557097"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = data.sort_values(['year', 'month', 'day'], ascending=[True, True, True]) # sort the data in chronological order\n",
    "data.reset_index(inplace=True, drop=True) # reset the index of the DataFrame\n",
    "# the number of articles after excluding articles based on the title\n",
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({1999: 27928, 1994: 25990, 2000: 25904, 1995: 25712, 1996: 24801, 1997: 24766, 2001: 24733, 1998: 24690, 2008: 24457, 2010: 23498, 2009: 22863, 2006: 22702, 2011: 22673, 2005: 22456, 2007: 22063, 2003: 21940, 2004: 21911, 2012: 21220, 2002: 20874, 2013: 19314, 2014: 18242, 2016: 17922, 2017: 17550, 2015: 17473, 2018: 15415})\n"
     ]
    }
   ],
   "source": [
    "counter=collections.Counter(data['year'])\n",
    "print(counter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Umlauts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the given dataset, some texts contain incorrect umlaut encodings, such as '&auml;', '&uuml;', '&ouml;', '&Auml;', '&Uuml;', and '&Ouml;'. These encodings are typically used in HTML documents to represent umlaut characters like 'ä', 'ü', 'ö', 'Ä', 'Ü', and 'Ö'. However, in this case, the encodings are not being interpreted correctly, leading to broken umlauts in the text.\n",
    "\n",
    "To fix this issue, we use the function `umlauts_correct_sz` that replaces these incorrect umlaut encodings with the correct umlaut characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Mühl Product & Service kauft Aktien zur&uuml;ck. Kranichfeld (Reuters) - Der am Neuen Markt gelistete Baudienstleister Mühl Product & Service beginnt mit dem R&uuml;ckkauf eigener Aktien. Ab sofort sollen bis zu drei Prozent der eigenen Anteilsscheine &uuml;ber die B&ouml;rse erworben werden, wie die Th&uuml;ringer Gesellschaft am Freitag mitteilte. Die Erm&auml;chtigung der Hauptversammlung sehe vor, dass bis zum 15. Dezember 2001 bis zu zehn Prozent des derzeitigen Grundkapitals der Gesellschaft &uuml;ber die B&ouml;rse oder im Rahmen eines &ouml;ffentlichen Kaufangebots erworben werden k&ouml;nnten. Der Aktienr&uuml;ckkauf sei auch in Teilen m&ouml;glich. Der Kaufpreis je Aktie darf den Angaben zufolge den durchschnittlichen Schlusskurs an den f&uuml;nf B&ouml;rsentagen vor Erwerb der Anteilsscheine um nicht mehr als zehn Prozent unter- oder &uuml;berschreiten. Im Falle eines &ouml;ffentlichen Kaufangebotes d&uuml;rfe der Kaufpreis je Aktie den durchschnittlichen Schlusskurs des achten bis vierten Handelstages vor Ver&ouml;ffentlichung des Kaufangebots um nicht mehr als 15 Prozent &uuml;ber- oder unterschreiten, hieß es.'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define a list of incorrect umlaut encodings\n",
    "incorrect_umlauts = [\"&auml;\", \"&uuml;\", \"&ouml;\", \"&Auml;\", \"&Uuml;\", \"&Ouml;\"]\n",
    "\n",
    "pattern = \"|\".join(incorrect_umlauts)\n",
    "sz_uml_encoding = data[data.texts.str.contains(pattern)]\n",
    "\n",
    "sz_uml_encoding.iloc[1]['texts']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:00:11.447588\n"
     ]
    }
   ],
   "source": [
    "startTime = datetime.now() # track time\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    pool = mp.Pool(NUM_CORE)\n",
    "    texts_corrected = pool.map(umlauts_correct_sz.umlauts_correct_sz, [text for text in data['texts']]) \n",
    "    pool.close()\n",
    "    pool.join()\n",
    "    \n",
    "print(datetime.now()-startTime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['texts'] = texts_corrected"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second problem with umlauts in the early texts (published before 1995) is that they are replaced with digraphs like 'ae' for 'ä', 'oe' for 'ö', 'ue' for 'ü', 'ss' for 'ß', 'AE' for 'Ä', 'OE' for 'Ö', and 'UE' for 'Ü'. This discrepancy can lead to an increased vocabulary size and potential ambiguity in word representation. To fix the issue with umlauts, we use the notebook `Umlauts_fix` written in Python 2. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "umlauts = ['ä', 'ö', 'ü', 'ß', 'Ä', 'Ö', 'Ü']\n",
    "umlauts_replace = ['ae', 'oe', 'ue', 'ss', 'AE', 'OE', 'UE']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "sz_umlauts_fix = data[(data.texts.str.contains('|'.join(umlauts_replace))) & (~data.texts.str.contains('|'.join(umlauts))) & (data.year<1995)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Trotz aller Friedensappelle in Suedafrika Terrorwelle zur Jahreswende. Mord an Pub-Besuchern in Kapstadt gibt Raetsel auf. Kapstadt (AP/Reuter/AFP) - Trotz aller Friedensappelle hat das neue Jahr in Suedafrika mit blutigen Gewalttaten begonnen. Fuenf maskierte Schwarze toeteten in der Silvesternacht in einem Pub in Kapstadt vier Weisse. In Johannesburg wurden zwei Polizisten erschossen. In den Schwarzensiedlungen Tokoza und Katlehong oestlich von Johannesburg entdeckte die Polizei die Leichen von fuenf Maennern und einer Frau. Die meisten der Opfer seien erschossen worden, erklaerte die Polizei. Bei einem weiteren Zwischenfall in der Naehe der Hafenstadt Durban seien bei der von ethnischen Indern bewohnten Vorstadt Chatsworth drei Menschen getoetet und zwei weitere verletzt worden. Ebenfalls in Durban wurden nach einem Fernsehbericht bei einem Streit an dem vielbesuchten Strand Brighton Beach ein Mann erschossen und ein weiterer durch einen Messerstich verletzt. Die Hintergruende des UEberfalls auf den Pub in einer Weissensiedlung Kapstadts sind zunaechst ungeklaert. Bewaffnete Schwarze waren an Silvester in das ueberwiegend von Weissen besuchte Lokal eingedrungen und hatten um sich geschossen. Namens zweier militanter Schwarzenorganisationen gingen Bekenneranrufe ein. Die Polizei teilte mit, bei dem UEberfall seien Waffen von der Art verwendet worden, wie sie bei den Streitkraeften benutzt werden. Sie loeste damit Spekulationen aus, die Moerder koennten von weissen Extremisten gedungen sein. Zum Zeitpunkt des UEberfalls hielten sich noch zwischen 20 und 50 Personen in dem Lokal Heidelberg auf. Nach Augenzeugenberichten schossen die Angreifer mit automatischen Waffen wild um sich. Drei weisse Frauen waren nach Angaben der Polizei auf der Stelle tot. Das vierte Todesopfer war ein Mann, der von den Taetern auf der Flucht erschossen wurde. Der Fluchtwagen wurde spaeter in der Schwarzenwohnstadt Guguletu, einer Hochburg des Panafrikanistischen Kongresses (PAC), gefunden. Zu der Bluttat bekannte sich ein Anrufer bei der suedafrikanischen Nachrichtenagentur SAPA im Namen der militanten schwarzen Gruppierung Azanische Volksbefreiungsarmee (APLA), die der militaerische Fluegel des PAC ist. Der Anrufer kuendigte weitere Anschlaege vor den ersten allgemeinen Wahlen am 27. April an. Spaeter ging bei SAPA ein weiterer Anruf ein, in dem der Anrufer im Namen der militanten Schwarzenorganisation Azanische Nationale Befreiungsarmee (AZANLA) die Verantwortung fuer den Anschlag auf das Lokal uebernahm. Die AZANLA bekannte sich bereits zu drei kleineren Bombenanschlaegen in dieser Woche in der Hafenstadt Port Elizabeth. Die Polizei setzte inzwischen eine Belohnung in Hoehe von 200 000 Rand (rund 100 000 Mark) fuer Hinweise aus, die zur Ergreifung der Taeter fuehren. (Seite 4)'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# example of the text that we try to fix with a spellchecker\n",
    "sz_umlauts_fix.texts[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "sz_umlauts_fix.to_csv('sz_umlauts_fix.csv', encoding='utf-8-sig', sep = ';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "sz_umlauts_fixed = pd.read_csv('sz_umlauts_fixed.csv', encoding = 'utf-8', sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.loc[sz_umlauts_fixed['Unnamed: 0.1'], 'texts'] = sz_umlauts_fixed.texts.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Trotz aller Friedensappelle in Südafrika Terrorwelle zur Jahreswende. Mord an Pub - Besuchern in Kapstadt gibt Rätsel auf. Kapstadt (AP / Reuter / AFP) - Trotz aller Friedensappelle hat das neue Jahr in Südafrika mit blutigen Gewalttaten begonnen. Fünf maskierte Schwarze töteten in der Silvesternacht in einem Pub in Kapstadt vier Weiße. In Johannesburg wurden zwei Polizisten erschossen. In den Schwarzensiedlungen Tokoza und Katlehong östlich von Johannesburg entdeckte die Polizei die Leichen von fünf Männern und einer Frau. Die meisten der Opfer seien erschossen worden, erklärte die Polizei. Bei einem weiteren Zwischenfall in der Nähe der Hafenstadt Durban seien bei der von ethnischen Indern bewohnten Vorstadt Chatsworth drei Menschen getötet und zwei weitere verletzt worden. Ebenfalls in Durban wurden nach einem Fernsehbericht bei einem Streit an dem vielbesuchten Strand Brighton Beach ein Mann erschossen und ein weiterer durch einen Messerstich verletzt. Die Hintergründe des Überfalls auf den Pub in einer Weissensiedlung Kapstadts sind zunächst ungeklärt. Bewaffnete Schwarze waren an Silvester in das überwiegend von Weißen besuchte Lokal eingedrungen und hatten um sich geschossen. Namens zweier militanter Schwarzenorganisationen gingen Bekenneranrufe ein. Die Polizei teilte mit, bei dem Überfall seien Waffen von der Art verwendet worden, wie sie bei den Streitkräften benutzt werden. Sie löste damit Spekulationen aus, die Mörder könnten von weißen Extremisten gedungen sein. Zum Zeitpunkt des Überfalls hielten sich noch zwischen 20 und 50 Personen in dem Lokal Heidelberg auf. Nach Augenzeugenberichten schossen die Angreifer mit automatischen Waffen wild um sich. Drei weiße Frauen waren nach Angaben der Polizei auf der Stelle tot. Das vierte Todesopfer war ein Mann, der von den Tätern auf der Flucht erschossen wurde. Der Fluchtwagen wurde später in der Schwarzenwohnstadt Guguletu, einer Hochburg des Panafrikanistischen Kongresses (PAC), gefunden. Zu der Bluttat bekannte sich ein Anrufer bei der südafrikanischen Nachrichtenagentur SAPA im Namen der militanten schwarzen Gruppierung Azanische Volksbefreiungsarmee (APLA), die der militärische Flügel des PAC ist. Der Anrufer kündigte weitere Anschläge vor den ersten allgemeinen Wahlen am 27. April an. Später ging bei SAPA ein weiterer Anruf ein, in dem der Anrufer im Namen der militanten Schwarzenorganisation Azanische Nationale Befreiungsarmee (AZANLA) die Verantwortung für den Anschlag auf das Lokal übernahm. Die AZANLA bekannte sich bereits zu drei kleineren Bombenanschlägen in dieser Woche in der Hafenstadt Port Elizabeth. Die Polizei setzte inzwischen eine Belohnung in Höhe von 200 000 Rand (rund 100 000 Mark) für Hinweise aus, die zur Ergreifung der Täter führen. (Seite 4)'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fixed version\n",
    "data.texts[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Identify and Delete non-German Articles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we filter out articles written in any language other than German using a **langdetect** library."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We remove the following types of articles:\n",
    "\n",
    "1) texts predominantly comprising English names of individuals, states, and organizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Gewählte Gouverneure in 36 Bundesstaaten Alabama (AL): Donald Siegelman (Dem) Alaska (AK): Tony Knowles (Dem) Arizona (AZ): Jane Dee Hull (Rep) Arkansas (AR): Mike Huckabee (Rep) Colorado (CO): Bill Owens (Rep) Connecticut (CT): John Rowland (Rep) Florida (FL): John Ellis 'Jeb' Bush (Rep) Georgia (GA): Roy Barne (Dem) Hawaii(HI): Ben Cayetano (Dem) Idaho (ID): Dirk Kempthorne (Rep) Illinois (IL): George Ryan (Rep) Iowa (IA): Tom Vilsack (Dem) Kalifornien (CA): Gray Davis (Dem) Kansas (KS): Bill Graves (Rep) Maine (ME): Angus King (unabh.) Maryland (MD): Parris Glendening (Dem) Massachusetts (MA): Paul Cellucci (Rep) Michigan (MI): John Engler (Rep) Minnesota (MN): Jesse Ventura (unabh.) Nebraska (NE): Mike Johanns (Rep) Nevada (NV): Kenny Guinn (Rep) New Hampshire (NH): Jeanne Shaheen (Dem) New Mexiko (NM): Gary Johnson (Rep) New York (NY): George Pataki (Rep) Ohio (OH): Bob Taft (Rep) Oklahoma (OK): Frank Keating (Rep) Oregon (OR): John Kitzhaber (Dem) Pennsylvania (PA): Thomas Ridge (Rep) Rhode Island (RI): Lincoln Almond (Rep) South Carolina (SC): Jim Hodges (Dem) South Dakota (SD): William Janklow (Rep) Tennessee (TN): Don Sundquist (Rep) Texas (TX): George W. Bush (Rep) Vermont (VT): Howard Dean (Dem) Wisconsin (WI): Tommy Thompson (Rep) Wyoming (WY): Jim Geringer (Rep)\""
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.iloc[122092]['texts']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) texts including informal language, slang, and interjections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Das Streiflicht. (SZ) Ja. Ja! Jaaaaaaa!!! Ja, Franz. Jaaaaaaaa. Ja. Ja. Ja. Jajaja-ja, jajaja-ja! Ja! Jaaaaaaaaaaaaaaaaaaaaa - jahaha. O ja, Franz. Jaaaa! Jaaaa! Jaaaa! Jaaaa! Ja!! Ja, ja, ja, ja, ja. Ehhh-ja!! Ja, logo. Jaaaaaaa! Jaaa. Jaaa. Hahaaaaaa! Jawollja. Jaaaaaaaa!! Mensch, Franz, oide Fischhaut! Jaaaaaaaa. Ja! Ja! Ja! Jaaaaaa! Jajajajajaja. Jaaaa. Hahaaaa: Zürich. Züri brännt, jaha. Jaaaa. End se winner is? Franz! Se Kaiser! Kaiser Fränz se Bayer or se först. Senks! Senks, Franzl, old Giesing Gwachs! Ja, ja, jaaaaa, ja. Jaaaaaaaaaaaaa! Jaaaaaaaaaaaaaaa!! Ja. Ja. Jaaaaaa! Mhhhja. \"Ja! Ja! Ja!\" (Bild) bzw. \"Jaaa!\" (tz). Jaaaaaaaaa. Jaaaaaaaaa. Jaja. Ja. Ja. Ja. Tjaja. Yes. Si. Da. Oui. Wuiwui, Franz. Jaaa. Jaaaaaaaa. Ja. Ja? Jaaaaaaa??? Hm. Tja. Naja. Blatter. Netzer. Völler. Becker. Schiffer. Wieso Becker? Hm. Tja. Und wieso Schiffer? Ahhhhhhh ja. Bravo, Claudia! Jaaajaja! Jaaa. Jaaa. Jaaa. Schaumermal. Nnnnnnnnn-ja! Jaja. Jo, doch. Jaaaaa-ha! Oja, o là là! Jajajajajajajajajajaja-jaaaa. 2006, Reschpekt! Jaaaa. Jaaaa. Ja. Jaa. Jajajajajaja: \"Deutschland jubelt\" (AZ). Jaha! Ja! Ja! Ja! Ja! \"Gewaltig\", so Schorsch \"Weißwurst\" Hackl. Jaaaa. Kinkel the Pinkel: \"Ein Golden Goal.\" Ja, ja, und tausendmal ja. Jaaaaa! Jaaaa. Jaaa. Jaa. Ja. J. a. J-a. Die Grammatik des Spiels, die Lesbarkeit des Balls. Jaaahaa. Das Genom Ball. Hoho und haha! CAAAAC GTATAA TCCTTG AATGGT: Der Ball ist viereckig. Boah ey. Jaaaaa! Ja, ja, ja, ja, ja, ja. JA! Jaaaaaaaa. Franz. Frantz. Fratz. Uiuiui! Kaiser. Daradda-rombombombom-baadaram! Franz I. der Kicker, Kaiser der Flanken, Beherrscher so der Rasen wie der Parkette, allezeit Mehrer des DFB. Jaaaaa. Jaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa!!!!!! Ja. Jaa. Jaaa. Danke, Franz. Danke, Schily. Danke, Claudia-Maus. Danke, Joseph \"Sepp\" Blatttrrrr. Danke, Südafrika. Danke, Fifa-Exekutiv-Komitee. Special thanks to the Right Honorable Charles Dempsey, New Zealand. Jaja, der Dempsey. Jaaaa. Jaaa. Jaaaaaaaaaaaaa!!!! Hip hip Hurra! Es braust ein Dingsbumms wie - egal! Und abermals schaumermal. Jaaaa-hahaa. Ja! Ja! Ja! Sind wieder wer! Dank Franz. Jaaaa. Mein lieber Schwan, da wennsd ma ned gangst! Gangerst, also gingest. Jajajajaja. Jaaaaaaaaaaaa-(schnauf!)-aaaaaaaaaaaaaaaaaaaaaaaaaa. Ja. Franz for president. Why not? Darauf ein dreifach \"Schaumermal-nasehngmascho!\" Jaaaaa. Jaaaaa. Ärmel hochkrempeln, sagt Daum. Jaja. Die alten Wurzeln von Franz Beckenbauer, sagt TV-Lady Schürmann. O ja, weiß der Geier: die alten Wurzeln! Ja. Jaaaaaa. Jahahaha. Wurzelfranz, der Winner und Bringer und Macher und Einfädler. Au ja! Und dabei so bescheiden, du glaubst es nicht. Mann, o Mann! Ja! Jaa! Jaaa! Jaaaa! Jaaaaa!! Du meine Güte: 2006, WM in Deutschland. Fußball satt, bis zum Abwinken. Kriegt der Kicker auf den Ranzen, ruft um Hülf er zu Sankt Franzen. Amen. Ja.'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.iloc[167241]['texts']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3) tables without any supporting text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'29. 12. 00 2)Veränd. gegen Emissionspr.Veränd. geg. ErstnotizUpdate. com 11.4.NM 3)23,0024,0040,001,851,51-93,43%-93,71%Intraware Software12.5.NM28,0029,0034,902,002,50-91,07%-91,38%Mediascape Comm.22.5.NM47,0045,0047,503,804,40-90,64%-90,22%Comtelco7.2.GM 4)18,0023,0028,901,201,89-89,50%-91,78%Ad Pepper Media9.10.NM17,0017,5018,102,102,22-86,94%-87,31%Travel24.com13.3.NM29,0032,0032,003,054,30-85,17%-86,56%Openshop Holding21.3.NM54,00106,00130,007,808,30-84,63%-92,17%Arbo Media.net9.5.NM43,0053,0056,006,607,10-83,49%-86,60%Lycos Europe22.3.NM24,0024,0024,003,404,10-82,92%-82,92%Matchnet27.6.NM7,506,506,801,401,30-82,67%-80,00%Valor Computerized Syst.15.5.NM12,0014,3015,701,962,13-82,25%-85,10%d + s online23.5.NM25,0025,0025,404,104,50-82,00%-82,00%Alphaform28.6.NM17,0016,0016,702,253,40-80,00%-78,75%Advanced Vision Techn.28.2.NM14,0026,0029,002,002,85-79,64%-89,04%Adlink Internet Media11.5.NM17,0016,2020,953,253,50-79,41%-78,40%Softing16.5.NM14,5016,0031,502,853,07-78,83%-80,81%Carrier 124.2.NM87,00135,00174,0015,0019,00-78,16%-85,93%PlasmaSelect1.3.NM45,00120,00179,009,0010,00-77,78%-91,67%aeco25.7.NM14,0016,0019,202,653,20-77,14%-80,00%Adori10.5.NM13,5013,5014,003,203,10-77,04%-77,04%Concept27.3.NM49,0056,0074,0010,0011,50-76,53%-79,46%United Visions Entert.20.6.NM24,0028,0029,805,005,65-76,46%-79,82%Trius9.3.NM36,5036,5052,008,508,90-75,62%-75,62%Focus Digital13.7.NM14,5012,0014,403,403,75-74,14%-68,75%Allgeier Systems11.7.NM22,5021,0027,504,005,90-73,78%-71,90%Pro-DV Software22.3.NM23,0022,0025,505,556,15-73,26%-72,05%DCI Database13.3.NM32,0065,00101,006,008,60-73,13%-86,77%Ifco6.3.AH 5)15,5036,0038,003,154,25-72,58%-88,19%Nexus24.7.NM10,009,009,802,252,80-72,00%-68,89%Web.de17.2.NM26,0065,0078,007,007,50-71,15%-88,46%Orbis25.9.NM14,0012,5012,803,504,20-70,00%-66,40%Cycos18.4.NM19,0032,5039,005,505,70-70,00%-82,46%Net17.3.NM14,0037,0039,004,104,20-70,00%-88,65%QS Communications19.4.NM13,0019,0021,302,853,98-69,38%-79,05%Eichborn Verlag9.5.SM 6)12,0013,3514,403,303,70-69,17%-72,28%1) Reihenfolge nach Kursveränderung im Vergleich zum Emissionspreis; 2) in Euro; 3) Neuer Markt; 4) geregelter Markt; 5) amtlicher Handel; 6) Smax (Qualitätssegment des geregelten Marktes) Quellen: Reuters; eigene Berechnungen'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.iloc[179911]['texts']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:15:47.300177\n"
     ]
    }
   ],
   "source": [
    "# Delete all non-German articles from the data  \n",
    "startTime = datetime.now()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    pool = mp.Pool(NUM_CORE)\n",
    "    ger_results = pool.map(identify_ger.identify_ger, [text for text in data['texts']]) \n",
    "    pool.close()\n",
    "    pool.join()\n",
    "\n",
    "print(datetime.now()-startTime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n",
      "557090\n"
     ]
    }
   ],
   "source": [
    "data['language'] = ger_results\n",
    "# the number of non-German articles\n",
    "print(len(data[data.language == 0]))\n",
    "# keep articles written in German\n",
    "data = data[data.language==1]\n",
    "data.reset_index(inplace=True, drop=True)\n",
    "# the number of articles after excluding non-German articles\n",
    "print(len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_csv('non_german_delete.csv', encoding = 'utf-8-sig', sep = ';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('non_german_delete.csv', encoding = 'utf-8-sig', sep=';', index_col = 0, dtype = {'newspaper': 'str',\n",
    "                                                                                                 'newspaper_2': 'str',\n",
    "                                                                                                 'quelle_texts': 'str',\n",
    "                                                                                                 'page': 'str',\n",
    "                                                                                                 'rubrics': 'str'})\n",
    "data.page = data.page.fillna('')\n",
    "data.newspaper = data.newspaper.fillna('')\n",
    "data.newspaper_2 = data.newspaper_2.fillna('')\n",
    "data.rubrics = data.rubrics.fillna('')\n",
    "data.quelle_texts = data.quelle_texts.fillna('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove URLs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A function called `correct_url_sz` takes a text input and processes it to remove URLs, HTML file references, and email addresses. A set called `exceptions` contains specific URLs that are not removed from the text. Preserving specific URLs, such as 'amazon.de', is crucial for maintaining the context and sentence structure, as removing them could result in loss of important information or topic identification, especially when certain internet companies are known primarily by their website names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:00:40.701737\n"
     ]
    }
   ],
   "source": [
    "startTime = datetime.now()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    pool = mp.Pool(NUM_CORE)\n",
    "    url_corrected = pool.map(correct_url_sz.correct_url_sz, [text for text in data['texts']]) \n",
    "    pool.close()\n",
    "    pool.join()\n",
    "\n",
    "print(datetime.now()-startTime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['texts'] = url_corrected"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## O instead of 0 problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In some cases, Optical Character Recognition (OCR) can not distinguish between '0' and 'O' ('o'). As a result, there are tokens like '1OO'. Using regular expressions, we identify problematic tokens and replace 'O' ('o') with '0'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:00:12.808099\n"
     ]
    }
   ],
   "source": [
    "startTime = datetime.now()\n",
    "\n",
    "import ocr_replace\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    pool = mp.Pool(NUM_CORE)\n",
    "    ocr_corrected = pool.map(ocr_replace.ocr_replace, [text for text in data['texts']]) \n",
    "    pool.close()\n",
    "    pool.join()\n",
    "\n",
    "print(datetime.now()-startTime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['texts'] = ocr_corrected"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fixing tokens containing a number and a word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have identified several cases where numbers and words are erroneously merged into a single token. To address this problem of merged tokens, the following code has been implemented."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:00:17.302124\n"
     ]
    }
   ],
   "source": [
    "startTime = datetime.now()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    pool = mp.Pool(NUM_CORE)\n",
    "    split_corrected = pool.map(split_number_word.split_number_word, [text for text in data['texts']]) \n",
    "    pool.close()\n",
    "    pool.join()\n",
    "\n",
    "print(datetime.now()-startTime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['texts'] = split_corrected"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Concatenate chained articles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By the chained articles we mean the news reports that start on one page and continue on another page of the newspaper. Since the news database is prepared using OCR technology, each part of a chained news article represents a separate entry in the database, i.e. a separate news report.\n",
    "\n",
    "Ideally, all the parts of a chained article must be combined into one news report. However, as nothing in the metadata helps us identify chained articles, the merging process is not straightforward. We do our best to merge at least some chained articles based on the text alone.\n",
    "\n",
    "The second part of a chained article normally contains one of the following strings: 'Fortsetzung von Seite' (continuantion from page) or 'FORTSEZUNG VON SEITE'. We select the artilces that meet this critetion and try to merge them with their beginnings.\n",
    "\n",
    "However, we also exclude the articles that contain strings from both lists, `continued_strings_1` and `continued_strings_2`. This is because these chained articles have already been merged by the SZ team."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "continued_strings_1 = ['Fortsetzung S\\\\.', 'Fortsetzung auf Seite', 'Fortsetzung Seite', 'Fortsetzung nächste Seite',\n",
    "                       'Fortsetzung Seiten', 'FORTSETZUNG S\\\\.', 'FORTSETZUNG AUF SEITE', 'FORTSETZUNG SEITE', \n",
    "                       'FORTSETZUNG NÄCHSTE SEITE', 'FORTSETZUNG SEITEN']\n",
    "continued_strings_2 = ['Fortsetzung von Seite', 'FORTSETZUNG VON SEITE']\n",
    "\n",
    "# Articles that contain one of the strings from 'continued_strings_1' and one of the strings from \n",
    "# 'continued_strings_2' are examples of chained articles that have already been merged. \n",
    "continued_articles = data[(data.texts.str.contains('|'.join(continued_strings_2))) & \n",
    "                         (~data.texts.str.contains('|'.join(continued_strings_1)))]\n",
    "continued_articles = continued_articles.reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the SZ data, there are only 17 chained articles that have not been merged. For the sake of completeness and to demonstrate that it is not an issue for this dataset, we try to merge them using the existing function `chained_articles` written for Handelsblatt data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# There are 17 articles that could potentially be part of the chained articles.\n",
    "len(continued_articles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An input of the function `chained_articles` is a tuple, where the first element is a row of the dataframe corresponding to the second part of the chained article. The second element of the tuple is a dataframe containing all articles published on the same day as the second part of the chained article, i.e. all articles that could potentially be the first part of the chained article."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Required input for the function 'chained_articles': a row of the 'continued_articles' df, and a subset of the\n",
    "# 'data' df corresponding to the articles published on the same day as the considered chained article.\n",
    "\n",
    "# List with the potentially chained articles\n",
    "cont_input = []\n",
    "# List with the dataframes containing articles published on the same day as the considered chained article\n",
    "data_input = []\n",
    "\n",
    "# To merge the articles from 'chain_exceptions' with the corresponding first parts, more complicated rules \n",
    "# are required than those we use in the funciton 'chained_articles'. Therefore, we list them as exceptions.\n",
    "chain_exceptions = [15,16]\n",
    "# An input for the function 'chained_articles'.\n",
    "for ind in continued_articles.index:\n",
    "    if ind not in chain_exceptions:\n",
    "        cont_input.append(continued_articles.iloc[[ind]])\n",
    "        data_input.append(data[(data.day == continued_articles['day'][ind]) & \n",
    "                      (data.month == continued_articles['month'][ind]) &\n",
    "                     (data.year == continued_articles['year'][ind])])\n",
    "    \n",
    "inputs_cont = list(zip(cont_input, data_input))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function `chained_articles` returns a list with two tuples. The first tuple contains two indices corresponging to the first and second part of a chained article. These two parts must be merged into one article.\n",
    "    \n",
    "The second tuple contains two indices as well. The first index corresponds to a part of the chained article that is a duplicate because the merged version of this chained article already exists in the database. The article with this index must be deleted. The second index corresponds to the merged version of the chained article. This article must be kept in the database.\n",
    "\n",
    "All the rules we use to find the first part of a chained article are described in detail in the function `chained_articles`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:00:05.895131\n"
     ]
    }
   ],
   "source": [
    "startTime = datetime.now() \n",
    "\n",
    "chained_pair = []\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    pool = mp.Pool(NUM_CORE)\n",
    "    chained_intermediate = pool.map(chained_articles.chained_articles, inputs_cont)\n",
    "    chained_pair = chained_pair + chained_intermediate \n",
    "    pool.close()\n",
    "    pool.join()\n",
    "    \n",
    "print(datetime.now()-startTime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The list of unique tuples, where the first element is a duplicate and \n",
    "# the second element is a merged chained article.\n",
    "duplicates = list(set([sublist[1] for sublist in chained_pair if sublist[1] != (-1, -1)]))\n",
    "# The list with indices of all the duplicated/merged articles.\n",
    "dup_merged = [tup[0] for tup in duplicates]+[tup[1] for tup in duplicates]\n",
    "# The list with all the chained articles that we would like to merge.\n",
    "chained_pair_list = [sublist[0] for sublist in chained_pair if sublist[0] != ()]\n",
    "# Exclude duplicated/merged articles from the list of the chained articles.\n",
    "chained_pair_list = [pair for pair in chained_pair_list if (pair[0] not in dup_merged) and \\\n",
    "                     (pair[1] not in dup_merged)]\n",
    "# The rules we use allow us to merge 12 chained articles\n",
    "len(chained_pair_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Die Rede des neuen Staatsoberhaupts Herzog will Präsident aller Deutschen sein. Die Ostdeutschen müssten begreifen, dass Sie für uns keine Last, sondern dass Sie für uns ein Gewinn sind. Berlin (dpa) - Nach seiner Wahl zum Bundespräsidenten sagte Roman Herzog vor der Bundesversammlung in Berlin: Frau Präsidentin, meine Damen und Herren, das ist eine bewegende Stunde für mich. Es ist immer wieder in den letzten Tagen daran erinnert worden, wie die letzte Bundespräsidentenwahl, die hier in Berlin im Reichstag stattgefunden hat, sich im März 1969 abgespielt hat. Vielleicht wissen es viele von Ihnen nicht. In dieser Zeit waren meine Frau und ich Bürger von West - Berlin, und wir haben es miterlebt, wie damals die Regierung der DDR - für einige Stunden wenigstens, aber niemand wusste ja, wie lang das dauern würde -, die Zufahrtswege zu Lande abgesperrt hat. Und wir haben es nie für möglich gehalten, dass eine Stunde wie diese noch einmal zu unseren Lebzeiten möglich sein würde. Es ist ein Wunder, dass wir leben, und wir sind Ihnen allen, ich vor allem, Ihnen allen, denen, die mich jetzt gewählt haben und denen, die mich nicht wählen konnten, herzlich dafür dankbar, dass das möglich geworden ist. Ich sage es bewusst auch an diejenigen, die mir Ihre Stimme aus guten Gründen nicht geben konnten. Ich werde mich bemühen, das Amt so zu führen, dass Sie es am Ende bereuen, mich nicht gewählt zu haben. Lassen Sie mich ein Wort an meine Mitbewerber richten. Wir sind ja über viele Monate viel in unserem Kreis gewesen, wir haben immer wieder untereinander Kontakt gehabt, es war ein gutes Verhältnis. Ich will nicht behaupten, dass ich nicht hätte die Mehrheit bekommen wollen, aber ich hätte sie jedem von den drei anderen gegönnt. Und das ist jetzt keine Attitüde, das meine ich wirklich so. Ich bedanke mich auch für die Art, wie wir in den letzten Monaten miteinander umgehen konnten. Ich glaube schon, das war ein gutes Stück Demokratie, wie wir es vorgemacht haben, und das ist nie verkehrt, wenn in einem Staat wie dem unsern das so läuft. Meinen herzlichen Dank, meinen Respekt für Sie alle. Ich werde alles daran setzen - obwohl das nur eine Formel ist, wie Sie wissen -, ich werde alles daran setzen, der Bundespräsident aller Deutschen zu sein. Es gibt in unserem Staat, es gibt in unserer Gesellschaft viele Grenzen, die zwischen uns hindurchgehen - Grenzen der Überzeugung, Grenzen der Landsmannschaft, Grenzen der historischen Erfahrung, Grenzen auch des ökonomischen Interesses. Aber, meine Damen und Herren, es gibt unendlich viel, was uns gemeinsam ist, was uns allen gemeinsam ist, was in dem Streit der Parteien, in dem Streit der großen Verbände so nicht immer deutlich wird, was aber deutlich sein muss, damit dieser Staat und diese freiheitliche Gesellschaft fortexistieren und handlungsfähig sein können. Ich werde alles daransetzen, dieses Einende, das, was uns alle in diesem Saale hier und was uns die von uns Vertretenen einig macht, dieses zu betonen. Unser Weg in die Zukunft ist unsicherer, als er es noch vor fünf oder zehn Jahren war. Die Zukunft, in die wir hineingehen, ist schwierig. Sie ist nicht ohne weiteres mehr kalkulierbar, so wie das Fortsetzung auf Seite 2'"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# An example of the first part\n",
    "data['texts'][chained_pair_list[0][0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Herzog will Präsident aller.... Fortsetzung von Seite 1 einige Jahrzehnte in der Vergangenheit war. Aber meine Damen und Herren, wir Deutschen haben die Kraft, wir Europäer haben die Kraft, diesen Weg zu finden und diesen Weg dann auch zu gehen. Daran sollten wir nie zweifeln und das sollten wir auch nie verschweigen. Es gehört zu unserem Leben dazu. Hätten wir diese Hoffnungen nicht mehr, dann könnten wir den Weg, der vor uns liegt, auch nicht gehen. Und das sage ich insbesondere in Irritationen hinein, die sich heute aus den Schwierigkeiten im Gefolge der deutschen Wiedervereinigung ergeben. Ich sage es an die Bürger der früheren Bundesrepublik, die jetzt viele Opfer bringen müssen. Meine Damen und Herren, ich kann es Ihnen nicht ersparen, das immer wieder zu sagen, das ist der Ausgleich für eine Ungerechtigkeit der Weltgeschichte, die durch Zufall an der Elbgrenze gelegt worden ist. Diesseits und jenseits der alten Mauer haben Deutsche gelebt, diesseits und jenseits der alten Mauer haben fleißige, phantasievolle, kreative Menschen gelebt. Und die einen sind schneller vorangekommen und die anderen nicht. Die einen deswegen, weil ihnen die Amerikaner geholfen haben, die wir in dieser Stunde nicht vergessen wollen. Und deswegen, meine Damen und Herren, weil sie ein politisches System hatten, das die Früchte ihres Fleißes gefördert und nicht immer wieder zunichte gemacht und gehindert hat. Das ist eine Ungerechtigkeit der Geschichte gewesen, und diejenigen, die bevorzugt waren im Rahmen dieser Ungerechtigkeit, haben die moralische Pflicht, jetzt den Ausgleich zu schaffen. Und ich sage es an die Menschen in den neuen Bundesländern, Sie müssen begreifen, dass Sie für uns keine Last, sondern dass Sie für uns ein Gewinn sind. Sie bringen unendlich viel ein an Erfahrungen, die wir im Westen nicht hatten, in einer ganz anderen Welt, in der vieles auch humaner gewesen ist als bei uns, in der vieles ganz anders war. Sie bringen Erfahrungen ein aus Ihrem beruflichen Bereich. Es liegen dort Schätze brach, die wieder gehoben werden müssen und auf die diese Menschen sich auch stützen und verlassen können. Wir müssen froh sein und ich bin froh, dass Sie wieder bei uns sind. Frau Präsidentin, meine Damen und Herren, ich will Deutschland so repräsentieren in den nächsten fünf Jahren, wie dieses Deutschland wirklich ist: friedliebend, freiheitsliebend, leistungsstark, um Gerechtigkeit zumindest bemüht, zur Solidarität bereit, tolerant, weltoffen und - was mir fast das wichtigste erscheint, meine Damen und Herren - unverkrampft. Wir Deutschen waren nicht immer unverkrampft. Das wird man nicht behaupten können. Wir haben es einigermaßen geschafft, diese Verkrampfung der späten Nation abzulegen. Es ist heute meine größte Sorge, dass das wieder eintreten könnte, und dem müssen wir vielleicht auch durch manche Eleganz in der Formulierung durch manches lockere Wort, dem müssen wir entgegenwirken. Dieses Deutschland muss in der Welt seine Rolle spielen, aber unverkrampft und ohne gefletschte Zähne. Das ist das allerwichtigste, was wir in diese nächsten Jahrzehnte hineinbringen müssen. Ich will, So wie ich mit einer persönlichen Reminiszenz begonnen habe, mit einer persönlichen Reminiszenz auch zu Ende kommen. Ich habe vor fast 30 Jahren in dieser Stadt in West - Berlin, wie es damals hieß, meine berufliche Laufbahn begonnen. Ich bin dann 25 Jahre unterwegs gewesen, jetzt hat mich Ihr Votum wieder in diese Stadt, die ich nie aufgehört habe zu lieben, zurückgeholt. Ich bedanke mich bei Ihnen. Berlin, ich bin wieder hier!'"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# An example of the second part\n",
    "data['texts'][chained_pair_list[0][1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The indices of the first part of the chained articles.\n",
    "first_part_ind = [pair[0] for pair in chained_pair_list]\n",
    "# The indices of the second part of the chained articles.\n",
    "second_part_ind = [pair[1] for pair in chained_pair_list]\n",
    "# Merge the first and the second parts.\n",
    "data.loc[first_part_ind, 'texts'] = data.loc[first_part_ind, 'texts'].values + \\\n",
    "\" \" + data.loc[second_part_ind, 'texts'].values\n",
    "# Drop the second part.\n",
    "data.drop(data.index[second_part_ind], inplace = True)\n",
    "# The indices of the duplicates.\n",
    "dup_indices = [tup[0] for tup in duplicates]\n",
    "# Drop the duplicates.\n",
    "data.drop(data.index[dup_indices], inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:00:02.831637\n"
     ]
    }
   ],
   "source": [
    "# Calculate the word count of the merged articles.\n",
    "\n",
    "startTime = datetime.now() \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    pool = mp.Pool(NUM_CORE)\n",
    "    count_results = pool.map(count_words_mp.count_words_mp, [text for text in data.loc[first_part_ind, 'texts']]) \n",
    "    pool.close()\n",
    "    pool.join()\n",
    "    \n",
    "print(datetime.now()-startTime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace the \"word_count\" of the merged articles.\n",
    "data.loc[first_part_ind, 'word_count'] = count_results\n",
    "data.reset_index(inplace=True, drop=True) # reset the index of the DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "557078"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the number of articles after concatenating chained articles\n",
    "len(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove fuzzy duplicates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By fuzzy duplicates we understand nearly duplicated articles. These are:\n",
    "\n",
    "* drafts/minor revisions of the articles saved in the database;\n",
    "* slightly changed advertisements which are published several times during a month.\n",
    "\n",
    "We identify 'fuzzy' duplicates using cosine similarity and choose a threshold of 93% based on some visual exploration. Here is the article by Ryan Basques we used as a reference: [Link](https://towardsdatascience.com/a-laymans-guide-to-fuzzy-document-deduplication-a3b3cf9a05a7). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a 'date' column in the data DataFrame\n",
    "data['date'] = pd.to_datetime(data[['year', 'month', 'day']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Duplicates exploration\n",
    "\n",
    "We must first identify and visually inspect the fuzzy duplicates using the `fuzzy_duplicates_test` function before proceeding with their removal. This step ensures the algorithm's performance is as expected, and it accurately identifies articles as fuzzy duplicates based on their cosine similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Required input for the function 'fuzzy_duplicates': a dataframe for each month-year combination.\n",
    "# List with a year\n",
    "inputs_year = []\n",
    "# List with a month\n",
    "inputs_month = []\n",
    "# List with the dataframes containing 'year', 'month', and 'texts' columns\n",
    "inputs_month_year = []\n",
    "for year in list(set(data['year'])):\n",
    "    for month in list(set(data['month'])):\n",
    "        # Exclude December 2018\n",
    "        if year == 2018 and month == 12:\n",
    "            continue\n",
    "        inputs_year.append(year)\n",
    "        inputs_month.append(month)\n",
    "        inputs_month_year.append(data[(data['year'] == year) & (data['month'] == month)][[\"month\", \"year\", \"texts\", \"word_count\", \"date\"]])\n",
    "        \n",
    "inputs = list(zip(inputs_year, inputs_month, inputs_month_year))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:05:34.361982\n"
     ]
    }
   ],
   "source": [
    "startTime = datetime.now() # track time\n",
    "\n",
    "import fuzzy_duplicates_test_all_dpa \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    pool = mp.Pool(NUM_CORE)\n",
    "    dup_intermediate = pool.map(fuzzy_duplicates_test_all_dpa.fuzzy_duplicates_test, inputs) \n",
    "    duplicates = pd.concat(dup_intermediate) \n",
    "    pool.close()\n",
    "    pool.join()\n",
    "    \n",
    "print(datetime.now()-startTime)\n",
    "\n",
    "duplicates.to_csv('duplicates.csv', encoding = 'utf-8-sig', sep = ';')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drop the duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:05:35.367368\n"
     ]
    }
   ],
   "source": [
    "startTime = datetime.now() # track time\n",
    "\n",
    "delete_indices = []\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    pool = mp.Pool(NUM_CORE)\n",
    "    # apply function to all combinations of month-year in parallel\n",
    "    delete_intermediate = pool.map(fuzzy_duplicates_dpa.fuzzy_duplicates, inputs)\n",
    "    delete_indices = delete_indices + delete_intermediate # create one list of indices\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "    \n",
    "print(datetime.now()-startTime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Free memory\n",
    "inputs = None\n",
    "# List of indices corresponding to the duplicated articles\n",
    "delete_indices = [item for sublist in delete_indices for item in sublist]\n",
    "# List of unique indices\n",
    "delete_indices = list(set(delete_indices))\n",
    "# Drop the fuzzy duplicates\n",
    "data.drop(data.index[delete_indices], inplace = True)\n",
    "data.reset_index(inplace=True, drop=True) # reset the index of the DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "552092"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the number of articles after removing fuzzy duplicates\n",
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_csv('fuzzy_duplicates_delete.csv', encoding = 'utf-8-sig', sep = ';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('fuzzy_duplicates_delete.csv', encoding = 'utf-8-sig', sep=';', index_col = 0, dtype = {'newspaper': 'str',\n",
    "                                                                                                 'newspaper_2': 'str',\n",
    "                                                                                                 'quelle_texts': 'str',\n",
    "                                                                                                 'page': 'str',\n",
    "                                                                                                 'rubrics': 'str'})\n",
    "data.page = data.page.fillna('')\n",
    "data.newspaper = data.newspaper.fillna('')\n",
    "data.newspaper_2 = data.newspaper_2.fillna('')\n",
    "data.rubrics = data.rubrics.fillna('')\n",
    "data.quelle_texts = data.quelle_texts.fillna('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SZ-specific problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We encountered a minor issue specific to the SZ dataset while addressing other problems. This issue involves texts containing broken umlauts. To resolve this, we identify such texts and correct their spelling accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:00:15.640010\n"
     ]
    }
   ],
   "source": [
    "startTime = datetime.now()\n",
    "\n",
    "# Find the words like '}ber' and '|ffentlichen' in the articles' texts.\n",
    "if __name__ == \"__main__\":\n",
    "    pool = mp.Pool(NUM_CORE)\n",
    "    problem_umlaut = pool.map(find_umlaut.find_umlaut, [text for text in data['texts']]) \n",
    "    pool.close()\n",
    "    pool.join()\n",
    "\n",
    "print(datetime.now()-startTime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Indices of potentially problematic articles.\n",
    "problem_umlaut_list = [tup[0] for tup in enumerate(problem_umlaut) if tup[1] != []]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "89229\n",
      "89461\n",
      "184123\n"
     ]
    }
   ],
   "source": [
    "index1 = problem_umlaut_list[1]\n",
    "print(index1)\n",
    "index2 = problem_umlaut_list[2]\n",
    "print(index2)\n",
    "index3 = problem_umlaut_list[3]\n",
    "print(index3)\n",
    "\n",
    "# Replace '|' with 'ö' and '}' with 'ü' in articles with indices 89229 and 89461\n",
    "data.loc[index1, 'texts'] = data.loc[index1, 'texts'].replace('|', 'ö').replace('}', 'ü')\n",
    "data.loc[index2, 'texts'] = data.loc[index2, 'texts'].replace('|', 'ö').replace('}', 'ü')\n",
    "\n",
    "# Replace 'Biotechnolog|ieindustrie' with 'Biotechnologieindustrie' in the article with index 184123\n",
    "data.loc[index3, 'texts'] = data.loc[index3, 'texts'].replace('Biotechnolog|ieindustrie', 'Biotechnologieindustrie')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean articles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SZ articles include some text passages that are unlikely to be relevant for either topic modeling or sentiment analysis. We decided to clean the affected articles from these text passages to make the analysis easier for our models.\n",
    "\n",
    "We remove the following information from the texts:\n",
    "\n",
    "   * 1) addresses and Internet addresses\n",
    "   * 2) websites\n",
    "   * 3) copyright information\n",
    "   * 4) references to photo sources\n",
    "   * 5) references to additional information\n",
    "   * 6) telephone and fax numbers\n",
    "   * 7) references to page numbers\n",
    "   * 8) a sentence that is repeated 96 times in one text\n",
    "   * 9) references to extended versions of interviews\n",
    "   * 10) references to contact information\n",
    "   * 11) references to podcasts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:00:45.024807\n"
     ]
    }
   ],
   "source": [
    "startTime = datetime.now()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    pool = mp.Pool(NUM_CORE)\n",
    "    cleaned_articles = pool.map(clean_sz_articles.clean_sz_articles, [text for text in data['texts']]) \n",
    "    pool.close()\n",
    "    pool.join()\n",
    "    \n",
    "print(datetime.now()-startTime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['texts'] = cleaned_articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:00:15.376690\n"
     ]
    }
   ],
   "source": [
    "# Calculate an updated word count of the cleaned articles.\n",
    "\n",
    "startTime = datetime.now()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    pool = mp.Pool(NUM_CORE)\n",
    "    count_results = pool.map(count_words_mp.count_words_mp, [text for text in data['texts']]) \n",
    "    pool.close()\n",
    "    pool.join()\n",
    "    \n",
    "print(datetime.now()-startTime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update the word count in the data frame.\n",
    "data['word_count'] = count_results\n",
    "# Drop short articles.\n",
    "data = data[data['word_count']>=100]\n",
    "data.reset_index(inplace=True, drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "551817"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the number of articles after excluding articles shorter than 100 words\n",
    "len(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exclude tables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we preprocess the news articles by removing tables to minimize noise and emphasize relevant content. We begin by calculating a numerical density metric for each text, which is computed as the ratio of the count of numbers to the total word count (excluding numbers). Texts with a numerical density of at least 20% are considered as candidates for containing tables. We manually examine these texts to identify recurring strings that typically precede tables. Using regular expressions, we exclude the tables based on these strings. Moreover, we delete some text segments predominantly comprising numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:00:09.421605\n"
     ]
    }
   ],
   "source": [
    "# use the 'numeric_articles' function to identify articles with a high share of numbers in them\n",
    "inputs = zip(data['texts'], data['word_count'], itertools.repeat(0.20))\n",
    "\n",
    "startTime = datetime.now()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    pool = mp.Pool(NUM_CORE)\n",
    "    tables = pool.starmap(numeric_articles.numeric_articles, inputs)\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "\n",
    "print(datetime.now()-startTime)\n",
    "data['tables'] = tables\n",
    "tables = data[data.tables == True]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is an example of an article containing a table with emission data for newly listed companies. To remove this table, we apply the following regular expression:  `r'1\\) prozentuale Veränderung gegenüber.{0,}'`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "145683\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Neuemissionen `99. Mehr als die Hälfte liegt im Minus Debütanten an deutschen Börsen seit Anfang dieses Jahres (Reihenfolge nach Kursentwicklung1). Aktien von Film-Firmen verzeichnen die höchsten Gewinne. München, 30. September - Wer bei den Neuemissionen dieses Jahres zu den Erstzeichnern gehörte, machte zumeist Verluste. Gut die Hälfte aller Titel notiert derzeit unter ihrem Ausgabepreis. Die Zahl von Neuemissionen hat schon jetzt einen Höchstwert in der bundesdeutschen Geschichte erreicht. Während im bisherigen Rekordjahr 1998 insgesamt 77 Unternehmen Deutschland als Ort ihrer Börsenpremiere wählten, waren es seit Anfang 1999 bereits 143. Dabei gingen 99 Gesellschaften an den Neuen Markt, 20 in den amtlichen Handel, 12 in den geregelten Markt und 12 weitere an den Freiverkehr oder ähnliche Segmente der Regionalbörsen. Insgesamt 75 Titel notieren derzeit unter jenem Preis, zu dem sie an die Börse kamen. Dabei schneiden die Debütanten am Neuen Markt etwas besser ab als ihre Kollegen an anderen Segmenten. An der sogenannten Wachstumsbörse verzeichnet gut jeder zweite Newcomer Gewinne, während es bei den übrigen nur jeder dritte ist. Wie stark der Neue Markt jedoch an Attraktivität verloren hat, zeigt sich daran, dass zur Jahresmitte noch drei Viertel der dortigen Debütanten Zuwächse verzeichnet hatten. Die jüngste Schwäche zog auch die Gesamtbilanz der Neuemissionen ins Minus; Ende Juni hatten noch zwei von drei aller Zugänge über ihrem Ausgabepreis gelegen. Verlust von knapp 75 Prozent Die höchsten Abschläge sind bei artnet. com (-73,70 Prozent) zu verzeichnen. Für diese Talfahrt gibt es zwei Hauptgründe: Zum einen wurde Altaktionären des Online-Auktionators vorgeworfen, sie hätten das Verbot missachtet, Papiere frühestens sechs Monate nach Erstnotiz zu verkaufen. Zum anderen erweckte das Unternehmen den Eindruck, seine Geschäftszahlen schön zu färben. Zweitgrößter Verlierer sind Gigabell (-66,32 Prozent). Der Internet-Zugangsdienst korrigierte schon kurz nach seinem Börsengang im August seine Geschäftsprognosen für das laufende Jahr. So wird nun ein Viertel weniger Umsatz und ein fast drei Mal so hoher Fehlbetrag erwartet wie ursprünglich angekündigt. An dritter Stelle rangieren die Papiere von Netlife (-65,10 Prozent). Die Softwarefirma hatte ihre Vorhersagen ebenfalls deutlich nach unten geschraubt. Zudem sorgten Pläne der vier Gründungsgesellschafter für Aufruhr. Sie wollten sich ihre Forderungen von über 60 Millionen DM an das Unternehmen auszahlen lassen, die aus unübersichtlichen Transaktionen vor dem Börsenstart entstanden waren. Obwohl das Quartett schließlich auf das Geld verzichtete, blieben die Anleger verstimmt. Die drei Aktien mit den höchsten Gewinnen stammen sämtlich von Film-Produzenten oder -Rechtehändlern: VCL Film + Medien (+493,33 Prozent), RTV Family Entertainment (+363,86 Prozent) und Intertainment (+302,78 Prozent). Doch konnten sich bei diesen Titeln nur Erstzeichner ausnahmslos freuen. Viele Anleger, die erst nach der Premiere zum Zuge kamen, mussten hohe Verluste verbuchen - die aktuellen Kurse des Trios liegen um etwa die Hälfte unter ihren Höchstwerten. Ähnliche Abstürze trafen auch andere der 25 Besten. Sieben von ihnen notieren sogar unter ihrer Erstnotiz. Beate Uhse überrascht Bemerkenswert ist, dass mit Beate Uhse (+156,94 Prozent) ein Wert aus dem amtlichen Handel auf dem sechsten Platz landete. Die anderen Neulinge aus diesem Segment haben unterdurchschnittlich abgeschnitten. Während dies schon in den vergangenen Jahren der Fall war, haben sich Mammut-Emissionen, die üblicherweise schlechter als der Gesamtmarkt laufen, diesmal vergleichsweise gut gehalten. Die zehn größten Platzierungen können sich in etwa mit dem Rest der Debütanten messen: Fünf rangieren im Minus (Agfa, debitel, Maxdata, MVV Energie und Infor), drei im Plus (Software AG, Consors und Medion), die Kurse von Stinnes und Procon liegen exakt auf Höhe der Ausgabepreise. Zumindest ein Neuling wird bis Jahresende noch in diese Zehnergruppe vorstoßen: Epcos, das Joint-Venture von Siemens und Matsushita. Möglicherweise ist dieser Hersteller passiver Bauelemente, bei dem derzeit die Zeichnungsfrist läuft, sogar der größte Debütant des Jahres. Ansonsten scheint es mit dem massiven Ansturm an die Börse erstmal vorbei zu sein. An diesem Freitag wollen zwar noch drei Gesellschaften am Neuen Markt starten (Fabasoft, musicmusicmusic und Norcom), doch für die Zeit danach haben nur acht Aspiranten exakte Daten ihrer Emission genannt. Für September hatte diese Vorschau-Liste ursprünglich rund 30 Unternehmen umfasst. Das schlechte Klima für Neuemissionen schreckte allerdings viele Firmen davon ab, tatsächlich den Sprung an die Kapitalmärkte zu wagen. Martin Reim 1) prozentuale Veränderung gegenüber dem Ausgabepreis; 2) in Euro; 3) geregelter Markt; 4) Neuer Markt; 5) amtlicher Handel; 6) Freiverkehr; 7) Listing ohne Ausgabe zusätzlicher Aktien; Rang ergibt sich aus Veränderung gegenüber Erstnotiz Quellen: Reuters; eigene Berechnungen DIE BESTEN 25 Name Emissions- datumBörsen- segmentEmissions-preis2)Erst- notiz 2)Höchstkurs2) Tiefstkurs2) Kassakurs 30.9.2)Veränd. geg. Emissionspr.Veränd. geg. ErstnotizVCL Film + Medien27.05.GM3)15,0017,00194,50 15,8089,00+493,33%+423,53%RTV Family Entertainment08.06.NM4) 8,30 21,00 96,00 20,0538,50+363,86%+ 83,33%Intertainment08.02.NM18,00 70,00145,00 62,5072,50+302,78%+ 3,57%Highlight Comm.11.05.NM25,50 28,00 98,00 27,0072,00+182,,35%+157,14%Medion26.02.NM85,00150,00283,50128,00225,00+164,71%+ 50,00%Beate Uhse27.05.AH5) 7,20 13,20 28,20 12,5218,50+156,94%+ 40,15%D-Logistics28.04.NM30,00 33,00 94,00 32,5070,00+133,33%+112,12%Jumptec26.03.NM26,00 53,50 95,50 43,0060,00+130,77%+ 12,15%ACG01.06.NM46,00 70,00 153,00 58,00106,00+130,43%+ 51,43%MWG Biotech07.05.NM27,00 27,00 82,00 21,05 61,00+125,93%+125,93%Adva29.03.NM32,00 39,00 87,50 39,00 72,00+125,00%+ 84,62%GFT28.06.NM23,00 44,00 66,00 38,50 47,90+108,26%+ 8,86%Steag Hamatech12.05.NM 9,25 9,35 30,10 7,91 19,00+105,41%+103,21%Kabel New Media15.06.NM 6,15 17,00 41,50 9,80 12,60+104,88%- 25,88%i:FAO01.03.NM21,00 65,00100,5039,50 40,60+ 93,33%- 37,54%Emprise Management Consulting16.06.NM11,50 19,50 27,9016,00 21,70+ 88,70%+ 11,28%Advanced Medien06.08.NM 5,60 14,50 22,50 9,50 10,50+ 8750%- 27,59%Endemann Internet06.08.NM23,00106,00117,2036,10 42,40+ 84,35%- 60,00%Primacom22.02.NM29,00 38,50 58,0029,00 51,50+ 77,59%+ 33,77%Senator Film29.01.NM38,00135,00153,0054,20 67,00+ 76,32%- 50,37%CPU Software19.04.NM26,00 58,00 80,0045,20 45,20+ 73,85%- 22,07%Hancke & Peter25.01.NM14,70 45,00 48,5021,50 25,00+ 70,07%- 44,44%aap Implantate10.05.NM10,00 11,20 18,10 8,50 17,00+ 70,00%+ 51,79%Zapf Creation26.04.AH19,65 19,65 36,0019,30 33,10+ 68,45&+ 68,45%I-D Media17.06.NM23,00 23,00 48,0023,00 38,50+ 67,39%+ 67,39% DIE SCHLECHTESTEN 25 Name Emissions- datumBörsen- segmentEmissions-preis2)Erst- notiz2)Höchstkurs2) Tiefstkurs2) Kassakurs 30.9.2)Veränd. geg. Emissionspr.Veränd. geg. .05.NM46,0048,0066,5010,5012,10-73,70%-74,79%Gigabell11.08.NM38,0033,0037,9012,8012,80-66,32%-61,21%Netlife01.06.NM25,5026,0027,50 8,90 8,90-65,10%-65,77%Bit-by-bit03.05.FV6)-7)21,5025,75 7,50 8,80--59,07%Metabox07.06.NM45,0048,5052,4019,0019,60-56,44%-59,59%Wizcom29.03.NM12,0017,0020,50 5,60 5,75-52,08%-66,18%German Brokers03.05.GM17,0017,9017,95 7,50 8,65-49,12%-51,68%DV-Job02.06.FV11,0011,0011,45 6,10 6,10-44,55%-44,55%Maxdata09.06.NM31,00 31,0032,0017,5017,70-42,90%-42,90%Softmatic01.06.NM20,00 19,5020,2011,5011,50-42,50%-41,03%Cash.Medien21.09.GM11,40 9,8010,40 6,50 6,60-42,11%-32,65%Brain International10.03.NM42,00 55,0066,5024,8025,05-40,36%-54,45%Brain Force Software10.06.NM26,00 .03.NM15,00 17,0032,00 8,30 9,10-39,33%-46,47%Euromed16.06.NM 9,00 8,509,00 5,50 5,50-38,89%-35,29%Infor Business Solution11.05.NM31,00 30,0031,0020,0020,00-35,48%-33,33%Beko Holding14.06.NM62,00 56,0062,9038,5040,00-35,48%-28,57%Shuttlesoft09.06.FV23,00 23,0031,0015,0515,10-34,35%-34,35%On Track Innovations31.08.NM 8,50 7,508,10 5,10 5,60-34,12%-25,33%Kleindienst Datentechnik02.06.NM24,00 24,0024,9515,2516,40-31,67%-31,67%Schnigge17.08.GM41,00 41,2041,2028,0028,50-30,49%-30,83%Elexis Elektroholding04.05.NM 7,00 7,00 8,00 4,90 4,90-30,00%-30,00%Haitec14.06.NM40,00 42,0051,2028,5028,50-28,75%-32,14%P & I Personal & Informatik 07.06.NM12,50 12,5013,00 8,50 8,95-28,40%-28,40%Mania Technologie26.06.NM18,00 18,5019,5012,3013,00-27,78%-27,78%'"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correct_ind = tables[tables.texts.str.contains('Mehr als die Hälfte')].index[0]\n",
    "print(correct_ind)\n",
    "tables.loc[correct_ind,'texts']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clean tables using regular expressions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:00:13.471488\n"
     ]
    }
   ],
   "source": [
    "startTime = datetime.now()\n",
    "if __name__ == \"__main__\":\n",
    "    pool = mp.Pool(NUM_CORE)\n",
    "    remove_tables = pool.map(clean_tables_sz.clean_tables_sz, [text for text in data['texts']]) \n",
    "    pool.close()\n",
    "    pool.join()\n",
    "    \n",
    "print(datetime.now()-startTime)\n",
    "\n",
    "data['texts'] = remove_tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:00:24.099785\n"
     ]
    }
   ],
   "source": [
    "startTime = datetime.now()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    pool = mp.Pool(NUM_CORE)\n",
    "    count_results = pool.map(count_words_mp.count_words_mp, [text for text in data['texts']]) \n",
    "    pool.close()\n",
    "    pool.join()\n",
    "    \n",
    "print(datetime.now()-startTime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update the content of the column \"word_count\"\n",
    "data['word_count'] = count_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "551604\n"
     ]
    }
   ],
   "source": [
    "# remove articles with less than 100 words, while retaining those that contain the string 'Konjunktur-Kompass'\n",
    "data = data[(data['word_count']>=100) | (data.texts.str.contains('Konjunktur-Kompass'))]\n",
    "data = data.sort_values(['year', 'month', 'day'], ascending=[True, True, True]) # sort the data in chronological order\n",
    "data.reset_index(inplace=True, drop=True)\n",
    "# the number of articles after excluding articles shorter than 100 words\n",
    "print(len(data))\n",
    "del data['tables']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Identify articles that predominantly consist of numbers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Articles that predominantly consist of numerical data present challenges for sentiment or topic analysis as, once numbers are removed, they tend to contain limited information. Therefore, we remove articles with a numerical density of 50% or greater."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:00:11.328068\n"
     ]
    }
   ],
   "source": [
    "# use the 'numeric_articles' function to identify economic articles with a high share of numbers in them\n",
    "inputs = zip(data['texts'], data['word_count'], itertools.repeat(0.50))\n",
    "\n",
    "startTime = datetime.now()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    pool = mp.Pool(NUM_CORE)\n",
    "    numeric_list = pool.starmap(numeric_articles.numeric_articles, inputs)\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "\n",
    "print(datetime.now()-startTime)\n",
    "data['numeric'] = numeric_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Die Ergebnisse in den Wahlkreisen. Bei der Landtagswahl in Niedersachsen hat jeder Wähler wie bei der Bundestagswahl zwei Stimmen. Mit der Erststimme wird der Wahlkreiskandidat und mit der Zweitstimme die Partei gewählt. Der Anteil der Zweitstimmen legt gemäß Verhältniswahlrecht die Zahl der Sitze einer jeden Partei im Landtag fest. Die folgenden Werte geben den prozentualen Anteil der Erststimmen wieder. Die Ergebnisse von 1990 stehen in Klammern. 1 Braunschweig - Nordost SPD 41,0 (43,4), CDU 39,7 (44,3), FDP 3,8 (5,3), Grüne 10,5 (7,0), REP 2,7 (0,0), STATT Partei 2,3; gew. Isolde Saalmann (SPD), bisher Wolfgang Sehrt (CDU). 2 Braunschweig - Südost SPD 41,9 (43,7), CDU 41,0 (46,2), FDP 3,5 (4,8), Grüne 7,7 (5,3), REP 3,6 (0,0), STATT Partei 2,2; gew. Klaus - Peter Bachmann (SPD), bisher Heiner Herbst (CDU). 3 Braunschweig - Südwest SPD 49,0 (49,7), CDU 32,5 (40,1), FDP 2,8 (4,2), Grüne 9,4 (6,0), REP 4,1 (0,0), STATT Partei 2,2; gew. Jürgen Buchheister (SPD), bisher Friedhelm Schuricht (SPD). 4 Braunschweig - Nordwest SPD 48,6 (49,4), CDU 34,7 (41,3), FDP 3,3 (3,9), Grüne 7,7 (5,3), REP 3,3 (0,0), STATT Partei 2,3; gew. Gerhard Glogowski (SPD) wie bisher. 5 Peine SPD 54,0 (51,5), CDU 34,3 (39,7), FDP 3,3 (3,7), Grüne 6,2 (5,1), REP 0,0 (0,0), STATT Partei 0,0; gew. Ulrich Biel (SPD) wie bisher. 6 Peine - Land SPD 52,4 (49,1), CDU 34,9 (40,3), FDP 3,0 (3,5), Grüne 7,6 (5,3), REP 0,0 (1,7), STATT Partei 2,1; gew. Rosemarie Tinius (SPD), bisher Werner Kirschner (SPD). 7 Gifhorn - Süd SPD 40,4 (41,5), CDU 42,3 (49,4), FDP 3,2 (4,3), Grüne 7,0 (4,8), REP 4,2 (0,0), STATT Partei 1,5; gew. Helmut Kuhlmann (CDU) wie bisher. 8 Gifhorn - Nord SPD 45,1 (41,2), CDU 41,8 (46,7), FDP 6,1 (7,5), Grüne 5,1 (3,6), REP 0,0 (0,0), STATT Partei 0,0; gew. Marion Lau (SPD), bisher Wilfried Wolter (CDU). 9 Helmstedt SPD 48,0 (43,8), CDU 40,2 (47,5), FDP 2,4 (4,7), Grüne 4,9 (3,0), REP 3,1 (0,0), STATT Partei 0,0; gew. Hans - Hermann Wendhausen (SPD), bisher Rolf Reinemann (CDU). 10 Königinmutter SPD 47,6 (43,2), CDU 40,2 (47,6), FDP 3,5 (3,3), Grüne 6,7 (4,2), REP 0,0 (0,0), STATT Partei 0,0; gew. Ingolf Viereck (SPD), bisher Heinrich - Wilhelm Ronsöhr (CDU). 11 Wolfsburg SPD 48,6 (46,4), CDU 40,2 (46,0), FDP 4,1 (2,8), Grüne 5,1 (3,5), REP 0,0 (0,0), STATT Partei 0,0; gew. Irmela Hammelstein (SPD) wie bisher. 12 Wolfenbüttel SPD 44,0 (42,9), CDU 41,0 (48,4), FDP 3,5 (3,8), Grüne 5,7 (4,1), REP 3,3 (0,0), STATT Partei 2,3; gew. Gerhild Jahn (SPD), bisher Ernst - Henning Jahn (CDU). 13 Schöppenstedt SPD 52,8 (52,9), CDU 35,0 (40,8), FDP 1,8 (2,8), Grüne 4,5 (3,5), REP 4,1 (0,0), STATT Partei 1,8; gew. Karl - Heinz Mühe (SPD) wie bisher. 14 Salzgitter SPD 52,6 (54,6), CDU 33,6 (38,5), FDP 2,2 (3,0), Grüne 4,7 (3,9), REP 4,9 (0,0), STATT Partei 1,9; gew. Peter - Jürgen Schneider (SPD) wie bisher. 15 Seesen SPD 50,3 (48,5), CDU 38,3 (41,6), FDP 4,0 (4,8), Grüne 6,7 (5,1), REP 0,0 (0,0), STATT Partei 0,0; gew. Peter Kopischke (SPD) wie bisher. 16 Goslar SPD 48,5 (46,4), CDU 36,7 (42,7), FDP 4,4 (6,4), Grüne 6,9 (4,5), REP 3,5 (0,0), STATT Partei 0,0; gew. Sigmar Gabriel (SPD) wie bisher. 17 Harz SPD 43,7 (42,2), CDU 45,0 (47,5), FDP 3,6 (4,9), Grüne 7,7 (5,4), REP 0,0 (0,0), STATT Partei 0,0; gew. Jürgen Dorka (CDU) wie bisher. 18 Osterode SPD 52,7 (49,8), CDU 37,9 (41,4), FDP 3,7 (5,2), Grüne 5,7 (3,6), REP 0,0 (0,0), STATT Partei 0,0; gew. Heinz - Wolfgang Domöse (SPD) wie bisher. 19 Duderstadt SPD 38,0 (34,3), CDU 48,8 (59,6), FDP 3,3 (2,9), Grüne 5,1 (3,1), REP 4,8 (0,0), STATT Partei 0,0; gew. Lothar Koch (CDU), bisher Willi Göring (CDU). 20 Münden SPD 52,3 (52,8), CDU 35,1 (36,4), FDP 3,9 (5,3), Grüne 8,7 (5,5), REP 0,0 (0,0), STATT Partei 0,0; gew. Wolfgang Senff (SPD) wie bisher. 21 Göttingen SPD 42,8 (49,6), CDU 29,7 (32,6), FDP 4,5 (6,9), Grüne 18,7 (10,9), REP 2,7 (0,0), STATT Partei 0,0; gew. Hulle Hartwig (SPD), bisher Gudrun Hartwig (SPD). 22 Göttingen - Land SPD 47,7 (51,1), CDU 33,2 (35,9), FDP 4,6 (6,0), Grüne 10,2 (7,1), REP 2,8 (0,0), STATT Partei 0,0; gew. Thomas Oppermann (SPD) wie bisher. 23 Northeim SPD 53,1 (52,5), CDU 33,2 (38,4), FDP 4,1 (4,8), Grüne 5,6 (3,9), REP 4,1 (0,0), STATT Partei 0,0; gew. Axel Endlein (SPD) wie bisher. 24 Einbeck SPD 53,0 (52,3), CDU 32,3 (37,7), FDP 3,7 (6,6), Grüne 4,5 (3,4), REP 4,0 (0,0), STATT Partei 0,0; gew. Uwe Schwarz (SPD) wie bisher. 25 Holzminden SPD 52,5 (51,8), CDU 34,7 (35,9), FDP 6,7 (7,6), Grüne 6,1 (4,0), REP 0,0 (0,0), STATT Partei 0,0; gew. Ernst - August Wolf (SPD), bisher Willi Waike (SPD). 26 Bad Münder SPD 55,3 (54,6), CDU 31,5 (36,1), FDP 4,7 (5,2), Grüne 6,3 (4,1), REP 0,0 (0,0), STATT Partei 2,2; gew. Wolfgang Schultze (SPD) wie bisher. 27 Bad Pyrmont SPD 49,4 (46,3), CDU 39,7 (43,3), FDP 3,6 (4,9), Grüne 7,2 (4,6), REP 0,0 (0,0), STATT Partei 0,0; gew. Christel Thielke (SPD) wie bisher. 28 Hameln SPD 50,0 (47,2), CDU 34,2 (38,4), FDP 4,2 (6,6), Grüne 7,6 (6,7), REP 0,0 (0,0), STATT Partei 2,5; gew. Klaus Nolting (SPD) wie bisher. 29 Alfeld SPD 58,2 (55,8), CDU 32,5 (35,7), FDP 3,3 (4,7), Grüne 4,9 (3,8), REP 0,0 (0,0), STATT Partei 0,0; gew. Jürgen Lanclee (SPD), bisher Friedrich Deike (SPD). 30 Bad Salzdetfurth SPD 51,4 (50,9), CDU 35,1 (41,7), FDP 3,0 (3,2), Grüne 5,0 (3,6), REP 4,4 (0,0), STATT Partei 0,0; gew. Reiner Wegner (SPD), bisher Jochen Patzschke (SPD). 31 Hildesheim SPD 48,2 (46,9), CDU 36,4 (42,6), FDP 3,5 (4,9), Grüne 5,7 (5,0), REP 4,8 (0,0), STATT Partei 0,0; gew. Monika Griefahn (SPD), bisher Leonore Aürbach (SPD). 32 Sarstedt SPD 47,8 (45,7), CDU 41,3 (46,2), FDP 3,1 (3,9), Grüne 5,9 (4,2), REP 0,0 (0,0), STATT Partei 0,0; gew. Werner Buss (SPD), bisher Heinrich Biermann (CDU). 33 Hannover - Mitte SPD 43,8 (46,5), CDU 31,9 (38,5), FDP 4,7 (6,8), Grüne 12,3 (8,0), REP 3,9 (0,0), STATT Partei 2,3; gew. Heidrun Alm - Merk (SPD) wie bisher. 34 Hannover - List SPD 52,8 (55,1), CDU 25,0 (33,1), FDP 3,7 (3,7), Grüne 11,7 (8,0), REP 4,5 (0,0), STATT Partei 0,0; gew. Herbert Schmalstieg (SPD) wie bisher. 35 Hannover - Nordwest SPD 49,1 (52,5), CDU 31,0 (39,3), FDP 3,5 (4,1), Grüne 5,7 (4,1), REP 8,3 (0,0), STATT Partei 0,0; gew. Rolf Wernstedt (SPD) wie bisher. 36 Hannover - Nordost SPD 46,9 (46,9), CDU 35,9 (41,1), FDP 5,0 (7,3), Grüne 6,5 (4,7), REP 5,7 (0,0), STATT Partei 0,0; gew. Axel Ölaue (SPD) wie bisher. 37 Hannover - Südost SPD 40,1 (42,1), CDU 39,5 (44,6), FDP 5,8 (8,1), Grüne 7,8 (5,1), REP 4,1 (0,0), STATT Partei 1,7; gew. Sigrid Leuschner (SPD), bisher Christoph von Bredow (CDU). 38 Hannover - Linden SPD 52,1 (56,9), CDU 23,3 (28,8), FDP 3,1 (3,7), Grüne 14,9 (10,6), REP 5,0 (0,0), STATT Partei 0,0; gew. Wolfgang Jüttner (SPD) wie bisher. 39 Hannover - Limmer SPD 48,8 (53,2), CDU 31,2 (35,2), FDP 3,2 (5,0), Grüne 8,6 (6,7), REP 5,6 (0,0), STATT Partei 0,0; gew. Christa Elsner - Solar (SPD), bisher Michael Auditor (SPD). 40 Laatzen SPD 47,3 (48,4), CDU 35,8 (42,1), FDP 3,6 (4,7), Grüne 5,4 (4,3), REP 4,8 (0,0), STATT Partei 2,9; gew. Gertraude Kruse (SPD) wie bisher. 41 Lehrte SPD 51,7 (47,0), CDU 36,4 (43,2), FDP 4,5 (5,5), Grüne 5,1 (4,0), REP 0,0 (0,0) STATT Partei 0,0; gew. Gerhard Schröder (SPD) wie bisher. 42 Burgdorf SPD 45,1 (43,2), CDU 39,2 (45,3), FDP 5,9 (6,2), Grüne 6,3 (5,3), REP 0,0 (0,0), STATT Partei 0,0; gew. Michael Stolze (SPD), bisher Lutz von der Heide (CDU). 43 Langenhagen SPD 45,9 (44,6), CDU 39,2 (45,4), FDP 3,9 (5,3), Grüne 5,7 (4,7), REP 4,8 (0,0), STATT Partei 0,0; gew. Peter Fischer (SPD), bisher Jochen Haselbacher (CDU). 44 Neustadt SPD 44,3 (47,4), CDU 37,5 (42,0), FDP 4,4 (5,4), Grüne 5,6 (4,9), REP 5,5 (0,0), STATT Partei 0,0; gew. Dieter Wallraff (SPD) wie bisher. 45 Wunstorf SPD 48,1 (46,3), CDU 33,5 (39,1), FDP 7,1 (8,8), Grüne 5,5 (4,6), REP 4,6 (0,0), STATT Partei 0,0; gew. Heinrich Aller (SPD) wie bisher. 46 Springe SPD 48,5 (48,4), CDU 35,0 (43,1), FDP 4,0 (3,9), Grüne 6,3 (4,6), REP 0,0 (0,0), STATT Partei 0,0; gew. Udo - Hans Mientus (SPD) wie bisher. 47 Schaumburg SPD 50,3 (51,0), CDU 37,0 (41,1), FDP 3,0 (3,7), Grüne 4,8 (3,4), REP 4,9 (0,0), STATT Partei 0,0; gew. Heiner Bartling (SPD) wie bisher. 48 Bückeburg SPD 52,4 (52,2), CDU 33,4 (39,6), FDP 3,6 (4,0), Grüne 5,0 (4,2), REP 4,2 (0,0), STATT Partei 0,0; gew. Alfred Reckmann (SPD) wie bisher. 49 Nienburg - Süd SPD 41,8 (43,0), CDU 45,3 (48,4), FDP 3,2 (3,8), Grüne 5,8 (3,7), REP 3,5 (0,0), STATT Partei 0,0; gew. Willi Heineking (CDU) wie bisher. 50 Nienburg - Nord SPD 46,4 (45,3), CDU 39,0 (44,6), FDP 4,2 (5,3), Grüne 6,6 (4,7), REP 3,7 (0,0), STATT Partei 0,0; gew. Peter Gruber (SPD) wie bisher. 51 Diepholz SPD 42,7 (39,7), CDU 38,5 (39,8), FDP 8,5 (16,6), Grüne 5,4 (3,4), REP 4,3 (0,0), STATT Partei 0,0; gew. Günter Schlüterbusch (SPD), bisher Karl - Heinz Klare (CDU). 52 Syke SPD 46,9 (46,2), CDU 34,5 (42,0), FDP 7,0 (5,5), Grüne 8,3 (5,7), REP 3,3 (0,0), STATT Partei 0,0; gew. Ilse Lübben (SPD) wie bisher. 53 Osterholz - Scharmbeck SPD 56,1 (55,7), CDU 28,7 (34,4), FDP 3,8 (3,5), Grüne 9,4 (6,3), REP 0,0 (0,0), STATT Partei 0,0; gew. Uwe Bruns (SPD) wie bisher. 54 Achim SPD 49,9 (48,5), CDU 31,0 (37,1), FDP 6,7 (6,2), Grüne 9,3 (6,7), REP 3,1 (1,3), STATT Partei 0,0; gew. Christoph Rippich (SPD) wie bisher. 55 Verden SPD 47,3 (45,2), CDU 39,2 (43,1), FDP 4,4 (4,6), Grüne 9,0 (5,3), REP 0,0 (1,7), STATT Partei 0,0; gew. Christina Übermanne (SPD) wie bisher. 56 Fallingbostel SPD 48,7 (46,0), CDU 35,5 (44,1), FDP 3,8 (4,9), Grüne 7,1 (4,4), REP 4,0 (0,0), STATT Partei 0,0; gew. Peter Rabe (SPD) wie bisher. 57 Soltau SPD 43,2 (40,7), CDU 42,8 (50,4), FDP 3,3 (4,0), Grüne 5,4 (4,3), REP 4,5 (0,0), STATT Partei 0,0; gew. Dieter Möhrmann (SPD), bisher Gustav Isernhagen (CDU). 58 Celle - Land SPD 39,5 (39,4), CDU 42,2 (50,2), FDP 5,2 (6,2), Grüne 4,8 (3,5), REP 5,2 (0,0), STATT Partei 0,0; gew. Albert Heinemann (CDU) wie bisher. 59 Celle SPD 39,1 (39,0), CDU 43,3 (49,9), FDP 5,4 (6,5), Grüne 5,4 (3,5), REP 5,5 (0,0), STATT Partei 0,0; gew. Otto Stumpf (CDU), bisher Edzard Blanke (CDU). 60 Uelzen SPD 43,7 (43,5), CDU 42,9 (45,5), FDP 3,2 (5,1), Grüne 6,8 (4,6), REP 3,5 (0,0), STATT Partei 0,0; gew. Jacques Vollständiger (SPD), bisher Hans - Heinrich Ottens (CDU). 61 Lüchow - Dannenberg SPD 44,2 (43,1), CDU 45,8 (43,4), FDP 3,4 (5,8), Grüne 6,5 (5,5), REP 0,0 (2,2), STATT Partei 0,0; gew. Klaus Wojahn (CDU), bisher Kurt - Dieter Grill (CDU). 62 Lüneburg - Land SPD 42,8 (42,3), CDU 36,2 (46,2), FDP 3,9 (4,4), Grüne 8,3 (5,4), REP 3,6 (0,0), STATT Partei 5,2; gew. Wolfgang Schurreit (SPD), bisher Wilhelm Martens (CDU). 63 Lüneburg SPD 43,2 (45,3), CDU 32,2 (41,1), FDP 5,1 (5,7), Grüne 9,6 (7,1), REP 3,6 (0,0), STATT Partei 6,3; gew. Uwe Inselmann (SPD) wie bisher. 64 Winsen SPD 40,2 (40,0), CDU 36,7 (46,6), FDP 3,8 (6,1), Grüne 7,2 (5,1), REP 5,0 (1,9), STATT Partei 7,1; gew. Uwe Harden (SPD), bisher Franz Röhre (CDU). 65 Seevetal SPD 41,4 (40,4), CDU 38,9 (44,9), FDP 5,7 (9,0), Grüne 8,9 (5,0), REP 5,1 (0,0), STATT Partei 0,0; gew. Brigitte Somfleth (SPD), bisher Norbert Böhme (CDU). 66 Buchholz SPD 39,6 (38,7), CDU 38,1 (46,1), FDP 5,2 (6,5), Grüne 9,8 (7,1), REP 4,5 (0,0), STATT Partei 0,0; gew. Silva Seeler (SPD), bisher Gebhard Müller (CDU). 67 Rotenburg SPD 43,6 (42,6), CDU 39,5 (44,8), FDP 2,6 (2,7), Grüne 6,8 (4,6), REP 5,6 (5,3), STATT Partei 1,9; gew. Bodo Räte (SPD), bisher Hans - Cord Graf von Bothmer (CDU). 68 Bremervörde SPD 39,3 (38,1), CDU 45,6 (52,4), FDP 3,6 (4,2), Grüne 10,5 (4,5), REP 0,0 (0,0), STATT Partei 0,0; gew. Hans - Heinrich Ehlen (CDU), bisher Wilhelm Brunkhorst (CDU). 69 Buxtehude SPD 44,8 (44,0), CDU 39,6 (44,1), FDP 3,9 (5,6), Grüne 6,8 (4,5), REP 0,0 (0,0), STATT Partei 3,3; gew. Monika Wortärmer - Zimmermann (SPD), bisher Heinrich Augustin (CDU). 70 Stade SPD 47,2 (47,6), CDU 35,5 (42,6), FDP 2,5 (3,7), Grüne 8,0 (5,0), REP 3,0 (0,0), STATT Partei 3,8; gew. Heinrich von Borstel (SPD) wie bisher. 71 Hadeln SPD 43,6 (46,0), CDU 40,4 (46,1), FDP 5,3 (3,6), Grüne 5,2 (4,3), REP 3,0 (0,0), STATT Partei 2,4; gew. Birgit Meyn - Horeis (SPD), bisher Martin Döschen (CDU). 72 Cuxhaven SPD 49,4 (46,0), CDU 31,2 (43,0), FDP 2,9 (3,1), Grüne 5,4 (5,9), REP 4,1 (1,8), STATT Partei 6,5; gew. Hans - Heinrich Eilers (SPD) wie bisher. 73 Wesermünde SPD 48,8 (50,3), CDU 33,9 (41,0), FDP 3,1 (3,6), Grüne 6,7 (5,1), REP 3,7 (0,0), STATT Partei 3,1; gew. Edda Gödel (SPD) wie bisher. 74 Oldenburg - Süd SPD 45,1 (46,8), CDU 30,4 (35,5), FDP 4,2 (5,6), Grüne 16,1 (11,7), REP 0,0 (0,0), STATT Partei 2,3; gew. Wolfgang Wulf (SPD), bisher Werner Rettig (SPD). 75 Oldenburg - Nord SPD 51,1 (54,1), CDU 28,4 (32,5), FDP 4,3 (5,5), Grüne 11,4 (7,5), REP 2,3 (0,0), STATT Partei 2,5; gew. Horst Milde (SPD) wie bisher. 76 Wesermarsch SPD 58,9 (56,5), CDU 27,9 (31,9), FDP 5,1 (5,7), Grüne 7,4 (6,0), REP 0,0 (0,0), STATT Partei 0,0; gew. Hans - Joachim Beckmann (SPD) wie bisher. 77 Delmenhorst SPD 54,4 (52,7), CDU 29,4 (36,7), FDP 5,0 (4,5), Grüne 6,2 (6,0), REP 4,9 (0,0), STATT Partei 0,0; gew. Harald Groth (SPD) wie bisher. 78 Oldenburg - Land SPD 45,6 (44,9), CDU 34,3 (40,4), FDP 8,0 (8,5), Grüne 7,8 (5,7), REP 0,0 (0,0), STATT Partei 3,6; gew. Hans - Christian Schack (SPD) wie bisher. 79 Cloppenburg SPD 26,4 (23,5), CDU 63,5 (66,9), FDP 3,1 (4,6), Grüne 4,8 (5,1), REP 0,0 (0,0), STATT Partei 2,1; gew. Hans Evelslage (CDU) wie bisher. 80 Vechta SPD 30,4 (26,1), CDU 58,7 (64,6), FDP 6,2 (5,9), Grüne 4,7 (3,4), REP 0,0 (0,0), STATT Partei 0,0; gew. Friedhelm Biestmann (CDU), bisher Clemens - August Krapp (CDU). 81 Melle SPD 40,6 (41,4), CDU 47,9 (49,7), FDP 4,0 (4,4), Grüne 6,4 (4,4), REP 0,0 (0,0), STATT Partei 0,0; gew. Josef Stock (CDU) wie bisher. 82 Bissendorf SPD 40,7 (37,0), CDU 48,6 (51,3), FDP 4,4 (6,7), Grüne 4,5 (5,1), REP 0,0 (0,0), STATT Partei 0,0; gew. Georg Schirmbeck (CDU) wie bisher. 83 Osnabrück - Ost SPD 49,6 (50,9), CDU 39,4 (40,4), FDP 2,9 (4,0), Grüne 7,4 (4,7), REP 0,0 (0,0), STATT Partei 0,0; gew. Eckhard Fasold (SPD) wie bisher. 84 Osnabrück - West SPD 42,9 (45,8), CDU 44,3 (41,6), FDP 3,5 (5,5), Grüne 9,3 (7,1), REP 0,0 (0,0), STATT Partei 0,0; gew. Christian Wulff (CDU), bisher Karin Detert - Weber (SPD). 85 Georgsmarienhütte SPD 41,6 (41,1), CDU 47,6 (50,1), FDP 2,8 (3,2), Grüne 6,7 (5,2), REP 0,0 (0,0), STATT Partei 0,0; gew. Irmgard Vogelsang (CDU) wie bisher. 86 Bersenbrück SPD 43,2 (41,8), CDU 44,9 (48,4), FDP 5,2 (5,0), Grüne 6,7 (4,4), REP 0,0 (0,0), STATT Partei 0,0; gew. Reinhold Tönen (CDU), bisher Burkhard Ritz (CDU). 87 Nordhorn SPD 42,8 (44,5), CDU 45,7 (47,1), FDP 4,1 (4,0), Grüne 5,7 (4,0), REP 0,0 (0,0), STATT Partei 0,0; gew. Friedrich Kethorn (CDU) wie bisher. 88 Lingen SPD 32,1 (30,7), CDU 58,4 (61,7), FDP 2,8 (3,0), Grüne 4,4 (3,2), REP 0,0 (0,0), STATT Partei 0,0; gew. Heinz Rolfes (CDU), bisher Werner Remmers (CDU). 89 Meppen SPD 32,4 (29,0), CDU 57,1 (63,5), FDP 3,8 (3,6), Grüne 5,0 (3,6), REP 0,0 (0,0), STATT Partei 1,7; gew. Heinz Jansen (CDU) wie bisher. 90 Papenburg SPD 27,5 (22,9), CDU 60,6 (64,2), FDP 6,1 (7,5), Grüne 3,8 (4,1), REP 0,0 (0,0), STATT Partei 2,0; gew. Bernd Busemann (CDU), bisher Walter Remmers (CDU). 91 Leer SPD 49,3 (52,8), CDU 30,1 (38,4), FDP 2,3 (2,9), Grüne 6,4 (5,3), REP 1,9 (0,0), STATT Partei 1,3; gew. Günther Bökhoff (SPD) wie bisher. 92 Leer - Borkum SPD 61,0 (58,8), CDU 28,7 (32,7), FDP 2,5 (2,4), Grüne 5,1 (5,5), REP 0,0 (0,0), STATT Partei 2,8; gew. Helmut Collman (SPD) wie bisher. 93 Emden SPD 66,0 (67,1), CDU 20,7 (24,9), FDP 3,7 (2,8), Grüne 7,7 (5,2), REP 0,0 (0,0), STATT Partei 1,9; gew. Alwin Brinkmann (SPD), bisher Johann Bruns (SPD). 94 Aurich SPD 57,8 (59,6), CDU 29,5 (31,0), FDP 2,9 (2,7), Grüne 6,1 (6,8), REP 0,0 (0,0), STATT Partei 3,2; gew. Hermann Bontjer (SPD) wie bisher. 95 Norden SPD 62,5 (59,7), CDU 25,3 (28,9), FDP 3,4 (4,3), Grüne 6,6 (7,1), REP 0,0 (0,0), STATT Partei 2,1; gew. Hinrich Swieter (SPD) wie bisher. 96 Wittmund SPD 48,4 (49,8), CDU 38,0 (42,4), FDP 3,3 (3,3), Grüne 6,2 (4,5), REP 0,0 (0,0), STATT Partei 1,6; gew. Günter Peters (SPD), bisher Udo Könne (SPD). 97 Ammerland SPD 44,5 (43,3), CDU 38,5 (44,3), FDP 5,4 (6,7), Grüne 6,7 (5,1), REP 3,0 (0,0), STATT Partei 1,6; gew. Wolf Weber (SPD), bisher Johann Hinrichs (CDU). 98 Varel SPD 59,9 (57,9), CDU 27,7 (30,2), FDP 3,8 (5,9), Grüne 6,0 (4,6), REP 0,0 (0,0), STATT Partei 1,3; gew. Karl - Heinz Funke (SPD) wie bisher. 99 Jever SPD 53,3 (53,1), CDU 32,8 (34,4), FDP 4,0 (5,8), Grüne 7,1 (5,5), REP 0,0 (0,0), STATT Partei 2,8; gew. Bernd Theilen (SPD) wie bisher. 100 Wilhelmshaven SPD 54,0 (51,9), CDU 26,5 (36,0), FDP 3,6 (4,9), Grüne 7,4 (7,2), REP 5,2 (0,0), STATT Partei 2,8; gew. Wilfried Adam (SPD) wie bisher.'"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# inspect example article with high share of numbers\n",
    "data[data.numeric == True]['texts'].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Samstag / Sonntag, 16. / 17. April 1994 Samstag / Sonntag, 16. / 17. April 1994 Samstag / Sonntag, 16. / 17. April 1994 Samstag / Sonntag, 16. / 17. April 1994 Samstag / Sonntag, 16. / 17. April 1994 Samstag / Sonntag, 16. / 17. April 1994 Samstag / Sonntag, 16. / 17. April 1994 Samstag / Sonntag, 16. / 17. April 1994 Samstag / Sonntag, 16. / 17. April 1994 Samstag / Sonntag, 16. / 17. April 1994 Samstag / Sonntag, 16. / 17. April 1994 Samstag / Sonntag, 16. / 17. April 1994 Samstag / Sonntag, 16. / 17. April 1994 Samstag / Sonntag, 16. / 17. April 1994 Samstag / Sonntag, 16. / 17. April 1994 Samstag / Sonntag, 16. / 17. April 1994 Samstag / Sonntag, 16. / 17. April 1994 Samstag / Sonntag, 16. / 17. April 1994 Samstag / Sonntag, 16. / 17. April 1994 Samstag / Sonntag, 16. / 17. April 1994 Samstag / Sonntag, 16. / 17. April 1994 Samstag / Sonntag, 16. / 17. April 1994 Samstag / Sonntag, 16. / 17. April 1994 Samstag / Sonntag, 16. / 17. April 1994 Samstag / Sonntag, 16. / 17. April 1994 Samstag / Sonntag, 16. / 17. April 1994 Samstag / Sonntag, 16. / 17. April 1994 Samstag / Sonntag, 16. / 17. April 1994 Samstag / Sonntag, 16. / 17. April 1994 Seite 138 / Süddeutsche Zeitung Nr. 87 Seite 140 / Süddeutsche Zeitung Nr. 87 Seite 142 / Süddeutsche Zeitung Nr. 87 Seite 144 / Süddeutsche Zeitung Nr. 87 Seite 146 / Süddeutsche Zeitung Nr. 87 Seite 148 / Süddeutsche Zeitung Nr. 87 Seite 150 / Süddeutsche Zeitung Nr. 87 Seite 152 / Süddeutsche Zeitung Nr. 87 Seite 154 / Süddeutsche Zeitung Nr. 87 Seite 156 / Süddeutsche Zeitung Nr. 87 Seite 158 / Süddeutsche Zeitung Nr. 87 Seite 160 / Süddeutsche Zeitung Nr. 87 Süddeutsche Zeitung Nr. 87 / Seite 137 Süddeutsche Zeitung Nr. 87 / Seite 139 Süddeutsche Zeitung Nr. 87 / Seite 141 Süddeutsche Zeitung Nr. 87 / Seite 143 Süddeutsche Zeitung Nr. 87 / Seite 145 Süddeutsche Zeitung Nr. 87 / Seite 147 Süddeutsche Zeitung Nr. 87 / Seite 149 Süddeutsche Zeitung Nr. 87 / Seite 151 Süddeutsche Zeitung Nr. 87 / Seite 153 Süddeutsche Zeitung Nr. 87 / Seite 155 Süddeutsche Zeitung Nr. 87 / Seite 157 Süddeutsche Zeitung Nr. 87 / Seite 159'"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[data.numeric == True]['texts'].iloc[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "92"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data[data.numeric == True])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "551512\n"
     ]
    }
   ],
   "source": [
    "# drop articles predominantly consisting of numbers\n",
    "data = data[data.numeric == False]\n",
    "del data['numeric']\n",
    "data.reset_index(inplace=True, drop=True)\n",
    "# the number of articles after removing articles that predominantly consist of numbers\n",
    "print(len(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Identify articles that predominantly consist of names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We eliminate texts with a name density of at least 15% (relative to the total word count, excluding numbers) as part of our pre-processing pipeline. This exclusion is important to guarantee that the remaining articles contain sufficient content for effective topic analysis, as the removal of common German names is a standard pre-processing step in LDA model estimation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dictionary containing common German first and last names\n",
    "with open(\"names.txt\", \"r\", encoding=\"utf-8-sig\") as f:\n",
    "    names_list = f.read().splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:01:10.934713\n"
     ]
    }
   ],
   "source": [
    "inputs = zip(data['texts'], data['word_count'], itertools.repeat(names_list))\n",
    "\n",
    "startTime = datetime.now()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    pool = mp.Pool(NUM_CORE)\n",
    "    names_result = pool.starmap(count_names.count_names, inputs)\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "\n",
    "print(datetime.now()-startTime)\n",
    "\n",
    "data['names'] = names_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Abraham B. Jehoschua Die Manis Roman Aus dem Hebräischen von Ruth Achlama C R. Piper GmbH & Co. KG München, Zürich > Abraham B. Jehoschua Die Manis Roman Aus dem Hebräischen von Ruth Achlama C R. Piper GmbH & Co. KG München, Zürich > Abraham B. Jehoschua Die Manis Roman Aus dem Hebräischen von Ruth Achlama C R. Piper GmbH & Co. KG München, Zürich > Abraham B. Jehoschua Die Manis Roman Aus dem Hebräischen von Ruth Achlama C R. Piper GmbH & Co. KG München, Zürich > Abraham B. Jehoschua Die Manis Roman Aus dem Hebräischen von Ruth Achlama C R. Piper GmbH & Co. KG München, Zürich > Abraham B. Jehoschua Die Manis Roman Aus dem Hebräischen von Ruth Achlama C R. Piper GmbH & Co. KG München, Zürich > Abraham B. Jehoschua Die Manis Roman Aus dem Hebräischen von Ruth Achlama C R. Piper GmbH & Co. KG München, Zürich > Abraham B. Jehoschua Die Manis Roman Aus dem Hebräischen von Ruth Achlama C R. Piper GmbH & Co. KG München, Zürich >.'"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example of an article with a high proportion of names\n",
    "data[data.names>=0.15].iloc[0]['texts']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "551453\n"
     ]
    }
   ],
   "source": [
    "# Exclude texts with a name density of at least 15%\n",
    "data = data[data.names<0.15]\n",
    "del data['names']\n",
    "data.reset_index(inplace=True, drop=True)\n",
    "# the number of articles after removing articles that predominantly consist of names\n",
    "print(len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({1999: 26729, 2000: 25808, 1994: 25640, 1995: 25603, 1996: 24676, 2001: 24626, 1997: 24553, 1998: 24504, 2008: 24146, 2010: 22744, 2009: 22521, 2011: 22491, 2005: 22395, 2006: 22082, 2007: 21959, 2004: 21823, 2003: 21781, 2012: 21047, 2002: 20760, 2013: 19232, 2014: 18188, 2016: 17866, 2017: 17497, 2015: 17421, 2018: 15361})\n"
     ]
    }
   ],
   "source": [
    "counter=collections.Counter(data['year'])\n",
    "print(counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_csv('sz_prepro_final.csv', encoding = 'utf-8-sig', sep = ';')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
